{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8455b05a-d34d-4d3b-95f8-5e144f71185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval:false\n",
    "! [ -e /content ] && pip install -Uqq fastai # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbabe37a-b71d-4a3d-84d3-f12432936e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp l2r.models.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "918f37a4-b8f0-48d2-acf4-98dd9fd3887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.torch_imports import *\n",
    "from fastai.layers import sigmoid_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcd74f13-3ef3-487a-9266-c46f4dd2ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a8cac-3eda-4d61-aa2a-ef3c42db78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e012d-4e12-4585-bbb5-e1f08b0377dd",
   "metadata": {},
   "source": [
    "# L2R Models\n",
    "\n",
    "> Contains the models for learning to rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfd0b7-a150-4c51-a9bd-25465ca34e5f",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8ea5652-698c-4535-b330-cb9c1d472585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2R_DotProductBias(nn.Module):\n",
    "    def __init__(self, num_lbs, num_toks, num_factors, y_range=None):\n",
    "        super().__init__()\n",
    "        self.num_toks, self.num_lbs = num_toks+1, num_lbs+1 # +1 for the `padding_idx` \n",
    "        self.token_factors = nn.Embedding(self.num_toks, num_factors, padding_idx=-1)\n",
    "        self.token_bias = nn.Embedding(self.num_toks, 1, padding_idx=-1)\n",
    "        self.label_factors = nn.Embedding(self.num_lbs, num_factors, padding_idx=-1)\n",
    "        self.label_bias = nn.Embedding(self.num_lbs, 1, padding_idx=-1)\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        xb_toks = xb[:, :, :, 0].long() # xb[...,0] # shape (64, 2233, 64)\n",
    "        xb_lbs = torch.unique(xb[:, :, :, 1], dim=-1).flatten(start_dim=1).long() # shape (64, 2233, )\n",
    "        # To convert -1 which is the padding index to the last index:\n",
    "        xb_toks, xb_lbs= xb_toks%(self.num_toks), xb_lbs%(self.num_lbs)\n",
    "        \n",
    "        toks_embs = self.token_factors(xb_toks) # shape (64, 2233, 64, 400)\n",
    "        toks_shape = toks_embs.shape\n",
    "        toks_embs = toks_embs.view(-1, *toks_shape[2:]) # shape (64*2233, 64, 400)\n",
    "\n",
    "        lbs_embs = self.label_factors(xb_lbs) # shape (64, 2233, 400)\n",
    "        lbs_shape = lbs_embs.shape\n",
    "        lbs_embs = lbs_embs.view(-1, *lbs_shape[2:]).unsqueeze(dim=-1) # shape (64*2233, 400, 1)\n",
    "        \n",
    "        res = torch.bmm(toks_embs, lbs_embs) # shape (64*2233, 64, 1)\n",
    "        # res = torch.matmul(toks_embs, lbs_embs)\n",
    "        res = res.view(toks_shape[0], toks_shape[1], *res.shape[1:]) + self.token_bias(xb_toks) + self.label_bias(xb_lbs).unsqueeze(2) # shape (64, 2233, 64, 1)\n",
    "        \n",
    "        return sigmoid_range(res, *self.y_range) if self.y_range is not None else res\n",
    "        # return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cb931-0d4a-4c04-af5f-e15eda27ee3e",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e002024d-3303-495e-9ed7-3704ad375a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2R_NN(nn.Module):\n",
    "    def __init__(self, num_lbs, num_toks, num_factors, n_act = 200, y_range=None):\n",
    "        super().__init__()\n",
    "        self.num_toks, self.num_lbs = num_toks+1, num_lbs+1 # +1 for the `padding_idx` \n",
    "        self.token_factors = nn.Embedding(self.num_toks, num_factors, padding_idx=-1)\n",
    "        self.label_factors = nn.Embedding(self.num_lbs, num_factors, padding_idx=-1)\n",
    "        self.y_range = y_range\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_factors*2, n_act),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_act, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        xb_toks = xb[:, :, :, 0].long() # xb[...,0] # shape (64, 2233, 64)\n",
    "        xb_lbs = torch.unique(xb[:, :, :, 1], dim=-1).flatten(start_dim=1).long() # shape (64, 2233, )\n",
    "        # To convert -1 which is the padding index to the last index:\n",
    "        xb_toks, xb_lbs= xb_toks%(self.num_toks), xb_lbs%(self.num_lbs)\n",
    "        \n",
    "        toks_embs = self.token_factors(xb_toks) # shape (64, 2233, 64, 200)\n",
    "\n",
    "        lbs_embs = self.label_factors(xb_lbs) # shape (64, 2233, 200)\n",
    "        lbs_embs = lbs_embs.unsqueeze(2) # shape (64, 2233, 1, 200)\n",
    "        lbs_embs = lbs_embs.expand(-1, -1, xb.shape[2], -1)\n",
    "        \n",
    "        embs = torch.cat((toks_embs, lbs_embs), dim=-1) # shape (64, 2233, 64, 400)\n",
    "        res = self.layers(embs)\n",
    "        \n",
    "        return sigmoid_range(res, *self.y_range) if self.y_range is not None else res\n",
    "        # return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61316b13-6988-4ef8-b56c-f56942ddc5cf",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3cc98c0-b181-48ce-9028-6ee2f18d0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
