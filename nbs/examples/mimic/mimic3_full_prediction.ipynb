{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04a16c-ec06-4a19-bed6-4ab3451937ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e753d-7dec-4ef6-b891-168c5614be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf79eed-ae89-4216-8fab-2a38a27cae72",
   "metadata": {},
   "source": [
    "# MIMIC III Code Prediction\n",
    "\n",
    "> API details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d52d4d-05bd-40b7-a37d-9139b3762c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d50982-f79b-478c-a460-e61f0cac93bd",
   "metadata": {},
   "source": [
    "Application specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66767808-25be-4278-bca6-5ed90d81aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from fastai.distributed import *\n",
    "\n",
    "from xcube.text.learner import text_classifier_learner\n",
    "from xcube.metrics import PrecisionK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d578aee-3f61-4aed-b592-3bd1a84d0d42",
   "metadata": {},
   "source": [
    "#### Setting the base path as the path of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4c77f-03f7-4f9a-85b9-d7afc017e92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('/home/deb/xcube/nbs/examples/mimic'),\n",
       " Path('/home/deb/xcube/nbs/examples/mimic/data'),\n",
       " Path('/home/deb/xcube/nbs/examples/mimic/models'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path.cwd()\n",
    "path_data = path/'data'\n",
    "path_model = path/'models'\n",
    "\n",
    "path, path_data, path_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9aa85-43e6-4083-90d9-fa169502ead0",
   "metadata": {},
   "source": [
    "## `DataLoaders` for the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9805d-51ef-41fd-8665-8245d7f97c0a",
   "metadata": {},
   "source": [
    "To be able to use Transfer Learning, first we need to fine-tune our Language Model (which we pretrained on Wikipedia) on the corpus of Mimic-III (the one we prepared in the notebook 01_data_extraction2). Here we will build the `DataLoaders` object using the `DataBlock` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666c754-021c-48ce-8c93-b67c415cbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_data/'caml_notes_labelled.csv',\n",
    "                dtype={'text': str, 'labels': str, 'subject_id': np.int64, 'hadm_id': np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb52c3-3ebc-4017-a7d4-da5b08afea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['text', 'labels']] = df[['text', 'labels']].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84982b63-6d7f-4517-a791-30d440af517c",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5108ee-0951-4426-8716-8aa63e8b1323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>length</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86006</td>\n",
       "      <td>111912</td>\n",
       "      <td>admission date discharge date date of birth sex f service surgery allergies patient recorded as having no known allergies to drugs attending first name3 lf chief complaint 60f on coumadin was found slightly drowsy tonight then fell down stairs paramedic found her unconscious and she was intubated w o any medication head ct shows multiple iph transferred to hospital1 for further eval major surgical or invasive procedure none past medical history her medical history is significant for hypertension osteoarthritis involving bilateral knee joints with a dependence on cane for ambulation chronic...</td>\n",
       "      <td>801.35;348.4;805.06;807.01;998.30;707.24;E880.9;427.31;414.01;401.9;V58.61;V43.64;707.00;E878.1;96.71</td>\n",
       "      <td>230</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85950</td>\n",
       "      <td>189769</td>\n",
       "      <td>admission date discharge date service neurosurgery allergies sulfa sulfonamides attending first name3 lf chief complaint cc cc contact info major surgical or invasive procedure none history of present illness hpi 88m who lives with family had fall yesterday today had decline in mental status ems called pt was unresponsive on arrival went to osh head ct showed large r sdh pt was intubated at osh and transferred to hospital1 for further care past medical history cad s p mi in s p cabg in ventricular aneurysm at that time cath in with occluded rca unable to intervene chf reported ef 1st degre...</td>\n",
       "      <td>852.25;E888.9;403.90;585.9;250.00;414.00;V45.81;96.71</td>\n",
       "      <td>304</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88025</td>\n",
       "      <td>180431</td>\n",
       "      <td>admission date discharge date date of birth sex f service surgery allergies no known allergies adverse drug reactions attending first name3 lf chief complaint s p fall major surgical or invasive procedure none history of present illness 45f etoh s p fall from window at feet found ambulating and slurring speech on scene intubated en route for declining mental status in the er the patient was found to be bradycardic to the s with bp of systolic she was given atropine dilantin and was started on saline past medical history unknown social history unknown family history unknown physical exam ex...</td>\n",
       "      <td>518.81;348.4;348.82;801.25;427.89;E882;V49.86;305.00;96.71;38.93</td>\n",
       "      <td>359</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id  \\\n",
       "0       86006   111912   \n",
       "1       85950   189769   \n",
       "2       88025   180431   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  admission date discharge date date of birth sex f service surgery allergies patient recorded as having no known allergies to drugs attending first name3 lf chief complaint 60f on coumadin was found slightly drowsy tonight then fell down stairs paramedic found her unconscious and she was intubated w o any medication head ct shows multiple iph transferred to hospital1 for further eval major surgical or invasive procedure none past medical history her medical history is significant for hypertension osteoarthritis involving bilateral knee joints with a dependence on cane for ambulation chronic...   \n",
       "1  admission date discharge date service neurosurgery allergies sulfa sulfonamides attending first name3 lf chief complaint cc cc contact info major surgical or invasive procedure none history of present illness hpi 88m who lives with family had fall yesterday today had decline in mental status ems called pt was unresponsive on arrival went to osh head ct showed large r sdh pt was intubated at osh and transferred to hospital1 for further care past medical history cad s p mi in s p cabg in ventricular aneurysm at that time cath in with occluded rca unable to intervene chf reported ef 1st degre...   \n",
       "2  admission date discharge date date of birth sex f service surgery allergies no known allergies adverse drug reactions attending first name3 lf chief complaint s p fall major surgical or invasive procedure none history of present illness 45f etoh s p fall from window at feet found ambulating and slurring speech on scene intubated en route for declining mental status in the er the patient was found to be bradycardic to the s with bp of systolic she was given atropine dilantin and was started on saline past medical history unknown social history unknown family history unknown physical exam ex...   \n",
       "\n",
       "                                                                                                  labels  \\\n",
       "0  801.35;348.4;805.06;807.01;998.30;707.24;E880.9;427.31;414.01;401.9;V58.61;V43.64;707.00;E878.1;96.71   \n",
       "1                                                  852.25;E888.9;403.90;585.9;250.00;414.00;V45.81;96.71   \n",
       "2                                       518.81;348.4;348.82;801.25;427.89;E882;V49.86;305.00;96.71;38.93   \n",
       "\n",
       "   length  is_valid  \n",
       "0     230     False  \n",
       "1     304     False  \n",
       "2     359     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6664931-d445-440a-8985-5b1c47227637",
   "metadata": {},
   "source": [
    "We will now create the `DataLoaders` using `DataBlock` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de686c8-1053-42b4-bf59-688d6b7c68ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm = DataBlock(\n",
    "    blocks   = TextBlock.from_df('text', is_lm=True, max_vocab=51920),\n",
    "    get_x    = ColReader('text'),\n",
    "    splitter = RandomSplitter(0.1)\n",
    ").dataloaders(df, bs=384, seq_len=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1fa19-be56-493c-b5af-3f140fb86941",
   "metadata": {},
   "source": [
    "Let's take a look at the batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f76e8-9d27-422d-9133-861efc0e6b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deb/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:475: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "/home/deb/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:475: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "/home/deb/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:475: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "/home/deb/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:475: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos admission date discharge date date of birth sex m service medicine allergies ace inhibitors attending first name3 lf chief complaint shortness of breath red tinged sputum production major surgical or invasive procedure bedside bronchoscopy with bronchoalveolar lavage xxunk intubation and ventilation nasogastric tube placement history of present illness this is a year old first name3 lf speaking male with h o dm infarct related cardiomyopathy chf most recent ef of in month only nadir of cad s p cabg</td>\n",
       "      <td>admission date discharge date date of birth sex m service medicine allergies ace inhibitors attending first name3 lf chief complaint shortness of breath red tinged sputum production major surgical or invasive procedure bedside bronchoscopy with bronchoalveolar lavage xxunk intubation and ventilation nasogastric tube placement history of present illness this is a year old first name3 lf speaking male with h o dm infarct related cardiomyopathy chf most recent ef of in month only nadir of cad s p cabg x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remained stable through the rest of her stay and did not require further blood transfusions afib with rvr patient with known afib in setting of hypotension and tachycardia patient was xxunk diltiazem drip for rate control originally the patient s tachycardia resolved and the patient was transitioned back to her normal regimen of dilt 30 mg po hospital unit name and metoprolol 25 mg po tid the patient was on 3 mg coumadin on admission with therapeutic inr which was</td>\n",
       "      <td>stable through the rest of her stay and did not require further blood transfusions afib with rvr patient with known afib in setting of hypotension and tachycardia patient was xxunk diltiazem drip for rate control originally the patient s tachycardia resolved and the patient was transitioned back to her normal regimen of dilt 30 mg po hospital unit name and metoprolol 25 mg po tid the patient was on 3 mg coumadin on admission with therapeutic inr which was held</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51ab8b-4379-4201-b00b-8ec45986cdfd",
   "metadata": {},
   "source": [
    "The length of our vocabulary is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780e8ed-ac92-4bd9-b9df-78e226f2a353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51928"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls_lm.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9da0bf-2a3e-48b8-8dd3-70a3f8d78603",
   "metadata": {},
   "source": [
    "Let's take a look at some words of the vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ab254-2527-46b1-a338-b8097927829a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#51928) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','and','to','of','was','with','a','on','in','for','mg','no','tablet','patient','is','he','at','blood','name','po','she'...]\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_repr(L(dls_lm.vocab), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa02541-8770-48a1-b599-6305fa5bcf8e",
   "metadata": {},
   "source": [
    "Creating the `DataLaoders` takes some time, so smash that *save* button (also a good idea to save the `dls_lm.vocab` for later use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d732508-e303-4e7d-988e-a6e755aceafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dls_lm, path_model/'caml_dls_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa19a5-77a6-44dd-b35e-612801b4a0f3",
   "metadata": {},
   "source": [
    "To load back the `dls_lm` later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb5e82-cabf-4ccf-8713-45ba39803806",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_lm = torch.load(path_model/'caml_dls_lm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85993758-3e22-4f0c-b86e-1ce34e8558c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dls_lm.vocab, path_model/'caml_dls_lm_vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee3d1b-896c-4c06-9547-73a99a91eda1",
   "metadata": {},
   "source": [
    "## `Learner` for the Language Model Fine-Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a79a12-2c1e-470b-96e3-ab87d51454b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3,\n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b5024-72d6-498b-9329-2ea7b3a73643",
   "metadata": {},
   "source": [
    "Let's compute the learning rate using the `lr_find`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366cf40-7973-417e-a062-400529e20fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.13182567358016967,\n",
       " 0.03981071710586548,\n",
       " 0.004365158267319202,\n",
       " 0.02754228748381138)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEQCAYAAACqduMIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA8AUlEQVR4nO3deXxU5dn/8c+Vyb6QhSyEBAgkoqCCIsriArhghaL1UdDiQql9XGor+rNPtT5qUauPtrYVtC60dde6tC5FoFhBLLgVXFDZ14SwZ9/36/fHTNKAk5CEmTmT5Hq/XvPKzDn3zPnOBHLNOfc59y2qijHGGHO4EKcDGGOMCU5WIIwxxnhlBcIYY4xXViCMMcZ4ZQXCGGOMV1YgjDHGeBXqdABfSU5O1qysLKdjGGNMt/LZZ58VqGqKt3U9pkBkZWWxZs0ap2MYY0y3IiK5ba2zQ0zGGGO8CliBEJEsEVksIsUisk9EHhMRr3swInKLp02piDwtIhGBymmMMcYtkHsQjwMHgHTgJGAC8OPDG4nI+cDtwDlAFjAEuCdQIY0xxrgFskAMBl5T1RpV3Qf8AzjeS7tZwJ9VdZ2qFgP3AT8IXExjjDEQ2AIxD7hcRKJFJAO4AHeRONzxwNpWj9cCaSLSNwAZjTHGeASyQHyA+49/GZAPrAHe8tIuFiht9bj5ftzhDUXkWhFZIyJrDh486Nu0xhjTywWkQIhICLAUeAOIAZKBROAhL80rgD6tHjffLz+8oaouUNXRqjo6JcXrabwdsmnft17aGGN6vUDtQSQBA4DHVLVWVQuBZ4ApXtquA0a2ejwS2O95js+9tmYX35n3Lz7LLfbHyxtjTLcVkAKhqgXADuAGEQkVkQTcndFrvTR/HrhGRIaLSCJwJ/Csv7JNOTGdfn0iueONr6lvbPLXZowxptsJZB/EfwHfAQ4CW4EG4BYRGSgiFSIyEEBV/wH8GngfyPXcfumvULERodx70Qls2l/OH1du99dmjDGm2wnYUBuq+iUwsY3VsYe1/R3wOz9HanHe8DS+c3w/5r23haknpjOob0ygNm2MMUHLhtrwmHvh8YS5QrjzrW+webqNMcYKRIt+8ZH8/DvHsnJLAX9fu8fpOMYY4zgrEK1cMWYQIzPj+dWiDZTV1DsdxxhjHGUFohVXiHDf906goKKW3/9zs9NxjDHGUVYgDjMiM4ErxgzkuY92sn5PmdNxjDHGMVYgvPjZ5GNJiA7nrre/oanJOqyNMb2TFQgvEqLDuf2C4/gst5i/fp7vdBxjjHGEFYg2XDoqk1MGJfLrf2y0DmtjTK9kBaINISHC3GnHU1hZx6PLtjgdxxhjAs4KRDtOzIxnxikDeObDnWw7WOF0HGOMCSgrEEfws/OPJSrMxa/eWd/l12hobCK3sJLPcovJL66yQQGNMd1CwMZi6q5S4iK46ZxjuH/xBt7feIBJx6Uesl5V2XawkryiSnaX1LCvtJqiyjrKqhsoq6lnb2kNuYWV1Df+52yoEIHUuEgSosOIj3Lf+saGkxwbQXJsBJmJUQxNiyMjIYqQEAGgoraBg+W1VNc1UtfYRH1jE0PT4oiPCgvo52GM6T2kp4w7NHr0aF2zZo1fXruuoYnvPPIv9pXVMG5IX8bnJDMoKZpVWwv45/r97C6pbmnrChESo8OJjwqlT1QYKbERDEmJZUhyDMlx4Rwoq2VPSTV7SmsoqaqnrLqe0up6CitrKaqso/VZtTHhLlL7RHKwvJaK2oZv5QpzCWfkJDPlxHTOGZZGUky4X96/MabnEpHPVHW013VWIDpm64EKnv5wBx9vK2RHQSUAkWEhnJGTwrnDUhnaL47+8VGkxEXg8nzr76zGJqWoso68oko27atg8/5yDpTXkBoXSVqfSFLjIoiJcBER6gKBj7cVsuirvS0F6ti0OE4bnMSpg5M4eUACmYlRiBw5S1OTotDl3MaY7ssKhI/tKalmZ2ElJw9IJCrcFZBttkVV+Sq/lFVbC/h0RxGf7Syisq4RgOTYcE7MiCchOpzocBdRYS4qahsorKyjqLKO4so6SqrrKamqI9QVwvH9+zAyM4GTByZw2uAk0uOjHH1vxhj/swLRizQ0NrFxXzlf7Crhy7wS1u8to6K2nuq6RqrqGomJCKVvTDhJMeEkxoSTEBVGYnQ4NfWNfJVfyte7S6mudxeYrL7RjB3SlzFDkjhtcF8yEgJbMPKLq/hwawH7SmsJDw0hPDSE0BChrqGJ2oZGahuaqKlvpLq+kdr6JjIToxk5IJ6RmQkk+uhwm6pSUdtAY5OiCgrER4XZ3pbpMRwvECJy+DmiUcDjqvpTL20FuA+YjXsioS+AG1V1XXvbsALhG80F5pPthXyyvYhPdxRSXuPu/8hMjCI7JZbE6DASosOJiwwlIjSEiFAXjarsLKhk28EKdhVVExsZSkpsBKl9Igh3hdCoSlOT0tCk1Dc2UdfQhCtEGJgUw+CUGDIToiiqrCO/uJpdxVWs2VnEzsKqdrOKQFSYe88ozBXC/vIamv85p8S5O/szEtyH/cJdIYS6hHCXi76x4aTEuU8IiApzEeoSXCFCSVU9+cVV5BdXk1tYybaDlWw9UEFp9aEXSrpChLS4CPrFR5IaF0lSbDjJMeEkRIfTJyqMuMhQkmLCOSY1loRo6xcywc3xAnFYmBhgPzBFVf/lZf0M4BHgDNzTjf4KOF9VR7X3ulYg/KOxSdm4r4x/7yji3zuK2F1STUmV+7BUeW0Drf/59I0JJzsllgFJ0VTVuc+6OlBeS31jEyHi/iMcGiKEh4YQ5gqhvrGJ3MKqlj2WZilxEYzIiOeMY5I5IyeZwckx1DcqdQ1NNDQ1ERHmItwVQphLDuljKa+p55vdZazNL2H7wQp2l1STX1xNYUUdDU1NNDS6C1RHJMe630t2aiyDkqIJc4UQIu49iMKKOvaW1rC3tJqD5e6TC4qq6vD2XymtTwRD0+JIiYsgISqchOgwhqTEcPLARPrHR3aoj8gYfwq2AjEL9xzT2epl4yJyG3CKqs7wPD4e+ExVI9t7XSsQgafq/oNb29CEqhIX2flTblWV/WW17C6pIikmgvT4SCLD/Nev09DYRFFlHQfKazlYUUttfRONTUpDUxNxkaEMSIwmIzGK6PDOnQHe2KSUVddTXuM+vbmgopbN+8vZuLecLQcqKKqso7S6/pCz0VLjIhgzpC/nDktl0nGp9OnC52fM0WqvQDhxHcQs4HlvxcHjFeAyERkK7PC0/0egwpmOExHCXEKYq+vXW4oI/eIj6Rffbv33mVBXCKl9Iknt49vtuUKERE+/TrOJx6Z+q11tQyOb91Xwxa5iPs8tZtXWAhau3UOYSxg7pC/fOaEfk4f3IyUuwqf5jOmKgO5BiMhA3H/0c1R1RxttwoHfADcBjcAu4Gxv7UXkWuBagIEDB56Sm5vrr+jG+EVjk/LlrmLeXb+fpd/sY2dhFSJwalYSZx+XysRjUzg2Lc4ORRm/CZpDTCJyJ3Ceqk5op839wCRgBrAPuBL3IanjVbXNXks7xGS6O1Vl0/5ylny9j6Xr9rFxXzkA/fpEcu7wVKacmM6YwX3tDCrjU8FUIDYDD6rq0+20eQf4p6rOa7WsBDhXVdusAFYgTE+zr7SGf20+yPubDrBi00Gq6xtJjg1n6onpXHbqQIb37+N0RNMDBEWBEJHxwD+Bfqpa3k67XwLnAZcAB4ErgCeBDFUtaet5ViBMT1ZV18CKTQdZ9PVe/rl+P3UNTYzIjGfmaQO5eFSG++p6Y7ogWArEU0C0ql512PKBwHpguKrmiUgk8Fvgv4AYYCtwh6q221FtBcL0FiVVdbz1xW5eWb2LjfvK6dcnkhsmZnPZqQP8egaY6ZmCokD4mxUI09uoKh9uLWTess2s3llMalwE10/IZuaYgVYoTIdZgTCmB1NVPtlexLxlm/lkexEpnkJxhRUK0wHtFQibMMiYbk5EGJfdl1euHccr144lJyWW+95Zz3cfXcVX+SVOxzPdmBUIY3qQsUP68pdrx/LcD0+joqaBix//iN//c7PNYmi6xAqEMT3QhKEpLL35LC4c2Z95y7Yw/cmP2fHK39hy9jlsGDacLWefQ+nChU7HNEHOphw1poeKjw7j95edxLnD0lj4u6cpXf0qEY3ukWkb9uxh7113u9tNm+ZkTBPEbA/CmB5u6oh05ux4r6U4NNOaGg78/hFnQpluwQqEMb3Bgf1eFzfs3RvgIKY7sQJhTC8Qmp7udXlIWr8AJzHdiRUIY3qB1FtuRiIPHeK8xhXGEzmT2bivzKFUJthZgTCmF4ifNo30++4ltH9/ECG0f38ib7+Tj4eM5rKnPuHzvGKnI5ogZFdSG9OL7Sqq4qo/f8r+sloWXH0KZx6T4nQkE2B2JbUxxqsBSdG8fv14spJj+OGzq/nneu+d2aZ3sgJhTC+XEhfBK9eOZXj/eG586XM+2lrgdCQTJKxAGGOIjwrjudmnMjg5hh89v4YvrE/CYAXCGOOREB3OC9ecRnJsBD94ZjWb9rU5r5fpJaxAGGNapPaJ5KUfjSEyLIQfPruaoso6pyMZBwWkQIhIxWG3RhF5tJ32Q0TkHREpF5ECEfl1IHIaY9wd13+8ejQHK2r5ycuf02AjwfZaASkQqhrbfAPSgGrgdW9tRSQc99zVy4F+QCbwYiByGmPcRmQmcP/3TuCjbYX8eukmp+MYhzgxmuulwAFgZRvrfwDsUdXftVr2lb9DGWMONX30AL7ZXcqCf23nhIx4LhzZ3+lIJsCc6IOYBTyvbV+hNxbYKSJLPIeXVojIid4aisi1IrJGRNYcPHjQb4GN6a3u/O5wTstK4ra/fsXWAxVOxzEBFtACISIDgQnAc+00ywQuB+YD/YFFwNueQ0+HUNUFqjpaVUenpNgVoMb4WpgrhEdnnkxUuIufvPw5NfWNTkcyARToPYirgVWquqOdNtWeNktUtQ54GOgLDAtEQGPModL6RPLb6SPZuK+cBxZvcDqOCSAnCkR7ew/g7m/oGQNEGdNDTDoulf8+czDPf5zLP77Z53QcEyABKxAiMh7IoI2zl1p5ERgrIueKiAu4GSgA7KuLMQ76n/OPY0RmPD//61r2lFQ7HccEQCD3IGYBb6jqIZdnishAz7URAwFUdRNwJfAkUAxcBFzoOdxkjHFIeGgI8y8/mfpG5ba/fUVPGQnatC1gBUJVr1PVq7wsz/NcI5HXatkbqpqjqn1UdaKqrgtUTmNM27KSY7hjynGs3FLAy//OO/ITTLdmQ20YYzrlijGDOCMnmfsXbWBXUZXTcYwfWYEwxnRKSIjw0KUjcInws9fX0tRkh5p6KisQxphOy0iI4q5pw/l0RxEvfprrdBzjJ1YgjDFdMv2UTM48Jpnf/GMTB8pqnI5j/MAKhDGmS0SE+y46gdrGJu59Z73TcYwfWIEwxnRZVnIMP5mUwztf7eWDzTYeWk9jBcIYc1SumzCEIckx3P32NzZWUw9jBcIYc1QiQl386uITyC2s4vH3tzodx/iQFQhjzFEbn53M907qz5MfbGdnQaXTcYyPWIEwxvjEHVOGER4awj0L19kwHD2EFQhjjE+k9onk5nOP4f1NB3lvwwGn4xgfsAJhjPGZWeOzGJoWy73vrLMO6x7ACoQxxmfCXCHMvfB4dhVV8+QH25yOY46SFQhjjE+Nz07muyPSeWLFNvKLbTC/7swKhDHG5+6YMgwR+L/FG52OYo6CFQhjjM/1T4jihgk5LPp6L59sL3Q6jumigBQIz4xxrW+NIvJoB563XERUREIDkdMY4zvXTRhCRkIU9yxcT6MNCd4tBaRAeGaMi1XVWCANqOYIc1OLyBWAFQZjuqnIMBd3TBnGhr1lvLLaZp/rjpw4xHQpcABY2VYDEYkHfgn8PFChjDG+N+XEfpw2OImHl26irKbe6Timk5woELOA57X9Sy0fAJ4A9rX3QiJyrYisEZE1Bw/aSJLGBBsR4e7vDqe4qp6nV+1wOo7ppIAWCBEZCEwAnmunzWjgdOCIfRSqukBVR6vq6JSUFN8FNcb4zAkZ8Zx/fBp/XrmD0irbi+hOAr0HcTWwSlW9fpUQkRDgcWCOqjYENJkxxm9uPnco5bUN/HHldqejmE4IdCfw1cCD7azvA4wGXhURAJdneb6ITFfVNvstjDHBa1h6H6aOSGfle7n0XVFAVXEtsUkRjLsom6Fj+jkdz7QhYAVCRMYDGbR/9lIp0L/V4wHAv4FTAOtkMKYbm5GcyBflxVRRC0BFUS3vv+S+kM6KRHAK5CGmWcAbqlreeqGIDPRcGzFQ3fY13/hPUdivqnUBzGqM8bGdK/YQhhyyrKGuiY/ftjGbglXA9iBU9bo2lucBsW2s2wmH/YsyxnRLFUW1nVpunGdDbRhjAiI2KaJTy43zrEAYYwJi3EXZhIYf+icnNDyEcRdlO5TIHIkNZWGMCYjmjuiP395GeVEtFSHKd2YcYx3UQcwKhDEmYIaO6cfQMf34Or+UaY+tIq6+mpOcDmXaZIeYjDEBd2JmPBOPTeHPq3ZQVWfXxAYrKxDGGEf89OwciirrePlTG+k1WFmBMMY44pRBSYwdksQfV26ntqHR6TjGCysQxhjH3Dgph/1ltbzx+W6noxgvOlwgRGSSiAz23E8XkedE5GkRsVMQjDFdckZOMiMy43nyg200NDY5HcccpjN7EI8DzfuBvwXCAAUW+DqUMaZ3EBF+PDGH3MIqFn/T7vQvxgGdOc01Q1XzPPNDnw8MAuqAPX5JZozpFSYPTyMjcwN3f/YQd35ZQr+YfswZNYepQ6Y6Ha3X60yBKBORNOAEYL2qVohIOO49CWOM6ZIlOxdT3ecVGtU9JtPeyr3M/WgugBUJh3XmENOjwGrgJeAPnmWnAxt9HcoY03vM+3we9XrogH01jTXM+3yeQ4lMsw7vQajqQyLyJtCoqs3j8+4GfuSXZMaYXmFfpfe+h7aWm8Dp1Gmuqrq5uTiIyCSgn6p+7ZdkxpheoV+M9xMh21puAqczp7l+ICKne+7fBrwC/EVE7ujAcysOuzWKyKNttJ0lIp+JSJmI5IvIrz0d48aYHmjOqDlEuiIPWRYeEsGcUXMcSmSadWYP4gTgE8/9/wYmAmOB64/0RFWNbb4BaUA1bU89Gg3cDCQDY4BzgJ91IqcxphuZOmQqc8fPJT0mHUHQ+gSGumZbB3UQ6Mw38xBARSQbEFXdACAiiZ3c5qXAAWClt5Wq+kSrh7tF5CVgUie3YYzpRqYOmdpSEO5ZuI4XPs5ld0k1GQlRDifr3TqzB7EKeAx4GHgTwFMsCjq5zVnA86qqHWx/FrCuk9swxnRTPzpzCAB/XrnD4SSmMwXiB0AJ8BUw17PsOKDD56KJyEBgAvBcB9vPBkbjLkre1l8rImtEZM3Bgwc7GsMYE8QyEqK4cGR/XlmdR2GFzVftpA4XCFUtVNU7VPWXqlrhWbZIVR/pxPauBlap6hG/GojI94AHgQtU1eteiqouUNXRqjo6JSWlEzGMMcHsx5NyqKlvZMG/tjsdpVfrzFlMYSJyj4hsF5Eaz897PFdTd9TVdGDvQUS+A/wRmGan0RrT++SkxnLRSRk89/FODpTXOB2n1+rMIaZfA+fiPmtppOfn2cBDHXmyiIwHMmj77KXmdmfjvlr7ElX9dyfyGWN6kDnnHEN9o/LEim1Hbmz8ojMFYjpwoaq+q6qbVPVd4GJgRgefPwt4Q1XLWy8UkYGeayMGehbdBcQDi1tdN7GkEzmNMT1AVnIMl4zK4KVP89hbWu10nF6pMwVCOrn8EKp6nape5WV5nucaiTzP40mqGtr62glVvaATOY0xPcRPzz6Gpibl8fdtL8IJnSkQrwMLReR8ERnm6Sd4C3jNL8mMMb3egKRoZpw6gFdW55FfXOV0nF6nMwXi58B7uEdy/Qz36K7v454Twhhj/OInk3IQhMetLyLgOnOaa52q3q2qOaoararHAPcDt/ovnjGmt+ufEMWMUzN5fc0udpdYX0QgdWo0Vy+UDvZBGGNMV90wMQeAx9/f6nCS3uVoCwS4i4QxxvhNRkIU00cP4LU1u9hjexEBc8TB+jzXJbSlMxfJGWNMl/14Yjavr9nFEyu2cd/3TnA6Tq/QkdFc/3yE9Xm+CGKMMe3JTIzm0lMyeXX1Ln48KZv0eBvp1d+OeIhJVQcf6RaIoMYY8+OJOTSp8tQHNkZTIPiiD8IYYwJiQFI03zs5w0Z6DRArEMaYbuX6CUOobWji2Y92Oh2lx7MCYYzpVnJS45g8PI3nPtpJRW2D03F6NCsQxphu54aJOZTVNPDyp7lOR+nRrEAYY7qdkwYkMD67L39auYPahkan4/RYViCMMd3SDROzOVBeyxuf73Y6So9lBcIY0y2dkZPMiRnxPPnBNhoam5yO0yNZgTDGdEsiwo2TssktrGLxN/ucjtMjdeRK6qMmIhWHLYoCHlfVn7bR/hbgNk+7vwE3qGqXTnpuamqioKCAkpISGhvtWKWTXC4XCQkJJCcnExJi303M0Zs8vB/ZKTE8/v5Wpo1IR8TGDvWlgBQIVY1tvi8iMcB+2pibWkTOB27HPd/1HuBN4B7Psk7Lz89HRMjKyiIsLMz+ATlEVamvr2f//v3k5+czcODAIz/JmCMICRF+PDGHW19fy/KNBzhnWJrTkXoUJ77GXQocAFa2sX4W8GdVXaeqxcB9wA+6urHKykoyMjIIDw+34uAgESE8PJyMjAwqKyudjmN6kAtP6k9mYhSPvb8VVRtc2pecKBCzgOe17d/k8cDaVo/XAmki0rerG7TDGcHDfhfG18JcIVw3IZsv8kr4ZHuR03F6lID+bxWRgcAE4Ll2msUCpa0eN9+P8/J614rIGhFZc/DgQd8FNcZ0K9NPySQ5NoI/2IRCPhXor3NXA6tUdUc7bSqAPq0eN98vP7yhqi5Q1dGqOjolJcWHMYNPXl4esbGxHepo70xbY3qCyDAX/33mYFZtLeDLXSVOx+kxnCgQ7e09AKwDRrZ6PBLYr6qFfkvVDQwcOJCKigpcLpdP2xrTU1wxdhDxUWE2LakPBaxAiMh4IIM2zl5q5XngGhEZLiKJwJ3As36O591Xr8HvT4C5Ce6fX73mSAxjzJHFRoTyg/FZvLt+P5v2feuAg+mCQO5BzALeUNVDfnMiMlBEKjz9E6jqP4BfA+8DuZ7bLwOY0+2r12DhTVC6C1D3z4U3+bxIZGVl8Zvf/IYRI0YQExPDNddcw/79+7nggguIi4vj3HPPpbi4mJ07dyIiNDS4R6+cOHEid911F6effjpxcXFMnjyZgoICAK9t77zzTsaPH09sbCzTpk2jsLCQK664gj59+nDqqaeyc+dOr89tfv6f/vQnAJ599llOP/10brnlFhISEhgyZAgfffQRzz77LAMGDCA1NZXnnjvSTqIx/vGD8VlEh7t4YoXtRfhCwAqEql6nqld5WZ6nqrGqmtdq2e9UNU1V+6jq7K5eJHdUlt0L9YdNjl5f7V7uY3/729/45z//yebNm1m4cCEXXHABDzzwAAUFBTQ1NTF//nyvz3v55Zd55plnOHDgAHV1dTz88MNtbuOVV17hhRdeYPfu3Wzbto1x48Yxe/ZsioqKGDZsGPfcc0+H83766aeMGDGCwsJCZs6cyeWXX87q1avZunUrL774Ij/5yU+oqDj82khj/C8xJpwrxw7i72v3kFtop1MfLTvnsC2l+Z1bfhR++tOfkpaWRkZGBmeeeSZjxozh5JNPJiIigosvvpgvvvjC6/Nmz57N0KFDiYqKYsaMGXz55ZdtbmP27NlkZ2cTHx/PBRdcQHZ2Nueeey6hoaFMnz69zW14M3jwYGbPno3L5eKyyy5j165d3H333URERDB58mTCw8PZutW+wRln/OiMwYSGhPCkTUt61KxAtCU+s3PLj0Ja2n+u/oyKivrW47a+jffr16/lfnR0dLvf2ru6jY68lrdltgdhnJLaJ5LpozP522f57CutcTpOt2YFoi3n3A1hUYcuC4tyL+/BYmJiAKiqqmpZtm+fDYRmupfrJ2TTqMqfVtpexNGwAtGWETNg2nyIHwCI++e0+e7lPVhKSgoZGRm8+OKLNDY28vTTT7Nt2zanYxnTKQOSopk2Ip2X/51HcWWd03H86oHFG1i2Yb9fXtsKRHtGzIBbvoG5Je6fPbw4NPvjH//Ib37zG/r27cu6desYP36805GM6bQbJuZQVdfIcx/vdDqK39Q2NPLHldv5Kr/0yI27QHrK4FajR4/WNWvWfGv5hg0bGDZsmAOJTFvsd2IC5UfPrWZNbjEf3nY2MREBGbw6oHYUVDLp4RU8PH0kl57Stf5REflMVUd7W2d7EMaYHuuGiTmUVNXzl3/nHblxN7SryN1XOCAx6ggtu8YKhDGmxzplUCJjBifxp5U7qGvoedOS5he7r9XKTIr2y+tbgTDG9Gg/npTDvrIa3vzC99cwOW1XcRVhLqFfn0i/vL4VCGNMj3bWMcmcmBHPEyu20dDYs/Yi8our6Z8QhSvEP5OhWYEwxvRoIsJPzs5hZ2EVi77e63Qcn9pVVEWmn/ofwAqEMaYXOG9YGkPTYnls+VaamnrGmZsA+cVVDEj0T/8DWIEwxvQCISHCjZNy2HKggnfX94yRAarrGimoqLM9CGOMOVrfHdGfwckxPLp8Kz3h+q/8Ys8prn46gwmsQBhjeglXiHDDxGzW7SljxabuP4f9Lk+ByLRDTL3L3LlzufLKK52OYUyPc/HJGWQkRDF/+ZZuvxfRfA2Evy6SgwAXCBG5XEQ2iEiliGwTkTO9tBER+ZWI7BaRUhFZISLHBzJns0XbFzH5r5MZ8dwIJv91Mou2L3IihjHGR8JcIdwwMZsv8kpYtbXA6ThHZVdRFRGhIaTERfhtG4Gck/o84CFgNhAHnAV4G4t3OvBD4EwgCfgYeCFAMVss2r6IuR/NZW/lXhRlb+Ve5n401+dF4qGHHiIjI4O4uDiOPfZYFi1axAMPPMCrr75KbGwsI0eOBKC0tJRrrrmG9PR0MjIyuPPOO2lsbGx5naeffpphw4aRmJjI+eefT25ubss6EWH+/PkMGTKE5ORk/ud//oempp51PrgxHTV9dCb9+kQy773uvReRX1xNRmIUIv65BgICuwdxD3Cvqn6iqk2qultVd3tpNxhYparbVbUReBEYHsCcAMz7fB41jYdONlLTWMO8z+f5bBubNm3iscceY/Xq1ZSXl7N06VKOO+447rjjDi677DIqKipYu3YtALNmzSI0NJStW7fyxRdf8O6777bME/3WW2/xwAMP8MYbb3Dw4EHOPPNMvv/97x+yrTfffJM1a9bw+eef8/bbb/P000/77H0Y051EhLq4YWI2a3KL+Xh7odNxumyXn09xhQAVCBFxAaOBFBHZKiL5IvKYiHg7ePYKkCMiQ0UkDJgF/CMQOVvbV+n9VLi2lneFy+WitraW9evXU19fT1ZWFtnZ2d9qt3//fpYsWcIjjzxCTEwMqamp3HLLLbzyyisAPPXUU/ziF79g2LBhhIaGcscdd/Dll18eshdx2223kZSUxMCBA7n55pv5y1/+4rP3YUx3c9mpA0iNi2Dee1ucjtJlu4qqGZDkv/4HCNweRBoQBlyK+9DRScDJwJ1e2u4FVgKbgGrch5xu8faiInKtiKwRkTUHD/r2rIR+Mf06tbwrcnJyeOSRR5g7dy6pqalcfvnl7Nmz51vtcnNzqa+vJz09nYSEBBISErjuuus4cOBAy/o5c+a0rEtKSkJV2b37PztoAwYMaLk/aNAgr9sxpreIDHNx/YRsPt1RxCfdcC+irKae0up6v57BBIErENWen4+q6l5VLQB+B0zx0vaXwKnAACAS96Gp5SLyrU9CVReo6mhVHZ2SkuLTwHNGzSHSdegAWJGuSOaMmuPT7cycOZNVq1aRm5uLiHDbbbd965jigAEDiIiIoKCggJKSEkpKSigrK2PdunUt65966qmWdSUlJVRXVx8y0c+uXbta7ufl5dG/f3+fvg9jupuZYwaSHBvB/GXdby8iv6j5DKYeUCBUtRjIBzrSIzQSeFVV81W1QVWfBRIJcD/E1CFTmTt+Lukx6QhCekw6c8fPZeqQqT7bxqZNm1i+fDm1tbVERkYSFRWFy+UiLS2NnTt3tnQkp6enM3nyZG699VbKyspoampi27ZtfPDBBwBcf/31/N///V9LwSgtLeX1118/ZFu/+c1vKC4uZteuXcybN4/LLrvMZ+/DmO7IvRcxhI+2FbJmZ5HTcTplV8tFcj3jEBPAM8BPRSRVRBKBm4F3vLRbDUwXkTQRCRGRq3AfntoauKhuU4dM5d1L3+WrWV/x7qXv+rQ4ANTW1nL77beTnJxMv379OHDgAA888ADTp08HoG/fvowaNQqA559/nrq6OoYPH05iYiKXXnope/e6Bx67+OKLue2227j88svp06cPJ5xwAkuWLDlkWxdddBGnnHIKJ510ElOnTuWaa67x6XsxpjuaOWYgSTHhzF8e8D8vR6VlHgg/70EEbMpRT4fzPGAmUAO8BvwcSAXWA8NVNU9EIoHfAv8FxOAuDHeoarsd1TblaNtEhC1btpCTk+N0FMB+Jya4PL5iK7/+xybeuvF0ThqQ4HScDpn793W8vmYX39xz/lGf5hoUU46qar2q/lhVE1S1n6repKo1qpqnqrGqmudpV6OqN6pquqr2UdVRRyoOxhjTVVePyyIhOoxHu1FfRH5xNZmJ0X69BgJsqA1jTC8XGxHKD08fzLKNB/hmd6nTcTokv7jK7/0PYAWiV1DVoDm8ZEwwmjU+i7jIUB5dHvx7EarqmSjIv/0PYAXCGGOIjwpj9vgslq7bz4a9ZU7HaVdJVT2VdY1+nQeimRUIY4wBrjljCHGRofz23c1OR2lXbpH/54FoZgXCGGOA+OgwrjtrCO9t2M/necVOx2nTpn3uPZyhaXF+35YVCGOM8Zh9+mD6xoTz8NJNTkdp04a95USHuxhkexDGGBM4MRGh3Dgph4+2FfJhkM4XsWFvGcf2iyMkxL+nuIIViG5pxYoVZGZmtjzOysrivffeczCRMT3HzDED6R8fya+Xbgq6+SJUlQ17yxiW3icg27MCYYwxrUSGuZhz7jGs3VXC0nX7nY5ziD2lNZTVNDCsn//7H8AKRLtKFy5ky9nnsGHYcLacfQ6lCxc6HckYEwCXjMokJzWWB5dsoK4heGZf3Og5Bdf2IBxWunAhe++6m4Y9e0CVhj172HvX3T4tEg8++CCXXnrpIcvmzJnDTTfdxDPPPMOwYcOIi4tjyJAhPPXUUx16zaamJh588EGys7Pp27cvM2bMoKjIPVLl1KlTefTRRw9pP2LECN566y2fvB9jeopQVwh3Th3GzsIqnv94p9NxWjRfo3Gs7UE468DvH0FrDp1yVGtqOPD7R3y2je9///ssXryYsjL3L72xsZHXXnuNmTNnkpqayjvvvENZWRnPPPMMt9xyC59//vkRX3P+/Pm89dZbfPDBB+zZs4fExERuvPFGwD1t6YsvvtjSdu3atezevZspU7xNy2FM7zbx2FQmDE1h3rItFFbUOh0HcJ/BNCApirjIsIBszwpEGxo8Q2l3dHlXDBo0iFGjRrV8g1++fDnR0dGMHTuWqVOnkp2djYgwYcIEJk+ezMqVK4/4mk899RT3338/mZmZREREMHfuXP7617/S0NDARRddxJYtW9iyxT2cwAsvvMBll11GeHi4z96TMT3JnVOHUVXXyCNBMjXphr1lDOsXmMNLYAWiTaHp6Z1a3lUzZ85smR/65ZdfZubMmQAsWbKEsWPHkpSUREJCAosXL6ag4Min3eXm5nLxxRe3TD86bNgwXC4X+/fvJyIighkzZvDiiy/S1NTEX/7yF6666iqfvh9jepJj0uK4csxAXvo0l037yh3NUl3XyI7CyoD1P4AViDal3nIzEnnolKMSGUnqLTf7dDvTp09nxYoV5Ofn8+abbzJz5kxqa2u55JJL+NnPfsb+/fspKSlhypQpHTrlbsCAASxZsuSQ6UdramrIyMgA3IeZXnrpJZYtW0Z0dDTjxo3z6fsxpqe5+dyhxEWGce876xw97XXT/nJUA9dBDVYg2hQ/bRrp991LaP/+IEJo//6k33cv8dOm+XQ7KSkpTJw4kdmzZzN48GCGDRtGXV0dtbW1pKSkEBoaypIlS3j33Xc79HrXX389//u//0tubi4ABw8e5O23325ZP27cOEJCQrj11ltt78GYDkiMCefWyUP5cGshi7/e51iODS1nMAWmgxoCXCBE5HIR2SAilSKyTUTObKPdEBF5R0TKRaRARH4dyJzN4qdN45jlyxi2YT3HLF/m8+LQbObMmbz33nsth5fi4uKYP38+M2bMIDExkZdffpkLL7ywQ681Z84cLrzwQiZPnkxcXBxjx47l008/PaTN1Vdfzddff82VV17p8/diTE90xZhBDE/vw68WraeytsGRDBv3lhET7mJAAIb5bhbIKUfPA/4EXAb8G0gHUNXdh7ULBzYAfwCeAhqBoar6VXuvb1OOdtzzzz/PggULWLVqlSPbt9+J6Y4+yy3ikic+5oaJ2dz2neMCvv0ZT35Moyp/u2G8T183KKYcBe4B7lXVT1S1SVV3H14cPH4A7FHV36lqpWcK0naLg+m4qqoqHn/8ca699lqnoxjTrZwyKIlLRmXyp5Xb2XawIqDbVlU27CsL6OElCFCBEBEXMBpIEZGtIpIvIo+JiLcZL8YCO0Vkiefw0goRObGN171WRNaIyJqDBw/68y30CEuXLiUlJYW0tLSWw1nGmI67/YLjiAxzMffvge2wzi+uprymgeMCeIorBG4PIg0IAy4FzgROAk4G7vTSNhO4HJgP9AcWAW97Dj0dQlUXqOpoVR2dkpLip+g9x/nnn09lZSVvv/02oaGhTscxpttJiYvgZ5OPZeWWAt75ynfXRB3JRs8ptoE8gwkCVyCqPT8fVdW9qloA/A7wdglvNbBKVZeoah3wMNAXsIPWxhjHXTl2ECdmxHPvO+spra4PyDY/zyvGFSIcF6AhNpoFpECoajGQD3Rkn+yrDrYzxpiAc4UID1x8IoUVtfz23cBMLPT+xgOcmpVITERg9/wD2Un9DPBTEUkVkUTgZuAdL+1eBMaKyLmevoubgQLcZzYZY4zjTsyM5+pxWbzwSS5rd5X4dVv5xVVs3FfOucPS/LodbwJZIO4DVgObcf+x/wK4X0QGikiFiAwEUNVNwJXAk0AxcBFwoedwkzHGBIVbJw8lNS6CO978moZG/w0J/v7GAwCcfVyq37bRloAVCFWtV9Ufq2qCqvZT1Zs8p7DmqWqsqua1avuGquaoah9Vnaiq6wKV0xhjOiIuMoy5045n3Z4yHl+xzW/bWbbxAIOTYxiSEuu3bbTFhtoIQnPnzm25yjkvL4/Y2FgaGxuP2NYYE1gXnJjORSf1Z96yLXzph0NNVXUNfLSt0JG9B7ACEfQGDhxIRUUFLpfL6SjGGC/uvegE0uIiuOXVL6mq8+0wHB9uLaSuoYlzrEAEn82f7uO5Oz7kD9cv57k7PmTzp84N1GWMCU7xUWE8PGMkOwsruX+Rb8+lWb5xP3ERoYzOSvLp63aUFYg2bP50H++/tJGKIvdMUhVFtbz/0kafF4mHHnqIjIwM4uLiOPbYY1m2bNkh63fu3ImI0NDg/mayY8cOJkyYQFxcHOedd9635oj45JNPGD9+PAkJCYwcOZIVK1b4NK8x5tvGZyfz32cO4aVP81i6zjd/I1SVZRsOcNbQFMJDnflTbQWiDR+/vY2GukPPTGioa+Ljt33XGbVp0yYee+wxVq9eTXl5OUuXLiUrK6vd58ycOZNTTjmFgoIC7rrrLp577rmWdbt372bq1KnceeedFBUV8fDDD3PJJZdgw5AY43+3Th7KiMx4bn1tLVsPHP1YTev2lHGgvNax/gewAtGm5j2Hji7vCpfLRW1tLevXr6e+vp6srCyys7PbbJ+Xl8fq1au57777iIiI4KyzzmJaqyHIX3zxRaZMmcKUKVMICQnhvPPOY/To0SxevNhnmY0x3kWEunjyylOIDAvh2ufXUFZzdFdZv7dhPyIw8VjnhhGyAtGG2KSITi3vipycHB555BHmzp1Lamoql19+OXv27Gmz/Z49e0hMTCQmJqZl2aBBg1ru5+bm8vrrr7dMN5qQkMCqVavY68N5tI0xbeufEMUfZo4ir6iKW175kqamrg0KUVnbwMuf5jFmcBJ9Y333N6ezrEC0YdxF2YSGH/rxhIaHMO6itr/hd8XMmTNZtWoVubm5iAi33XZbm23T09MpLi6msrKyZVleXsvlIwwYMICrrrrqkOlGKysruf32232a2RjTtjFD+nL3tOEs23iA+xdv6NKor4+v2MqB8lp+7sC8E61ZgWjD0DH9mHTFcS17DLFJEUy64jiGjunns21s2rSJ5cuXU1tbS2RkJFFRUe2ezjpo0CBGjx7NL3/5S+rq6li1ahULFy5sWX/llVeycOFCli5dSmNjIzU1NS3zXRtjAueqsYP4wfgs/rxqB3e8+TWNndiT2FVUxR9X7uDikzMYNTDRjymPzMZ8bsfQMf18WhAOV1tby+23386GDRsICwtj/PjxLFiwgAULFrT5nJdffplZs2aRlJTEuHHjuPrqqykpKQHcexBvv/02P//5z/n+97+Py+XitNNO44knnvDbezDGfJuI8Mtpw4mJcPGH97dRVt3A7y87qUNnIz2weAMuEUdmrTtcwKYc9TebcrT7sN+J6U0W/GsbDyzeyGlZScy98HiG9281p8NXr8Gye6E0H+Iz2XzC/2PysjT+33lDuemcYwKSL1imHDXGmF7n2rOy+e30kWzaX87UR1dyy6tfsquoyl0cFt4EpbsAhdJdDPjwdn4Q+2+uPWuI07EBO8RkjDF+d8kpmZw7LI3HP9jKMx/u5M0vdvNJ1B300+pD2kVRyy8iXici7B6Hkh7KCoQxxgRAfHQYv7hgGLPGZfHmF7tJ+6DAa7uIyrZPdQ80O8RkjDEB1D8hihsn5SDxmd4btLXcAb2iQDQ1+W8yD9M59rswxuOcuyEs6tBlYVHu5UEioAVCRC4XkQ0iUiki20TkzCO0Xy4iKiJdPhQWExPD7t27qaur69IFK8Y3VJW6ujp27959yJXgxvRaI2bAtPkQPwAQ989p893Lg0TA+iBE5DzgIeAy4N9A+hHaX4EP8mVmZlJQUEBubm7LiKjGGaGhocTHx5OcnOx0FGOCw4gZQVUQDhfITup7gHtV9RPP491tNRSReOCXwNXAx0ez0ZCQEFJTU0lNdW5ERGOM6Y4CcohJRFzAaCBFRLaKSL6IPCYiUW085QHgCaDdgdVF5FoRWSMia2xIa2OM8a1A9UGkAWHApcCZwEnAycCdhzcUkdHA6cCjR3pRVV2gqqNVdXRKinND4hpjTE8UqALRfDXIo6q6V1ULgN8BU1o3EpEQ4HFgjqpah4ExxjgoIAVCVYuBfOBIpxH1wX0o6lUR2Qes9izPP9IZT8YYY3wrYIP1ici9wAXAVKAe+DuwQlXvatVGcB+OajYA9xlPmcBBVa1r5/UPArmeh/FAaTv3D18WBni/rLFtrV+jI+sOX9bRjM0/kzuZMVD5mpfZZxhc+bpDxmDPdzQZ21sWbJ/hIFX1foxeVQNyw/0H5HGgBHfn83wgEhgIVAADvTwnC/deR2gnt7WgvfuHLwPWdOH9LOjMusOXdTRjq5+dyhiofPYZBme+7pAx2PMdTcYjZA2qz7C9W8BOc1XVeuDHnltreUBsG8/ZCUgXNrfwCPcPXzbqKLfRkXWHL+toxva2055A5Wu+b59hcOVra30wZQz2fG2t70jGIy3rDH9/hm3qMfNBHA0RWaNtjIceLII9Y7Dng+DPGOz5IPgzBns+6B4Zm/WKsZg6oO0p3IJHsGcM9nwQ/BmDPR8Ef8ZgzwfdIyNgexDGGGPaYHsQxhhjvLICYYwxxisrEB0kImeIyArPbbOI/N7pTIcTkYkiskxE3heRi53OczgRyRKRg60+x6AcH0VEvu+5riboiEiaiHwkIh94hsNvd1TkQBORcSLysSffX0QkzOlMhxOReBH5t4hUiMgJTudpJiL3i8hKEfmriEQ7nQesQHSYqq5S1YmqOhH4CHjL2USHEpFI4FbgAlWdpKpvOp2pDR80f46qGnR/hD3DvVwK7HI6SxsKgDNUdQLwPHCNw3kOlwuc7cm3HbjI4TzeVOG+YPevTgdp5ilU2ap6JvAe8EOHIwFWIDrN843oNGCl01kOMx73mFcLReRNEenndKA2nO75lvSA58r5YDMT9x+OoJz6TlUbVbU5Wxywzsk8h1PVParaPPZaA0H4OapqfRB+OTkTWOK5vwQ4w8EsLXpkgRCRn3iGAa8VkWcPW5fk+QNaKSK5IjKzky9/HrCs1X/SYMmXBuQA04A/AnO7ms+PGfd6Mp4FpAL/FUz5PMPSzwBe7Wouf2f0PPckEfkU+AnwebDl8zx/MO6hdd7paj5/Z/SHo8ibyH+GwCgFkgIUuV2BnDAokPYAvwLOBw6fc+IPQB3uP6gnAYtEZK2qrvN86/a223mpqjbPTTEdeCbY8uEewuRDVa0TkWXA7cGW0fMZ1gKIyBvAWOBvwZLP81qvqWqTj3Zu/PIZquqXwBgRmQH8Arg+mPKJSB/gOeAqbWf8NCczHmUmn+cFinGPjYTnZ5EfM3ZcZ8YE6W433L+oZ1s9jsH9CxraatkLwIMdfL0w4BsgJNjyAX1xH7sUYAzwTBBm7NPq/v8BVwdZvoeAd4F/4P4WNz8IP8OIVvfPB34XZPlCgUW4+yGO+rPzR8ZW7Z8FTvBlzq7mBU4EXvbcvxb4qT9ydfbWU/cg2jIUaFTVza2WrQUmdPD55wLL9SgOLx1Bl/OpaqGIvAl8gPu4r786uY7mM5wgInNxdxLuAO5qv3mXHM1neFvzfXEPh3CTH/LB0X2Go0TkIaARqME/v+ejyfd93F9Q7haRu4EnVNUnh+x8mBERWYz7W/yxIvKUqj7r84SHajevqn7tOey0EjiAe7plx/W2AhHLt4fGLcXd2XdEqrqE/3Qk+cPR5vsD7t1Yf+pyRlVdSNcHLOuoo/oMm6l/x8o5ms/wY9x9OP50NPlewP3N2N+O9v/KlCO38qkj5lXVXwQ0UQf0yE7qdlTgnpSotT5AuQNZvAn2fBD8GYM9HwR/xmDPB90jY2vdLS/Q+wrEZiBURI5ptWwkwXOqYLDng+DPGOz5IPgzBns+6B4ZW+tueYEeWiBEJFTcF465AJeIRIpIqKpWAm8A94pIjIicjvtCnkDsEnebfN0hY7Dn6w4Zgz1fd8nYnfMekdO95H46g2Au7pnoWt/metYl4b4KuhL3ZEUzLV/3yxjs+bpDxmDP110ydue8R7rZcN/GGGO86pGHmIwxxhw9KxDGGGO8sgJhjDHGKysQxhhjvLICYYwxxisrEMYYY7yyAmGMMcYrKxDG+ICInCkim5zOYYwvWYEw3Z6I7BSRc53MoKorVfVYf7y2iKwQkRoRqRCRAhF5Q0TSO/jciSKS749cpuezAmFMB4h7OlIn/URVY3FP2RoLPOxwHtMLWIEwPZaIhIjI7SKyTUQKReQ1EUlqtf51EdknIqUi8i8ROb7VumdF5AkRWSwilcAkz57Kz0TkK89zXvUMzPatb+rttfWs/7mI7BWRPSLyIxFREck50ntS1RLc4/mc1Oq1ZovIBhEpF5HtInKdZ3kM7vlL+nv2PipEpP+RPhdjmlmBMD3ZTcD3cM/a1R/3vL+tJ1RaAhwDpAKfAy8d9vyZwP24J3VZ5Vk2A/gOMBgYAfygne17bSsi3wH+H+4ZCnPo+IyGiEhf4L+Ara0WHwC+i3t+gdnA70VklLpHEL0A2KOqsZ7bHo78uRgDWIEwPdt1wP+qar6q1uIeafNSEQkFUNWnVbW81bqRIhLf6vlvq+qHqtqkqjWeZfNVdY+qFuGeHe+kdrbfVtsZuOcMX6eqVcA9HXgv80WkFCgAkoGfNq9Q1UWquk3dPsA9r/aZ7bxWu5+LMc2sQJiebBDwpoiUiEgJsAH3XM5pIuISkQc9h1nKgJ2e5yS3ev4uL6+5r9X9Ktz9AW1pq23/w17b23YOd5OqxuPeE0kEMptXiMgFIvKJiBR53ucUDn0fh2vzc+lADtOLWIEwPdku4AJVTWh1i1TV3bgPH12E+zBPPJDleY60er6/xsLfS6s/8MCAjj5RVb8GfgX8QdwigL/h7rROU9UEYDH/eR/e3kN7n4sxLaxAmJ4izDN7V/MtFHgSuF9EBgGISIqIXORpHwfUAoVANPBAALO+BswWkWEiEg3c3cnnP4e73+RCIByIAA4CDSJyATC5Vdv9QN/DDp2197kY08IKhOkpFgPVrW5zgXnA34F3RaQc+AQY42n/PJAL7AbWe9YFhKouAeYD7+PubP7Ys6q2g8+v8zz/LlUtx93p/BruzuaZuN9zc9uNwF+A7Z5DSv1p/3MxpoXNKGeMw0RkGPANEKGqDU7nMaaZ7UEY4wARuVhEwkUkEXgIWGjFwQQbKxDGOOM63P0G23CfQXSDs3GM+TY7xGSMMcYr24MwxhjjlRUIY4wxXlmBMMYY45UVCGOMMV5ZgTDGGOOVFQhjjDFe/X++teQIxdW7fQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n",
    "lr_min, lr_steep, lr_valley, lr_slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2049fd-4933-4b17-add1-0f6faa98ef36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.055411</td>\n",
       "      <td>2.889091</td>\n",
       "      <td>0.463165</td>\n",
       "      <td>17.976971</td>\n",
       "      <td>21:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5986b5c3-62ae-4f07-bfc8-a4bf78f5904b",
   "metadata": {},
   "source": [
    "It takes quite a while to train each epoch, so we'll be saving the intermediate model results during the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0bd542-4286-4dc5-9f1a-a4cb68e7fa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/deb/xcube/nbs/examples/mimic/models/caml_lm_recent.pth')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save(path_model/'caml_lm_recent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb019e-ca63-4990-b78a-378328f35611",
   "metadata": {},
   "source": [
    "Let's now load back the `Learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd36306-4dfa-490e-bcb0-726f7b44b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load(path_model/'caml_lm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7b4f8-36da-4f4e-bf8d-3e563c6f8a51",
   "metadata": {},
   "source": [
    "Let's validate the `Learner` to make sure we loaded the correct version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffaf1b6-bb8f-4a8e-960c-116361cf846d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#3) [2.1801044940948486,0.551633358001709,8.847230911254883]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6654e5-c5dc-43c0-81f3-30b14c60c274",
   "metadata": {},
   "source": [
    "Since we have completed the initial training, we will now continue fine-tuning the model after unfreezing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750dd485-240f-4873-90a6-fa0d6681ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6a7a5-6c96-4cf4-b537-6a1a0282a563",
   "metadata": {},
   "source": [
    "and run `lr_find` again, because we now have more layers to train, and the last layers weight have already been trained for one epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c60e3-0744-4e71-a7bd-47565b9a6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n",
    "lr_min, lr_steep, lr_valley, lr_slide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8267739b-1a4b-4361-be98-c0787b9f9a1f",
   "metadata": {},
   "source": [
    "Let's now traing with a suitable learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc69e9-edad-4a8f-b579-0e1c516b7da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.700497</td>\n",
       "      <td>2.601790</td>\n",
       "      <td>0.498114</td>\n",
       "      <td>13.487859</td>\n",
       "      <td>21:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.532751</td>\n",
       "      <td>2.447730</td>\n",
       "      <td>0.516926</td>\n",
       "      <td>11.562066</td>\n",
       "      <td>21:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.438119</td>\n",
       "      <td>2.364874</td>\n",
       "      <td>0.527396</td>\n",
       "      <td>10.642694</td>\n",
       "      <td>21:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.373062</td>\n",
       "      <td>2.317114</td>\n",
       "      <td>0.533622</td>\n",
       "      <td>10.146353</td>\n",
       "      <td>21:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.315276</td>\n",
       "      <td>2.284130</td>\n",
       "      <td>0.538038</td>\n",
       "      <td>9.817142</td>\n",
       "      <td>21:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.294005</td>\n",
       "      <td>2.259134</td>\n",
       "      <td>0.541297</td>\n",
       "      <td>9.574790</td>\n",
       "      <td>21:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.244216</td>\n",
       "      <td>2.240372</td>\n",
       "      <td>0.544038</td>\n",
       "      <td>9.396826</td>\n",
       "      <td>21:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.236868</td>\n",
       "      <td>2.227880</td>\n",
       "      <td>0.546116</td>\n",
       "      <td>9.280175</td>\n",
       "      <td>21:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.209610</td>\n",
       "      <td>2.221307</td>\n",
       "      <td>0.547150</td>\n",
       "      <td>9.219376</td>\n",
       "      <td>21:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.166203</td>\n",
       "      <td>2.220918</td>\n",
       "      <td>0.547269</td>\n",
       "      <td>9.215789</td>\n",
       "      <td>21:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 2.601789951324463.\n",
      "Better model found at epoch 1 with valid_loss value: 2.4477295875549316.\n",
      "Better model found at epoch 2 with valid_loss value: 2.3648736476898193.\n",
      "Better model found at epoch 3 with valid_loss value: 2.3171143531799316.\n",
      "Better model found at epoch 4 with valid_loss value: 2.284130096435547.\n",
      "Better model found at epoch 5 with valid_loss value: 2.2591335773468018.\n",
      "Better model found at epoch 6 with valid_loss value: 2.2403719425201416.\n",
      "Better model found at epoch 7 with valid_loss value: 2.2278804779052734.\n",
      "Better model found at epoch 8 with valid_loss value: 2.2213072776794434.\n",
      "Better model found at epoch 9 with valid_loss value: 2.2209181785583496.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, lr_max=2e-3, cbs=SaveModelCallback(fname=path_model/'caml_lm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3f646-b35f-458e-9f60-7017c8896755",
   "metadata": {},
   "source": [
    "Note: Make sure if you have trained the most language model `Learner` for more epochs (then you need to save that version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e44906-8fd3-457a-b3e5-bc0fd1a04cd3",
   "metadata": {},
   "source": [
    "### Saving the encoder of the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f987e81-281f-4cb0-9d94-1d53317b2dc2",
   "metadata": {},
   "source": [
    "**Crucial:** Once we have trained our LM we will save all of our model except the final layer that converts activation to probabilities of picking each token in our vocabulary. The model not including the final layer has a sexy name - *encoder*. We will save it using `save_encoder` method of the `Learner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286146dd-9e21-4464-9c33-e6df16e8c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder(path_model/'caml_lm_finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df34333-92eb-4e52-8f76-354fa3dad844",
   "metadata": {},
   "source": [
    "This completes the second stage of the text classification process - fine-tuning the Language Model pretrained on Wikipedia corpus. We will now use it to fine-tune a text multi-label text classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c085ce4a-2362-4f6e-9354-2827faa92acc",
   "metadata": {},
   "source": [
    "## `DataLoaders` for the Multi-Label Classifier (using fastai's Mid-level Data API):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c26f724-1d97-4971-b578-1c6f269dd96e",
   "metadata": {},
   "source": [
    "We will now move from language model fine-tuning to calssifier fine-tuning. In notebook 02_code_prediction2.ipynb we used the `DataBlock` API to built the Multi-Label Classifier `DataLoaders` object. In this notebook though we will build it using the Mid-Level Data API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d652206",
   "metadata": {},
   "source": [
    "### Loading Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228c2a5-ec48-4150-873f-d617813be71e",
   "metadata": {},
   "source": [
    "Let's load the Dataframe (stored in file: notes_labelled.csv) that we created in notebook 01_data_extraction2.ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332078b-c3f2-464e-a976-79ecf8e9d890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/deb/xcube/nbs/examples/mimic\u001b[00m\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "├── mimic3_data_extraction.ipynb\n",
      "├── mimic3_full_prediction.ipynb\n",
      "├── mimic3_sample_prediction.ipynb\n",
      "├── \u001b[01;34mmodels\u001b[00m\n",
      "├── train_mimic.py\n",
      "├── train_mimic_old.py\n",
      "└── \u001b[01;36mxcube\u001b[00m -> \u001b[01;34m/home/deb/xcube/xcube\u001b[00m\n",
      "\n",
      "3 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree -L 1 {path} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756fe87a-6df3-4030-a45b-95e7dd607c37",
   "metadata": {},
   "source": [
    "Aside: Some handy linux find tricks:\n",
    "1. https://stackoverflow.com/questions/18312935/find-file-in-linux-then-report-the-size-of-file-searched\n",
    "2. https://stackoverflow.com/questions/4210042/how-to-exclude-a-directory-in-find-command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe278c3-d9d2-45a4-afa8-bc95decf079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008K\t./models/caml_dls_lm_vocab.pkl\n",
      "1.2G\t./models/caml_dls_lm_recent.pkl\n",
      "539M\t./models/caml_after_warmup_sample.pth\n",
      "165M\t./models/caml_dls_clas_sample_10percent_32.pkl\n",
      "1.2G\t./models/caml_dls_lm.pkl\n",
      "180M\t./models/caml_clas_sample_10percent.pth\n",
      "180M\t./models/caml_after3_sample.pth\n",
      "912K\t./models/caml_dls_lm_vocab_recent.pkl\n",
      "193M\t./models/caml_clas_full.pth\n",
      "165M\t./models/caml_dls_clas_sample_10percent_64.pkl\n",
      "1.6G\t./models/caml_dls_clas_full_16.pkl\n",
      "157M\t./models/caml_lm_finetuned_recent.pth\n",
      "165M\t./models/caml_dls_clas_sample_10percent_16.pkl\n",
      "165M\t./models/caml_dls_clas_sample_10percent_128.pkl\n",
      "166M\t./models/caml_lm.pth\n",
      "165M\t./models/caml_lm_finetuned.pth\n",
      "157M\t./models/caml_lm_recent.pth\n"
     ]
    }
   ],
   "source": [
    "# !find -path ./models -prune -o -type f -name \"*caml*\" -exec du -sh {} \\;\n",
    "!find -not -path \"./data/*\" -type f -name \"*caml*\" -exec du -sh {} \\;\n",
    "# !find {path_data} -type f -name \"*caml*\" | xargs du -sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a516a0-f921-4d3c-b06e-7cbe6bded3f4",
   "metadata": {},
   "source": [
    "If you are loading the sample dataset use the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83282980-6d5b-4642-a973-1b0bd544ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = path_data/'caml_notes_labelled.csv'\n",
    "lm_vocab_file = path_model/'caml_dls_lm_vocab.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3da47-de95-4dfb-8585-02d2b69293aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(data_file, dtype= {\n",
    "    'text': str,\n",
    "    'labels': str,\n",
    "    'subject_id': np.int64,\n",
    "    'hadm_id': np.int64\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3aaaa-b44c-4e23-8545-c4df4b650fff",
   "metadata": {},
   "source": [
    "We need to convert the datatypes of the required columns, `text` and `labels`, to str:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9c728-278a-4597-8cef-8ee96abf913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['text', 'labels']] = df[['text', 'labels']].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079da0d-7ac1-4bd0-98c3-f4f88f03d837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>length</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86006</td>\n",
       "      <td>111912</td>\n",
       "      <td>admission date discharge date date of birth sex f service surgery allergies patient recorded as having no known allergies to drugs attending first name3 lf chief complaint 60f on coumadin was found slightly drowsy tonight then fell down stairs paramedic found her unconscious and she was intubated w o any medication head ct shows multiple iph transferred to hospital1 for further eval major surgical or invasive procedure none past medical history her medical history is significant for hypertension osteoarthritis involving bilateral knee joints with a dependence on cane for ambulation chronic...</td>\n",
       "      <td>801.35;348.4;805.06;807.01;998.30;707.24;E880.9;427.31;414.01;401.9;V58.61;V43.64;707.00;E878.1;96.71</td>\n",
       "      <td>230</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85950</td>\n",
       "      <td>189769</td>\n",
       "      <td>admission date discharge date service neurosurgery allergies sulfa sulfonamides attending first name3 lf chief complaint cc cc contact info major surgical or invasive procedure none history of present illness hpi 88m who lives with family had fall yesterday today had decline in mental status ems called pt was unresponsive on arrival went to osh head ct showed large r sdh pt was intubated at osh and transferred to hospital1 for further care past medical history cad s p mi in s p cabg in ventricular aneurysm at that time cath in with occluded rca unable to intervene chf reported ef 1st degre...</td>\n",
       "      <td>852.25;E888.9;403.90;585.9;250.00;414.00;V45.81;96.71</td>\n",
       "      <td>304</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88025</td>\n",
       "      <td>180431</td>\n",
       "      <td>admission date discharge date date of birth sex f service surgery allergies no known allergies adverse drug reactions attending first name3 lf chief complaint s p fall major surgical or invasive procedure none history of present illness 45f etoh s p fall from window at feet found ambulating and slurring speech on scene intubated en route for declining mental status in the er the patient was found to be bradycardic to the s with bp of systolic she was given atropine dilantin and was started on saline past medical history unknown social history unknown family history unknown physical exam ex...</td>\n",
       "      <td>518.81;348.4;348.82;801.25;427.89;E882;V49.86;305.00;96.71;38.93</td>\n",
       "      <td>359</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id  \\\n",
       "0       86006   111912   \n",
       "1       85950   189769   \n",
       "2       88025   180431   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  admission date discharge date date of birth sex f service surgery allergies patient recorded as having no known allergies to drugs attending first name3 lf chief complaint 60f on coumadin was found slightly drowsy tonight then fell down stairs paramedic found her unconscious and she was intubated w o any medication head ct shows multiple iph transferred to hospital1 for further eval major surgical or invasive procedure none past medical history her medical history is significant for hypertension osteoarthritis involving bilateral knee joints with a dependence on cane for ambulation chronic...   \n",
       "1  admission date discharge date service neurosurgery allergies sulfa sulfonamides attending first name3 lf chief complaint cc cc contact info major surgical or invasive procedure none history of present illness hpi 88m who lives with family had fall yesterday today had decline in mental status ems called pt was unresponsive on arrival went to osh head ct showed large r sdh pt was intubated at osh and transferred to hospital1 for further care past medical history cad s p mi in s p cabg in ventricular aneurysm at that time cath in with occluded rca unable to intervene chf reported ef 1st degre...   \n",
       "2  admission date discharge date date of birth sex f service surgery allergies no known allergies adverse drug reactions attending first name3 lf chief complaint s p fall major surgical or invasive procedure none history of present illness 45f etoh s p fall from window at feet found ambulating and slurring speech on scene intubated en route for declining mental status in the er the patient was found to be bradycardic to the s with bp of systolic she was given atropine dilantin and was started on saline past medical history unknown social history unknown family history unknown physical exam ex...   \n",
       "\n",
       "                                                                                                  labels  \\\n",
       "0  801.35;348.4;805.06;807.01;998.30;707.24;E880.9;427.31;414.01;401.9;V58.61;V43.64;707.00;E878.1;96.71   \n",
       "1                                                  852.25;E888.9;403.90;585.9;250.00;414.00;V45.81;96.71   \n",
       "2                                       518.81;348.4;348.82;801.25;427.89;E882;V49.86;305.00;96.71;38.93   \n",
       "\n",
       "   length  is_valid  \n",
       "0     230     False  \n",
       "1     304     False  \n",
       "2     359     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7071b-8f50-42e7-9b82-7c63dfcde216",
   "metadata": {},
   "source": [
    "let's now gather the labels from the 'labels' column of the df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc2ed0-30fc-4f8b-8934-b09873c44594",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq = Counter()\n",
    "for labels in df.labels: label_freq.update(labels.split(';'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c507b7-a753-43b4-9115-6ffcf522a09a",
   "metadata": {},
   "source": [
    "The total number of labels are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3727f7-b3ed-4162-bd4f-7903faced6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8922"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66b73e-dd80-4db0-a07b-2c6ffb592d44",
   "metadata": {},
   "source": [
    "Let's take a look at the most common labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b64e4d-3a39-44ed-9ee5-0b2fbf20dbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>401.9</td>\n",
       "      <td>20053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.93</td>\n",
       "      <td>14444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>428.0</td>\n",
       "      <td>12842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>427.31</td>\n",
       "      <td>12594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>414.01</td>\n",
       "      <td>12179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>96.04</td>\n",
       "      <td>9932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96.6</td>\n",
       "      <td>9161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>584.9</td>\n",
       "      <td>8907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>250.00</td>\n",
       "      <td>8784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>96.71</td>\n",
       "      <td>8619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>272.4</td>\n",
       "      <td>8504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>518.81</td>\n",
       "      <td>7249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>99.04</td>\n",
       "      <td>7147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>39.61</td>\n",
       "      <td>6809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>599.0</td>\n",
       "      <td>6442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>530.81</td>\n",
       "      <td>6156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>96.72</td>\n",
       "      <td>5926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>272.0</td>\n",
       "      <td>5766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>285.9</td>\n",
       "      <td>5296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>88.56</td>\n",
       "      <td>5240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  frequency\n",
       "0    401.9      20053\n",
       "1    38.93      14444\n",
       "2    428.0      12842\n",
       "3   427.31      12594\n",
       "4   414.01      12179\n",
       "5    96.04       9932\n",
       "6     96.6       9161\n",
       "7    584.9       8907\n",
       "8   250.00       8784\n",
       "9    96.71       8619\n",
       "10   272.4       8504\n",
       "11  518.81       7249\n",
       "12   99.04       7147\n",
       "13   39.61       6809\n",
       "14   599.0       6442\n",
       "15  530.81       6156\n",
       "16   96.72       5926\n",
       "17   272.0       5766\n",
       "18   285.9       5296\n",
       "19   88.56       5240"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(label_freq.most_common(20), columns=['label', 'frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ca573-d97b-417e-8681-b383416b59db",
   "metadata": {},
   "source": [
    "Let's make a list of all labels (We will use it later while creating the `DataLoader`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933d579-d557-43b7-88ed-71dc0c9a15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = list(label_freq.keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390ccd9-8d01-4971-be3d-992beda5fe23",
   "metadata": {},
   "source": [
    "### Steps for creating the classifier `DataLoaders` using fastai `Transforms`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd6e92-a962-4263-9f4d-22015ad04140",
   "metadata": {},
   "source": [
    "#### 1. train/valid `splitter`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a53f2-e140-48b3-903f-a41ac8471f26",
   "metadata": {},
   "source": [
    "Okay, based on the `is_valid` column of our Dataframe, let's create a splitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b01d462-834c-4a77-b093-361af5d88ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(df):\n",
    "    train = df.index[~df['is_valid']].tolist()\n",
    "    valid = df.index[df['is_valid']].to_list()\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9291bd4-5a47-4d81-a5e1-e4df06fababb",
   "metadata": {},
   "source": [
    "Let's check the train/valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d55a7-2920-4b2e-a026-ef2b8e123f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [train, valid] = splitter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa4639-d9bf-4a27-a578-c07f569b4c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#49354) [0,1,2,3,4,5,6,7,8,9...],\n",
       " (#3372) [1631,1632,1633,1634,1635,1636,1637,1638,1639,1640...])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(splits[0]), L(splits[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d49c77-b0a6-43cd-9fe9-442e4e606d63",
   "metadata": {},
   "source": [
    "#### 2. Define transforms for the independent and the dependent variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e20d36-b3bb-46e1-b5be-8a89d6c440fe",
   "metadata": {},
   "source": [
    "##### a) Transforms for independent variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a824f83-3618-400c-b476-f076fb27c4d3",
   "metadata": {},
   "source": [
    "We will now `Tokenize` and `Numericalize` the `text` column of the `df`. \n",
    "\n",
    "**Crucial:** We need the vocab of the language model so that we can make sure we use the same correspondence of token to index. Otherwise, the embeddings we learned in our fine-tuned language model won't make any sense to our classifier model, and the fine-tuning won't be of any use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c844701-3360-40de-8adf-fc8d911802cb",
   "metadata": {},
   "source": [
    "So let's load the vocab of the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f940db8-2271-4191-9692-f178cd1c565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_vocab = torch.load(path_model/'dls_lm_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb8647-2480-4154-b07c-1b2e697c3608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#60008) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','*'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(lm_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6426fb",
   "metadata": {},
   "source": [
    "**Crucial**: This is where we pass the `lm_vocab` to the `Numericalize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ecb41-c125-43e9-b4ed-78683fed3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms = [Tokenizer.from_df('text'), attrgetter(\"text\"), Numericalize(vocab=lm_vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec3d30-eb23-4136-a748-da8d9757017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tls_x = TfmdLists(df, x_tfms, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775ac4c-02f3-409a-83eb-2d0ce45fdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tls_x.train), len(tls_x.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02c20e-85de-4b01-8e4a-91c3ac4e9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_text = tls_x.train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e82fe-bd37-41bd-8ebb-cb140889b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f040a-ce82-48de-819c-9b8851823bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tls_x.decode(a_text)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a7060",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c498102",
   "metadata": {},
   "source": [
    "###### Sidebar: Practice Transforms:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8f9d3",
   "metadata": {},
   "source": [
    "`MultiCategorize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872903b-cac8-43b1-938c-bb2e5a3b3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = MultiCategorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1430f7c-38b0-4a3f-979c-db1ac1d97d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = [['b', 'c'], ['a'], ['a', 'c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08829a9-478f-4abb-8255-f76e482d29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.setup(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6be71-880d-424e-bec7-53737e7579e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83d5c4-89d8-497f-80ae-32b5c7a99821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([1, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = cat(lbls[0])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f622a-cee4-4c4c-8c75-4cd0e99f26ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) ['b','c']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.vocab.map_ids(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9917db26-2adb-494f-af88-f134d10e90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = MultiCategorize()\n",
    "tds = TfmdLists([['b', 'c'], ['a'], ['a', 'c'], []], tfms=[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77b737-6f8d-4f08-b418-8dd6ad32a573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dbe1d2-3473-4363-a14e-140a252fb4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([1, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291aa2e-532a-4894-948f-cefd39bd33b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887927d2-c4ee-4b8e-aae6-eb7d1d156ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([0, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6c349-1a97-4f20-80f0-c1f91e4db874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f0db1",
   "metadata": {},
   "source": [
    "`OneHotEncode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54dcdf8-cb1e-432f-8c8f-5eb25e793219",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tfm = OneHotEncode(c=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382516a-32c7-4e4f-bae4-0d158bd24822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncode -- {'c': 3}:\n",
       "encodes: (object,object) -> encodes\n",
       "decodes: (object,object) -> decodes"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d00c05-35d3-4e02-9b41-90008bec8f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([1., 0., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tfm([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c307d2-080d-49c5-9192-c287910a8750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([0., 1., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tfm([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622cfb03-bd1a-4bbf-a89b-1b88707b6291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([0., 0., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tfm([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2545d3b-c65d-425d-b8e9-e6687c0d62a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([1., 1., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tfm([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77804173-3d4c-4f4e-8170-1ff04e6a227c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4542/2991036562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_tfm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/menv/lib/python3.9/site-packages/fastcore/transform.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encodes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'decodes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34mf'{self.name}:\\nencodes: {self.encodes}decodes: {self.decodes}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/menv/lib/python3.9/site-packages/fastcore/transform.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, fn, x, split_idx, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_idx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/menv/lib/python3.9/site-packages/fastcore/transform.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'returns'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mretain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/menv/lib/python3.9/site-packages/fastcore/dispatch.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minst\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/menv/lib/python3.9/site-packages/fastai/data/transforms.py\u001b[0m in \u001b[0;36mencodes\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Couldn't infer the number of classes, please pass a value for `c` at init\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mencodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mTensorMultiCategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mone_hot_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/menv/lib/python3.9/site-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(x, c)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 3"
     ]
    }
   ],
   "source": [
    "_tfm([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [MultiCategorize(), OneHotEncode()]\n",
    "tds = TfmdLists([['b', 'c'], ['a'], ['a', 'c'], []], tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d4fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8785f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([0., 1., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952053b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([1., 0., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a72d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([1., 0., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4c66d",
   "metadata": {},
   "source": [
    "`ColReader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e4521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>1 2 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a      b\n",
       "0  a    1 2\n",
       "1  b      0\n",
       "2  c       \n",
       "3  d  1 2 3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'a': 'a b c d'.split(),\n",
    "    'b': ['1 2', '0', '', '1 2 3']\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1415f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ColReader('a', pref='0', suff='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908234c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pandas(Index=0, a='a', b='1 2'),\n",
       " Pandas(Index=1, a='b', b='0'),\n",
       " Pandas(Index=2, a='c', b=''),\n",
       " Pandas(Index=3, a='d', b='1 2 3')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o for o in df.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a9730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0a1', '0b1', '0c1', '0d1']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f(o) for o in df.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ColReader('b', label_delim=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd479d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2'], ['0'], [], ['1', '2', '3']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f(o) for o in df.itertuples()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7c865",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f334481c",
   "metadata": {},
   "source": [
    "##### b) Transforms for the dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded267ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#8922) ['038.9','785.59','584.9','427.5','410.71','428.0','682.6','425.4','263.9','96.04'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tfms = [ColReader('labels', label_delim=';'), MultiCategorize(vocab=lbls), OneHotEncode()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5ce6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>Admission Date:  [**2101-10-20**]     Discharge Date:  [**2101-10-31**]\\n\\nDate of Birth:   [**2025-4-11**]     Sex:  M\\n\\nService:  Medicine\\n\\nCHIEF COMPLAINT:  Admitted from rehabilitation for\\nhypotension (systolic blood pressure to the 70s) and\\ndecreased urine output.\\n\\nHISTORY OF PRESENT ILLNESS:  The patient is a 76-year-old\\nmale who had been hospitalized at the [**Hospital1 190**] from [**10-11**] through [**10-19**] of [**2101**]\\nafter undergoing a left femoral-AT bypass graft and was\\nsubsequently discharged to a rehabilitation facility.\\n\\nOn [**2101-10-20**], he presented a...</td>\n",
       "      <td>038.9;785.59;584.9;427.5;410.71;428.0;682.6;425.4;263.9;96.04;99.62;89.64;96.72;38.93;96.6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id  \\\n",
       "0           3   145834   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Admission Date:  [**2101-10-20**]     Discharge Date:  [**2101-10-31**]\\n\\nDate of Birth:   [**2025-4-11**]     Sex:  M\\n\\nService:  Medicine\\n\\nCHIEF COMPLAINT:  Admitted from rehabilitation for\\nhypotension (systolic blood pressure to the 70s) and\\ndecreased urine output.\\n\\nHISTORY OF PRESENT ILLNESS:  The patient is a 76-year-old\\nmale who had been hospitalized at the [**Hospital1 190**] from [**10-11**] through [**10-19**] of [**2101**]\\nafter undergoing a left femoral-AT bypass graft and was\\nsubsequently discharged to a rehabilitation facility.\\n\\nOn [**2101-10-20**], he presented a...   \n",
       "\n",
       "                                                                                       labels  \\\n",
       "0  038.9;785.59;584.9;427.5;410.71;428.0;682.6;425.4;263.9;96.04;99.62;89.64;96.72;38.93;96.6   \n",
       "\n",
       "   is_valid  \n",
       "0     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tls_y = TfmdLists(df, y_tfms, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81efdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49354, 3372)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tls_y.train), len(tls_y.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5168ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([1., 1., 1.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_label = tls_y.train[0]\n",
    "a_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c703f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['038.9', '785.59', '584.9', '427.5', '410.71', '428.0', '682.6', '425.4', '263.9', '96.04', '99.62', '89.64', '96.72', '38.93', '96.6', '042', '136.3', '799.4', '276.3', '790.7', '571.5', '041.11', 'V09.0', 'E931.7', '88.72', '33.23', '403.91', '444.0', '997.2', '276.6', '276.7', '285.9', '275.3', 'V15.82', '55.69', '91.0', '39.57', '38.06', '99.04', '431', '507.0', '276.5', '401.9', 'V30.00', '774.2', '765.25', '765.15', 'V29.0', '99.83', '99.15', '191.3', '15.9', '11.3', '92.29', '99.25', '157.0', '574.10', '997.1', '998.11', '568.0', '553.21', 'E878.2', '51.37', '52.12', '54.59', '53.51', '99.60', '54.12', '96.71', '414.01', '411.1', '250.00', '272.0', '36.12', '36.15', '39.61', '37.61', '423.9', '511.9', '785.51', '458.9', '311', '722.0', '719.46', '272.4', '37.31', '745.5', '458.29', 'V12.59', '35.71', '250.80', '780.39', '296.33', 'V58.67', 'E932.3', 'V58.69', '478.29', '780.57', '783.21', '426.4', 'V17.0', '93.9', '89.14', '94.25', '805.02', '599.0', '596.4', 'E880.9', '822.0', '733.00', '294.8', '443.21', '578.1', '459.2', '427.31', '427.1', '285.21', '112.2', '274.9', 'V10.46', '438.89', '66.0', '39.5', '36.06', '42.0', '47.0', '38.95', '39.95', '37.23', '88.56', '97.44', '038.8', '785.52', '707.09', '682.3', '998.59', '008.45', '572.0', '995.92', 'V09.80', '185', '443.9', '244.9', 'E878.8', '89.61', '14.0', '967.8', '969.3', 'E950.2', 'E950.3', '348.8', '296.20', '96.34', '225.2', '348.5', '424.1', 'V45.81', 'V45.79', '15.1', 'V45.82', '600.00', '389.9', '36.13', '99.05', '99.07', '410.41', '530.81', '36.01', '36.07', '88.52', '99.2', '250.11', '37.22', '99.20', '88.53', '996.04', '412', '593.9', '37.94', '89.59', '496', '426.3', '300.29', 'V17.4', '36.14', '38.91', '996.74', '416.0', '428.1', '86.01', '88.55', '92.27', '345.3', '201.90', '515', '486', '362.50', 'E933.1', '437.1', '11.4', '33.1', '519.1', '453.8', '278.01', '31.5', '31.79', '31.42', '31.99', '33.24', '492.8', '493.90', '427.89', '428.22', 'V58.61', '365.9', '37.72', '37.83', '89.45', '396.2', '426.0', '287.5', '427.32', '715.90', '35.23', '35.21', '36.11', '37.33', '998.31', '415.11', '996.72', 'V10.51', '34.79', '77.61', '39.32', '34.03', '86.74', '78.51', '518.5', '997.39', '518.0', '414.00', '300.00', '53.61', '78.61', '54.4', '99.77', '305.1', '596.8', '88.45', '535.51', '285.1', '45.13', '608.83', '998.2', '997.4', '560.1', '608.4', 'E870.8', '62.3', '31.1', '61.3', '43.19', '769', '770.8', '765.19', 'V05.8', 'V05.3', '93.90', '99.14', '99.55', '191.7', '513.0', '482.41', '707.05', '112.4', '482.82', '788.20', '512.1', '707.03', 'V10.05', 'V10.47', '709.9', '12.4', '43.11', '33.26', '427.41', '438.20', '438.19', '37.26', '37.34', '37.27', '805.2', 'E821.0', '81.04', '81.05', '77.89', '80.51', '77.79', '81.62', '84.51', '81.63', '39.0', '413.9', '596.0', '396.8', 'V17.3', '57.92', '824.9', '998.4', '873.0', '873.44', '911.0', '913.0', 'E812.2', '79.36', '79.66', '78.17', '86.69', '78.67', '86.59', '576.8', '203.00', '996.59', '280.9', '281.1', '228.09', '97.05', '51.85', '98.2', '88.47', '293.9', '403.90', '564.09', '580.89', '571.2', '070.54', '572.8', '284.1', '54.91', '410.21', '305.62', '433.10', 'V10.11', 'V45.02', '61.0', '63.0', '45.0', '40.0', '198.3', '162.8', '531.40', '276.1', '585.6', '041.4', '787.01', 'V45.01', '285.29', '96.07', 'V34.01', '765.18', '765.27', '202.80', '288.0', '999.8', '579.0', '693.0', '41.04', '284.8', '996.85', '569.83', '567.2', '276.2', '995.91', '112.0', '255.4', '351.0', '088.0', '41.31', '86.11', '88.43', '112.5', '038.49', '710.0', '714.0', '719.45', '574.20', '414.8', 'E888.9', 'E849.8', '780.8', '307.9', '038.11', '304.00', '511.1', '789.5', '711.07', '711.04', '421.0', '800.12', '305.00', '305.60', 'E814.7', '781.2', '378.54', '951.8', '765.28', '770.89', '779.3', '767.8', '766.1', 'V29.3', '22.', '276.8', '46.75', '570', '585.9', '428.20', 'V14.1', 'V07.1', '99.1', '99.12', 'V30.01', '765.17', '764.07', '775.6', '776.4', '99.01', '969.5', '295.90', '266.2', '746.6', '779.81', '767.19', '747.0', '746.89', '410.01', 'V10.42', '997.3', '997.09', '355.8', '553.3', '39.64', '34.04', '410.61', '272.1', '274.0', '965.00', '304.01', '969.4', 'E850.2', 'E853.2', '410.11', '426.89', '804.16', '873.42', 'E916', '202.10', '398.91', '396.3', '707.15', '39.50', '39.90', '88.41', '191.9', 'V42.81', '238.75', '300.4', '600.01', '788.43', '327.23', '332.0', 'V10.79', 'V43.3', '585.3', '38.97', 'V42.82', 'E879.8', '441.2', '38.45', '745.61', '424.0', '35.12', 'V31.01', '765.24', '770.7', '777.5', '772.11', '770.81', '776.6', '38.92', '861.13', 'E966', '37.4', '765.16', '228.01', '200.08', '555.9', '998.12', '518.81', '447.0', '135', '531.00', '286.6', '44.43', '765.02', '771.81', '362.21', '765.03', '765.23', '270.7', '38.85', '335.20', '518.84', '723.4', '079.99', '788.41', '851.81', '807.2', 'V46.1', '864.02', '865.09', '482.1', '860.4', '31.29', '76.74', '76.92', '79.32', '79.62', '50.61', '41.95', '11.8', '38.7', '440.1', '437.3', '39.51', '782.0', 'V10.83', '600.0', '996.02', '441.1', '35.22', '38.44', '482.40', '348.1', '780.03', '765.26', '752.61', '752.63', '162.3', '255.9', '764.08', '765.29', '774.6', 'V50.2', '778.4', '64.0', '780.6', '333.4', '276.51', '790.6', '276.0', '305.50', '288.8', '275.2', 'V02.62', '532.91', '996.81', 'V45.11', '252.00', '535.50', '535.60', '530.85', '428.33', '403.11', '278.00', 'E878.0', '88.48', '403.01', '560.9', '583.81', '355.9', '789.04', '338.29', '443.89', 'V02.54', 'V45.12', 'V15.81', '45.16', '48.1', '364.3', '437.2', '585.5', '041.19', '582.81', '432.9', '425.1', '446.6', '729.92', 'V12.51', 'V58.65', '789.07', '999.2', '451.84', '789.00', '345.90', '564.00', '790.92', '518.89', '786.59', '780.60', '782.1', '416.8', '401.0', '789.59', '789.09', '789.06', '787.91', '54.11', '787.02', 'E942.6', '959.6', 'E928.9', 'V45.1', '275.41', '588.81', '531.90', 'V12.54', '799.02', 'E935.2', 'V45.89', 'V64.2', 'V45.78', '724.3', '996.56', '995.1', '287.4', 'E878.1', '54.95', '97.82', '348.39', '283.9', '784.2', '611.72', '338.19', '733.99', '622.11', '996.62', '289.81', '041.04', 'E879.1', '041.12', '965.09', '780.97', '784.0', 'V49.83', 'V45.69', '795.00', '251.8', 'E932.0', '54.98', '558.9', '228.04', '404.92', '360.19', '578.0', '588.89', '443.0', '55.23', '99.71', '423.2', '99.6', '360.01', '112.89', '379.00', '376.32', '370.05', '364.00', '16.49', '786.1', '793.1', '88.67', '079.89', '776.1', '778.8', 'V72.1', '491.21', '410.81', '725', '191.5', '331.4', '23.9', '482.83', '194.0', '197.7', '255.3', '584.5', '682.2', '349.82', 'V10.00', 'V45.3', '041.3', '72.2', '50.22', '93.59', '34.91', '96.49', '86.04', '39.1', '070.44', '572.4', '511.8', '286.7', '518.82', '280.0', '303.93', '572.3', '070.70', '447.8', '414.02', '88.42', '440.21', '585.4', '440.0', 'V12.09', '38.18', '39.9', '41.0', '44.0', '560.89', '45.73', '46.21', '54.72', '433.31', '436', '433.20', '38.12', '996.71', '772.6', '779.89', '198.89', '197.2', 'V10.3', '37.0', '37.21', '853.00', '730.28', '77.81', '34.3', '86.22', '78.41', '83.82', '250.60', '357.2', '88.26', '772.14', '430', '89.6', '35.33', '721.41', '214.8', '724.01', '30.9', '437.5', '39.28', '242.90', '99.61', '189.0', '55.51', '40.3', '599.7', 'E934.2', '55.4', '530.7', '801.20', '434.91', 'V66.7', '155.0', '303.90', '592.0', '50.59', '93.0', '852.21', '839.00', 'E881.0', '13.1', '579.3', '720.2', '733.42', 'V44.2', '441.01', '569.62', '557.0', '576.1', '557.1', '39.54', '39.26', '45.62', '45.74', '45.79', '46.41', '44.39', '801.11', '871.1', '958.7', 'E812.0', '802.4', '802.29', '802.1', '802.6', '847.0', '957.0', '76.76', '76.79', '43.0', '76.69', '21.2', '16.09', '76.91', '20.2', '85.9', '93.55', '238.7', '428.30', 'V10.82', 'E879.9', '518.3', '397.0', '737.10', '415.19', '196.1', '423.8', '289.82', '346.90', 'V10.41', '34.22', '33.22', '34.25', '40.29', '36.05', '37.64', '997.5', '441.4', 'V13.01', '89.68', '97.64', '864.01', '802.8', '805.4', 'E968.9', '873.49', '441.03', '510.9', '286.9', '293.0', '13.', '14.', '39.59', '96.05', '34.51', '99.00', '780.79', 'V58.43', '731.0', '787.2', '438.11', '438.0', '852.22', '250.82', '432.1', '784.3', 'E885.9', '785.0', '790.01', '532.40', '493.20', '745.0', '812.20', '729.1', '156.2', '567.22', '433.30', '51.62', '51.22', '51.14', '696.1', '428.40', '501', '494.0', '577.1', '13.0', '995.94', '410.72', '33.28', '17.0', '428.23', '787.3', '275.49', '712.30', '599.70', 'V70.7', '296.7', '215.6', '754.70', '753.29', '99.69', '99.29', '508.1', '996.79', '32.01', '97.49', '203.01', '211.3', '88.51', '45.42', '860.0', '876.0', '42.23', '070.20', '305.61', '305.51', '285.8', '296.80', '806.04', 'V42.2', '807.01', '428.31', '707.07', '273.8', 'V54.17', '427.81', '578.9', '410.91', '530.10', '556.9', '394.9', '86.67', '54.0', '444.89', '998.81', '807.00', '924.00', 'E960.0', '514', '764.97', '763.83', '39.63', '218.1', '242.00', '68.29', '959.01', '359.4', '191.6', '348.30', '456.0', '340', '571.49', '276.52', '456.8', '42.33', '96.33', '038.3', '451.82', '331.0', '294.10', '996.82', '255.41', '456.21', '041.09', 'V10.07', '324.1', '999.31', '730.08', '722.93', '333.99', 'E939.0', '537.89', '737.30', '84.52', '50.11', '155.2', '780.71', '87.54', '99.06', '455.2', '997.31', '807.02', '455.5', 'E884.4', 'E849.7', '45.24', '86.05', 'V42.7', '995.93', '724.2', '746.4', '766.2', '038.42', '591', '038.0', '592.1', '55.03', '59.8', '55.93', '87.75', '250.01', '458.2', '37.78', '99.81', '575.0', '356.9', '368.8', '45.8', '552.8', '39.72', '785.6', '500', '40.11', '904.41', '928.11', '821.21', '823.00', 'E821.7', '958.92', '338.11', '39.29', '39.98', '83.61', '83.09', '83.65', '79.05', '150.5', '998.3', '42.42', '46.39', '17.', '35.24', '573.3', '398.90', '861.21', '070.51', '850.5', '807.4', 'E968.2', '250.40', 'V49.75', 'V49.72', '560.2', 'E938.3', 'V53.31', 'V55.6', '251.1', '56.52', '45.93', '728.88', '786.50', '851.42', '958.4', '482.30', '870.0', '873.43', '88.1', '27.51', '162.5', '198.5', 'V87.41', 'V15.3', '458.8', '440.20', '562.10', '788.30', '444.22', '440.22', 'V64.42', '32.39', '562.12', '162.9', '45.23', 'V60.0', '521.00', '301.0', '23.19', '164.2', '197.1', '423.0', '786.3', '054.9', '821.23', '998.0', '79.35', '78.15', '35.96', '253.6', '078.5', '577.0', '250.41', '741.90', '344.1', '34.1', '041.6', '784.7', '86.28', '81.91', '273.2', '88.64', '482.2', '289.83', '287.31', '482.0', '288.60', '537.82', '569.49', '46.0', '21.01', '464.51', '51.0', '35.14', 'V64.1', '37.66', '250.02', '50.3', '882.2', '852.20', '998.89', '957.9', 'E815.0', '86.62', '82.45', '82.29', '33.21', '493.22', '428.41', '466.0', '792.1', '88.57', '569.85', '434.11', '342.80', '784.51', '39.74', '153.4', '428.21', '198.2', '998.13', 'V49.86', '599.71', '45.25', '038.2', '481', '197.0', '32.29', '11.', '910.0', '456.20', '572.2', '453.41', '867.0', '348.31', '579.8', '070.1', 'E879.6', '357.5', '60.29', '041.86', '442.2', '39.71', '39.79', '441.3', '996.1', 'V15.07', '770.6', '150.8', '282.49', '250.73', '250.63', '250.43', '250.53', '785.4', '84.15', '99.10', '345.50', '428.32', '274.01', '070.32', '567.89', 'E849.0', '577.8', '54.19', '51.1', '97.56', 'V08', '37.99', '577.2', '574.00', 'E850.0', 'E854.3', 'E854.1', '965.01', '304.21', '070.30', '292.0', '304.31', '970.8', '969.6', '94.65', '44.32', '331.82', '530.20', '156.1', '51.87', '765.14', '485', '235.6', '996.44', '438.9', 'V43.64', 'V43.65', '372.72', 'V44.0', '806.21', '806.05', 'E882', '813.41', '806.25', '127.0', '839.61', '368.40', '719.41', '571.1', '35.3', '83.95', '81.03', '79.02', '81.64', '510.0', '440.24', '492.0', '112.1', '34.92', '99.21', '394.1', 'V44.1', '84.11', '198.4', '197.3', '160.3', '16.0', '22.63', '22.42', '324.0', '996.69', '170.0', '12.3', '83.45', '747.3', '750.9', 'E878.6', '43.99', '33.41', '410.31', 'E879.0', '724.00', '35.51', '292.81', 'E937.9', '999.9', '200.00', '45.19', '288.03', '53.9', '773.1', '576.2', '724.5', 'V10.09', '965.4', '305.90', 'E950.0', 'E849.9', '314.01', 'V65.2', '852.46', '38.04', '447.1', '348.4', '379.92', '13.9', '435.9', 'V10.52', 'E930.8', '568.82', '199.1', 'V45.73', '966.1', '758.89', 'E855.0', '741.01', '918.1', '724.8', '805.07', '305.91', '305.01', '89.60', '996.01', '733.82', '78.65', '78.05', '820.22', '997.62', 'V42.0', '369.4', '250.71', 'V49.76', '369.3', '536.3', '250.93', '369.00', '250.51', '152.0', '196.2', '52.7', '304.71', '970.1', '571.8', '97.55', '51.84', '997.02', 'E942.1', '752.51', '593.81', '568.81', '459.0', '574.21', 'V11.3', 'V63.2', '579.9', '89.62', '574.90', '553.1', '783.7', '52.01', '87.53', '53.49', '780.09', '790.5', '386.11', '88.91', '250.10', '623.8', '290.3', '041.00', '204.10', '39.62', '745.4', '611.0', '715.96', '444.81', '440.30', '39.49', '38.48', '38.08', '440.29', '275.42', '250.50', '362.01', '39.27', '722.92', '440.23', '682.7', '585', '707.14', '805.04', '996.4', '344.00', '93.42', '707.24', '788.5', '345.01', '518.83', '530.82', '038.10', '473.0', '11.0', '795.89', '573.9', '96.06', 'E910.2', '342.90', '008.8', '790.29', '396.0', '33.27', '758.0', '057.9', '780.52', '041.7', '426.11', '053.79', '276.50', '491.20', '271.3', '283.0', '821.11', '823.82', 'E819.0', '79.15', '79.65', '79.06', '852.02', 'E849.5', '811.00', '813.05', '884.0', 'E813.6', '751.5', '562.13', '455.0', '455.3', '136.9', '323.6', '780.2', '250.61', '583.89', '366.9', '50.0', '296.89', '290.40', '437.0', '297.1', '588.1', 'V62.5', 'E939.8', '458.21', '323.8', '276.9', '253.5', '344.89', '794.00', '793.5', '318.1', '30.3', '761.5', '936', 'E915', '98.04', '457.0', '395.0', '053.19', '38.14', '36.03', '117.9', '358.1', '250.62', '801.01', '950.0', '922.1', '458.0', '12.81', '21.72', '904.1', '426.13', '416.9', '86.07', '517.8', '289.84', '511.0', '473.9', '12.0', '346.80', '89.63', '763.0', '691.0', '205.10', '38.46', '54.61', '54.25', '518.4', '518.7', '595.82', '909.2', 'E934.7', 'E879.2', '722.10', '990', 'V88.01', '593.3', '569.3', '009.0', '619.1', '569.2', '338.4', '788.29', 'V54.19', 'V44.3', '87.61', '569.81', '54.92', '490', '343.9', '319', '574.50', '51.88', '281.9', '204.00', '288.00', '359.81', '357.82', '998.32', '567.21', '567.82', '357.0', '285.22', '46.23', '196.0', '33.91', '130.9', '695.1', '707.19', '459.81', '698.3', '786.09', '289.0', '553.29', '53.69', '57.94', 'E931.0', '35.39', '478.31', '39.22', '864.05', '851.80', 'E823.0', '426.82', '68.9', '426.7', '790.99', '794.39', '770.5', '770.82', '478.1', '771.6', '252.0', '246.9', '552.21', '37.12', '424.90', '157.3', '50.12', '51.51', '44.42', '51.98', '51.12', 'E885.1', '860.2', '807.09', '823.80', '891.0', '873.8', '820.32', '808.3', '805.6', '866.12', '823.02', '821.01', '81.45', '81.46', '902.0', '46.73', '054.71', '117.3', '484.6', '41.5', '707.01', '715.35', '966.3', 'E950.4', '707.06', '730.22', '729.9', '528.9', '530.3', '303.03', '721.3', '88.76', '99.0', '96.08', '320.2', '780.9', '041.89', '723.0', '289.7', '771.89', '772.4', '685.1', '34.09', '746.02', '733.90', '303.91', '719.47', '760.72', '162.4', '999.1', '39.65', '32.28', '88.73', '482.89', '785.2', 'V04.89', '99.59', '532.60', '789.2', '519.4', '775.0', '493.92', '996.86', 'V42.83', '268.9', '530.12', 'V10.44', 'E931.8', '291.81', '473.8', '96.56', '487.1', '780.01', '117.7', '484.7', '357.81', '276.4', '417.8', '279.49', '710.9', '257.2', '733.19', 'V10.53', 'V45.4', '574.91', '51.02', '800.70', '707.0', '980.0', 'E860.0', '788.39', '94.62', '138', '775.5', '770.2', '288.01', '775.81', '764.93', '295.32', '780.4', '424.2', '755.26', '764.92', '747.83', '333.94', '378.52', '991.6', '922.8', '923.8', '296.34', 'E957.1', '996.64', '982.8', 'E950.9', '567.38', '560.81', '45.61', '151.0', '805.01', '444.1', '96.48', '414.11', '414.2', 'V85.35', '426.2', '37.95', '286.2', '305.70', '969.7', '980.9', 'E980.4', '197.8', '291.0', '86.83', '37.82', '37.71', '300.11', '345.40', '530.2', '241.1', '562.11', 'V45.2', '10.2', '823.32', 'E980.9', '781.3', 'V10.85', '682.4', '88.49', '760.8', '801.26', '868.19', '810.00', '802.0', '808.2', '807.05', 'E816.0', '574.71', '51.21', '51.24', '51.10', '969.8', '86.09', '39.31', '806.07', 'V15.5', '81.02', 'V12.03', '95.43', '920', 'E888.8', '530.4', '519.2', '42.89', '42.82', 'V46.11', '333.2', '874.02', '800.09', '824.8', '756.51', '812.01', '79.26', '86.63', '200.28', '009.3', '293.83', '535.61', '530.21', '289.59', '575.9', '782.4', '41.32', '854.05', '868.03', '872.00', '54.63', '18.4', '807.03', '810.03', '707.21', '536.41', '536.42', '707.22', '041.85', '765.22', '770.84', '041.84', '261', '447.6', '429.9', '364.9', '775.7', '550.92', '53.1', '778.3', '574.51', '715.98', '866.02', '808.49', '808.0', '865.03', 'E819.2', '824.2', '79.39', '87.76', '047.9', '054.3', '083.9', '831.00', '812.00', '79.71', '820.8', '839.01', '835.01', '820.21', 'E884.2', '462', '765.13', '721.1', '723.6', '81.01', '336.3', 'E947.8', '756.10', 'V16.0', '34.0', '77.49', '33.0', '593.2', '410.82', '88.71', '571.6', '14.74', '16.31', '99.22', '51.69', '722.72', '733.13', '996.73', '733.09', '737.41', '338.3', 'E866.8', 'V15.84', '733.11', '77.42', '753.3', '403.10', '753.15', '753.22', '345.91', '796.3', '537.83', '530.84', '42.24', '31.44', '253.9', '766.21', '036.2', '036.0', '730.07', '362.81', '041.10', '112.3', '83.39', '354.2', '427.0', '255.0', '701.9', '754.53', '761.7', 'V06.3', 'V03.81', 'V03.82', '99.39', '99.41', '99.52', '227.0', '530.89', '530.19', '535.10', '614.6', '42.41', '43.5', '42.52', 'V58.66', '446.0', '533.90', '736.79', '47.09', '519.19', '414.04', '942.24', 'E924.8', '728.89', '348.3', '240.9', '250.13', '564.1', '721.0', '771.7', '737.43', 'E927', '851.86', '977.8', '787.03', '96.70', '447.2', '779.0', '759.9', '768.9', '038.12', '577.9', '965.02', 'E850.1', 'E853.8', '731.8', '84.12', '038.40', '730.27', '377.41', '86.4', '77.69', '38.21', '529.0', '250.70', '443.81', '923.00', '924.21', '97.88', '853.06', '805.05', '79.31', '533.40', '83.21', '482.49', '807.08', '681.00', '516.3', '487.0', '428.43', '718.47', '99.19', '295.70', '962.3', 'E858.0', '300.3', '803.25', '484.1', '799.0', '480.9', '674.54', '852.00', '441.00', '747.10', '420.90', 'V12.2', 'V12.71', '426.12', '790.4', '37.75', '48.23', '442.3', '317', '709.8', '582.1', '39.41', '77.88', '733.15', '453.52', '555.2', '99.09', '162.2', '34.73', '112.84', '088.81', '711.80', '314.00', '574.60', 'V64.4', '996.67', '711.05', '454.2', 'V64.41', '211.4', '45.76', '441.02', '441.7', '84.3', '801.22', '331.3', 'E880.1', '45.75', '46.11', '996.2', '24.2', '935.1', '530.0', '98.02', '770.83', '48.0', '537.84', '354.3', '796.0', '794.31', '567.23', '204.11', '840.4', '771.83', '93.96', 'E816.1', '724.02', '36.02', '455.8', 'V55.0', '44.62', '568.89', '793.6', '960.4', '967.1', '45.43', '586', '764.02', '438.83', '933.1', 'E911', '780.51', '575.12', '560.0', '51.79', '760.77', '742.1', 'V61.49', '89.38', 'E930.5', '288.50', 'E888', '197.6', '89.49', '453.42', '573.8', '998.6', '567.29', '575.8', '783.0', '273.0', '40.24', '99.28', '46.32', '453.2', '420.91', 'V12.01', '196.9', '597.89', '153.6', '350.1', '40.2', '44.1', '614.4', '38.67', '38.07', '730.25', '344.61', '970.81', '907.2', 'E929.0', '305.03', '730.09', '566', '86.72', '730.05', '730.15', '707.8', '719.05', '747.42', '820.09', '81.52', 'E980.3', 'E980.0', '603.9', '405.01', '786.6', '723.1', '36.04', '348.9', '054.79', '049.0', '202.88', 'E933.0', 'V45.86', '575.11', 'V85.4', '44.31', '786.52', '535.40', 'E935.9', '958.1', '823.20', '83.14', '277.30', '277.3', '459.9', '852.06', '865.00', '868.04', '880.02', '879.4', '777.8', '464.10', '550.12', '558.3', '152.2', '410.51', '323.9', 'V02.51', '274.82', '51.04', '453.3', '772.10', '325', '776.5', '433.01', '536.49', '805.00', '37.36', '235.2', '456.1', '934.9', '516.31', '386.00', '721.90', 'V45.71', 'V46.2', '039.1', '153.8', '865.01', '835.00', '79.75', '303.01', '994.2', '411.89', '553.20', '48.24', '250.92', '379.41', '585.2', '87.51', '666.32', '666.12', '648.22', '97.75', '429.3', 'V11.1', '595.89', '57.49', '57.0', '867.8', '902.53', '870.8', '38.88', '996.65', '296.24', '038.43', '54.74', '618.0', '281.0', '478.25', '70.50', '70.71', '304.41', '425.5', '996.61', '070.22', '707.11', '305.20', '975.5', '806.4', '975.4', 'E858.6', '971.1', '999.39', '567.9', '713.2', '46.10', '22.1', '945.36', '958.3', '052.1', '289.9', '948.00', 'E924.1', '446.5', '87.21', '801.25', '802.24', '812.21', '527.2', '582.9', '309.81', '50.91', '567.8', '997.01', '50.4', '93.', '801.00', '813.42', '802.20', '815.02', '79.03', '764.94', '478.4', '433.21', '378.51', '32.4', '51.01', '519.01', '97.23', '44.61', '30.29', 'E884.9', 'E934.4', '298.9', '81.08', '155.1', '51.11', '719.02', '191.2', '349.89', 'V55.1', '453.40', 'V12.02', '17.55', '009.2', '620.8', '528.0', '716.90', '410.42', '250.12', '996.76', '681.10', 'E935.8', '110.3', '605', '607.82', '75.0', '437.7', '599.4', '435.8', '214.9', '60.94', '44.5', 'V10.04', '461.9', '704.8', '89.03', 'V53.32', '786.02', '707.20', 'V12.53', '53.0', 'E939.7', 'E937.8', 'V18.0', '729.73', '54.62', '51.03', 'V10.03', '812.44', 'E812.1', '865.04', '569.1', '96.26', '275.0', '34.24', '535.01', '764.06', '706.1', '565.0', '277.4', '298.4', '394.0', '853.05', '322.9', '55.24', '92.0', '252.1', '154.1', '244.0', 'V10.87', '040.89', '753.12', '452', '573.4', '823.30', '824.7', '825.35', '041.02', '78.57', '78.59', '79.37', '79.67', '32.6', '457.8', '34.6', '34.21', '44.13', '552.3', '318.0', '756.19', '53.7', '44.66', '156.9', '790.09', '403.00', '410.02', '250.90', 'V01.7', '205.00', '295.92', 'V12.72', '574.40', '38.86', '482.9', 'V16.3', 'V12.79', '402.91', '429.79', '34.01', '290.0', 'V15.88', '87.03', '803.01', '869.0', '873.20', '21.81', '824.1', '851.85', 'E912', '31.74', '762.2', '440.31', '730.17', '682.8', '358.00', '93.56', '839.02', '238.4', '574.30', '188.2', '57.71', '56.51', '590.80', '599.1', 'E852.8', '401.1', 'V10.72', '557.9', 'E870.0', '886.0', 'E919.8', '84.22', '747.82', '32.5', '225.0', '22.0', '300.02', '183.0', '862.9', '864.15', '868.13', '875.0', 'E815.2', '860.1', '877.0', '890.0', '83.32', '250.83', 'V12.55', 'E944.4', '279.00', 'V02.61', '867.1', '824.0', '80.16', '83.02', '157.8', '707.10', '722.91', '516.8', '008.69', '429.0', '21.02', '713.5', '865.11', 'E819.1', '426.52', '58.22', '14.24', '39.30', '38.80', '51.83', '715.95', '755.63', '81.51', '965.1', '296.23', '881.22', 'E956', '388.32', '434.01', '411.0', '410.92', '853.01', 'E936.1', '372.00', '157.1', '52.59', '52.96', 'V44.4', '57.32', '237.3', '72.1', '52.9', '83.31', '41.42', '211.5', '287.3', '038.19', '807.07', '550.90', '273.4', 'V16.1', '996.57', '21.09', '519.02', '478.74', '478.6', '800.21', 'V55.3', '799.1', '46.52', '45.94', '860.3', '879.6', '959.8', '34.93', '71.71', '270.0', '211.6', '801.16', '801.10', 'E829.8', '33.48', '98.15', '753.4', '433.11', '891.1', '758.6', '778.6', '262', '348.89', 'E929.8', '787.22', 'E939.4', '743.61', '414.10', '37.32', '934.1', '39.56', '23.09', '422.91', '33.93', '707.23', '337.3', '423.3', '440.4', '37.49', '286.4', '770.0', '569.42', '625.9', '789.01', '728.85', '80.99', '569.0', 'E849.3', '905.3', 'E969', 'E884.6', '457.1', 'E816.2', '20.6', '807.04', '536.8', '437.8', '218.9', '620.2', '46.20', '304.91', 'E826.1', '88.54', '345.70', '45.41', '762.6', 'E818.7', '764.98', '235.5', '52.52', '200.02', '292.85', '617.9', '753.0', '512.0', '429.83', '802.21', '802.26', '802.27', 'E958.0', '76.75', '156.0', '575.5', '368.13', '754.50', 'V12.3', '93.53', '852.01', '881.01', '801.30', '765.04', '771.8', '748.60', '968.4', '292.84', 'E855.1', '292.11', '333.92', 'E854.2', 'E939.2', 'V14.6', 'V49.81', 'V15.1', '887.2', '444.21', '38.03', '375.55', '804.22', '805.06', '537.0', '334.1', '804.35', 'E848', '873.1', '33.39', '813.44', '866.00', '153.9', 'V58.1', '15.0', '803.20', '808.42', 'E918', '532.00', '356.8', '730.18', '512.8', '63.2', '996.03', '902.29', '807.06', '813.01', '815.00', '873.63', '714.30', '291.1', '284.9', '821.33', '598.8', '707.13', '283.19', '54.51', '711.01', '730.04', '567.31', '720.9', 'V55.2', '86.73', '80.11', '150.9', '851.82', 'E919.4', '84.01', '305.30', 'V02.59', 'V12.04', '482.42', '296.50', 'V13.02', '519.09', '53.59', '226', '62.0', '220', '46.01', '48.63', '65.49', '477.9', '793.11', '212.7', '35.5', '600.91', '228.02', '998.83', '43.41', '44.44', '478.75', '333.91', '38.65', '79.01', 'E878.9', '465.9', '357.8', '997.69', '37.89', '37.77', '433.00', 'V10.06', 'V17.49', '171.3', '719.06', '737.34', '738.4', '905.1', '770.1', '519.3', '389.8', '362.74', 'V10.02', '969.0', '873.52', '784.1', '808.41', '310.2', '189.2', '54.21', '39.25', '863.44', '799.3', '801.14', 'E834.8', '22.79', '21.71', '478.30', '31.75', '755.67', '96.23', '438.82', '357.4', '997.71', '719.7', '729.72', '286.1', '748.4', '242.01', '196.8', '23.4', '192.1', 'E936.3', '425.3', '562.01', '294.9', '164.8', '78.2', '762.1', 'E944.3', '596.54', '682.0', '035', 'V31.00', '765.21', 'E942.9', '764.96', '41.2', '22.19', '999.3', '851.01', '359.9', '692.9', '28.0', 'V58.83', '92.04', '852.12', '801.02', '921.1', '144.8', '27.49', '40.42', '27.24', '27.59', '43.', '300.01', 'V49.73', '24.0', '996.09', '443.29', '784.5', '760.79', '997.79', 'V54.89', '78.55', '63.', '61.', '754.79', '96.35', '716.91', '726.2', '429.71', '745.8', '747.49', '35.61', '62.5', '53.02', 'E819.7', '569.82', '569.69', '97.03', '965.8', '238.71', '530.5', '526.4', 'V10.89', 'E850.8', '728.84', '755.57', '253.0', '355.6', '726.91', '715.94', '84.02', '40.7', '81.75', '77.64', '82.69', '82.56', '81.74', '153.3', '65.61', '93.57', '253.1', 'E935.1', 'E876.8', '260', '304.70', '786.03', '987.9', 'E890.2', '345.80', '834.01', '041.49', '79.74', '93.44', '88.93', '522.5', 'V54.13', '782.3', '250.42', '537.9', '583.9', '707.12', '047.8', '049.8', '88.74', '552.20', '762.4', '996.63', '952.04', '952.09', '358.8', '310.9', '793.4', '825.25', '508.0', '786.05', '305.63', '205.30', 'E878.4', '205.01', 'E931.9', '96.09', 'V09.91', '46.03', '749.00', '744.29', '755.39', '750.10', '25.1', '736.00', '86.75', '81.71', '77.77', 'E922.2', '82.72', '38.05', 'E942.0', '445.81', '442.1', '54.3', 'V04.82', '81.06', '522.4', '337.9', '309.28', '117.5', '321.0', '054.2', '25.01', '263.0', '380.4', '198.7', '50.29', '438.7', '368.2', '32.3', '574.01', '461.3', '383.00', '435.2', '39.43', '745.2', '759.81', '593.89', '86.3', '874.8', '60.9', '39.52', '197.5', '437.4', '776.7', '742.3', '755.29', '755.59', '429.89', '573.0', '604.99', '98.03', '362.84', 'V53.91', '46.8', '36.16', '779.8', '705.1', '342.92', '29.5', '040.82', '939.2', 'E815.1', '804.25', '290.12', 'V29.1', '820.03', '881.00', '303.02', '795.5', '748.8', '410.12', '580.0', '034.0', '97.39', '141.9', '25.2', '40.41', '38.82', '38.94', '99.17', '491.22', '37.97', '751.2', '753.8', '427.60', '771.82', '243', '742.59', '58.39', '89.22', 'E901.0', '880.03', '755.13', '42.92', '354.0', '52.6', '38.09', '45.91', '44.3', '82.09', '56.82', '52.82', '28.11', '29.12', '845.03', '825.21', '79.87', '188.8', '272.9', '300.21', '493.91', '532.20', '54.64', '87.62', '87.63', '42.58', '197.4', '558.1', '70.74', '46.51', '787.6', '824.4', '823.81', '923.20', '914.2', '333.0', '77.11', '191.1', '292.12', '824.6', '801.06', '901.0', '868.02', '864.00', '863.89', '56.0', '728.86', '96.59', '852.05', '304.10', '550.91', '756.3', '804.26', '320.82', '900.03', '507.1', '68.59', 'V16.42', '37.65', '813.22', '764.04', '99.23', '152.1', '279.01', '780.1', '806.26', '309.0', '45.14', '989.4', 'E950.6', '386.30', '956.1', '801.31', '22.64', '996.93', 'E919.0', '99.99', '861.00', '77.31', '34.72', '710.1', '715.36', '996.42', '72.0', '80.05', '252.01', '35.41', 'E947.9', '402.01', 'V29.8', '95.47', '569.41', '52.80', '255.5', '250.03', '922.4', '569.61', '33.43', '34.99', '590.10', '695.4', '68.8', '537.3', '543.9', '99.02', '44.29', '287.9', '287.30', '286.5', '429.4', '810.10', 'E887', '79.69', '251.2', '427.69', '440.8', '88.44', '337.1', '746.2', '35.27', '37.74', '041.5', '564.01', '238.79', '540.0', '30.2', 'V32.01', '779.7', '304.60', '94.68', '83.43', '826.0', '873.40', '250.30', '245.9', '59.09', '337.0', '42.81', '466.11', '277.39', '070.71', '117.4', '571.0', 'E930.1', '83.85', '81.17', '78.37', '84.72', '046.3', '196.5', '54.24', '443.22', '250.81', '786.8', '753.10', '715.91', '191.8', '87.74', '995.0', 'E942.4', '871.2', '379.23', '823.90', '921.3', '16.82', '78.58', '608.9', '996.45', '81.53', '84.57', '37.91', '987.8', '506.0', 'E869.8', '746.9', '560.39', '556.1', '47.19', '158.0', '532.90', '702.19', '46.42', '48.62', '46.74', '596.7', '97.29', '696.0', '786.4', '745.69', '242.80', '079.3', '250.72', '84.17', '305.21', '919.0', '286.3', '307.50', 'E939.3', 'E941.1', '295.60', '88.14', '534.40', '575.3', '45.92', 'E934.8', 'E935.3', 'V10.21', '711.06', '719.86', '718.95', '718.46', '80.76', '049.9', '292.9', '763.84', 'V20.2', '180.0', '198.82', '626.8', '68.4', '96.14', '361.89', '40.64', '673.23', '648.63', '643.03', '87.41', '349.0', 'E879.4', '198.81', '986', '802.38', '802.22', '802.9', '005.1', '427.61', '753.5', '038.44', '453.72', '453.89', 'V85.41', '344.01', '569.5', '89.19', '806.01', '35.9', '574.70', '51.41', '252.02', '366.8', '789.05', '533.70', '729.89', '37.9', '804.21', '813.18', '825.0', '157.9', '535.41', '211.1', 'V06.1', '36.19', '806.00', '386.12', '291.2', '863.39', '863.21', '870.2', '99.9', '200.20', '996.39', '39.2', '785.50', '482.32', 'E884.0', '794.8', '562.00', '288.3', '39.42', '789.31', 'E813.0', '223.0', '300.9', '861.01', 'E958.5', '882.0', '269.9', '525.9', '806.29', '801.12', '81.07', '789.30', '434.90', '442.84', '57.95', '762.5', '77.68', '491.9', '372.30', '244.1', '195.3', '71.5', '70.4', '51.63', '46.71', '093.1', '54.93', '295.30', '997.91', 'V45.76', '760.0', '447.4', '96.36', '053.9', '800.22', '922.32', '532.41', '531.70', '742.9', '428.42', '965.61', '625.6', '70.77', '59.79', '420.99', '151.2', '43.7', '410.20', '531.50', '532.50', '747.81', '759.6', '174.8', '756.79', '49.21', '57.18', '575.10', '632', '639.1', '639.2', '253.2', '309.9', '69.02', '67.61', '759.89', '821.30', '900.82', '79.33', '76.72', '996.66', '730.16', '80.06', '742.2', '743.43', '569.89', '716.95', '607.84', 'E935.6', 'V53.39', '070.33', '963.0', '368.46', '454.9', '726.11', '188.9', '070.41', '295.62', '422.90', '727.81', '956.3', '51.36', '999.32', '698.8', '34.83', '802.5', '871.3', 'E822.8', '86.89', '075', 'V49.62', '416.2', '52.0', '917.2', '272.2', '534.50', '45.34', '217', '742.4', '304.11', '704.00', '593.71', '45.00', '776.2', '772.13', '772.0', '293.1', '796.1', '727.00', '836.2', '712.36', '81.92', '715.89', '787.20', '77.48', '737.22', '77.71', '737.39', '81.35', '77.39', '86.23', '729.30', '288.04', '239.7', '244.8', '232.9', 'V16.9', '71.4', '99.79', '429.5', '35.32', '391.1', '997.99', '958.8', 'E928.8', '923.10', '420.0', '99.03', '608.86', '182.0', '68.41', '65.63', '46.76', '94.49', '282.5', '796.2', '237.5', '310.1', '493.01', '368.16', '46.93', '369.60', '161.8', '30.4', '698.9', '996.49', '81.34', '81.32', '29.4', '93.41', '193', '63.9', '303.00', '304.90', '89.65', '283.2', '80.85', '78.68', '710.2', '426.10', '77.99', '466.19', '445.02', '37.79', '494.1', '37.11', '772.12', '519.8', '134.8', '189.1', '775.9', '821.10', '79.09', '523.3', '92.24', '336.1', '83.49', '569.84', '345.71', '281.2', '12.5', '97.41', 'V10.43', 'V44.6', 'V45.74', '153.2', '707.25', 'V13.8', 'E949.6', '295.72', '916.0', '370.8', '863.54', '902.54', '880.12', '453.85', '716.96', '295.74', '423.1', '37.24', '151.8', '972.6', '301.4', '301.83', '357.89', '358.9', '728.0', '726.10', 'V83.81', '730.23', '767.0', '51.94', '54.23', '596.1', '799.89', '969.09', '968.0', 'E941.3', 'V85.0', '453.86', '453.6', '348.0', '967.9', '801.72', '801.62', '825.31', '892.1', '950.9', '78.18', '825.29', '78.19', '35.93', '320.9', '901.3', '781.0', 'E815.6', '864.09', '845.00', '928.20', '928.10', '77.67', '441.6', '39.73', '34.27', '38.84', '556.3', '85.44', '296.40', '478.32', '309.24', '83.19', '99.11', '53.11', 'V55.5', '59.94', '801.21', '318.2', '525.10', '719.07', '29.9', 'V17.1', '137.0', '289.3', '482.31', '747.21', '299.80', '665.51', 'V27.0', '665.82', '674.32', '75.52', '57.19', '59.02', '38.8', '746.3', '449', '781.94', '342.91', '518.51', '276.69', '853.02', '304.20', 'V01.1', '722.4', '881.02', '365.63', '593.4', '55.01', '786.06', '323.41', '695.89', '851.44', '79.17', '088.82', '202.00', '746.1', '584.8', '784.41', '65.1', '31.41', '31.48', '794.2', '934.0', '873.65', 'E870.6', '198.0', '139.0', '81.54', '770.3', '446.29', '525.8', 'E934.6', '588.8', '572.1', '552.29', '709.2', '45.02', '801.82', '902.21', '289.89', '727.40', '684', '157.2', '850.11', 'E823.3', '873.41', '834.02', '379.43', '391.8', 'V71.4', '308.2', '633.20', '639.6', '639.8', '216.5', 'V23.0', '74.3', '733.14', '788.42', '362.10', '599.6', '824.5', '904.6', '731.3', '760.75', '337.20', '751.3', 'V49.63', '368.9', '180.8', '618.01', '68.6', '482.39', '198.1', '88.6', '825.1', '808.43', '839.05', '537.4', '97.02', '31.69', '173.2', '873.64', '707.02', 'E930.9', '648.04', '642.44', '647.84', '550.10', '751.0', '53.05', '773.0', '733.6', '852.26', 'E822.7', '779.5', '524.10', 'V05.2', '851.02', '426.53', '555.0', '173.3', '574.61', '576.4', '305.53', '52.13', '51.64', '273.3', '40.21', '46.81', '513.1', '478.24', '196.3', '174.9', 'V45.85', '518.1', 'V62.84', '77.41', '46.1', '892.0', '443.24', '813.51', '816.01', 'E922.9', '995.90', '748.3', '755.01', '377.39', '20.3', '540.1', '41.05', 'E870.4', '587', '275.03', '277.7', '389.10', '438.30', '36.99', '863.50', '862.39', '345.10', '48.5', '49.95', '172.9', '438.53', '344.9', '904.7', '137.3', '77.75', '304.61', '565.1', '48.81', '49.01', '32.0', 'V30.1', '764.09', '800.26', '296.90', '775.4', '301.22', 'E934.3', '995.29', '301.20', '782.7', '625.3', '250.52', '362.02', '791.9', 'V85.43', 'V49.87', '852.09', '336.8', 'V13.09', '789.1', '278.8', '079.6', '52.93', '80.86', '80.26', '564.7', '45.03', '110.5', '863.81', 'E906.8', '751.1', '99.82', 'E958.8', '451.89', '972.9', 'E858.3', '35.11', '201.50', '478.33', '729.81', '611.1', '33.99', '201.52', '370.34', '376.11', '376.12', '867.6', '715.34', '56.31', '51.23', '593.82', '357.7', 'E931.5', '97.62', '39.23', '250.22', '801.32', '202.60', '88.03', '140.9', '173.0', '43.89', '642.71', '648.62', '644.21', '654.21', '648.91', 'V25.2', '74.1', '66.32', '75.34', 'V43.5', '94.0', '821.00', '402.90', '756.0', 'V19.5', '601.0', '764.03', '35.52', '574.81', '394.2', '279.51', '780.61', 'V45.75', '527.5', '26.91', 'E852.9', '729.5', '93.54', '212.5', 'V43.4', '40.22', '438.22', '241.0', '493.00', '708.9', '786.07', '708.0', '46.94', '765.12', '867.2', '58.41', '790.93', '813.83', '238.1', '753.23', '733.41', '333.1', '996.52', '883.2', '927.20', '927.21', '86.71', '86.61', '761.2', '764.95', 'V06.8', '64.', '812.09', 'V15.2', '77.19', '78.69', '80.87', '694.4', '626.2', '473.3', '110.9', 'E930.0', '564.89', 'V55.4', '154.0', '86.06', '724.9', '041.9', '771.2', '414.12', '755.02', '520.6', '160.9', '16.', '20.4', '21.69', '779.82', '708.3', '759.82', '42.87', '53.8', '39.24', '88.63', '45.95', '46.79', '607.83', '581.9', '304.03', '31.93', '212.1', '295.80', '31.43', '42.51', '971.3', 'E855.6', '453.82', '719.43', '110.4', '301.51', '952.05', '852.40', 'E819.3', '359.89', '10.0', '753.7', '57.51', '595.0', 'V58.41', '18.79', '780.96', '843.9', '77.91', '478.5', '722.6', '38.87', '20.49', '690.18', '635.92', '647.83', '648.43', '642.03', '69.51', '574.41', '278.1', '78.48', '83.11', '821.22', '821.29', '55.92', '803.22', '803.12', '823.01', '721.42', '304.83', '239.5', '304.23', '287.49', '765.01', '777.6', '762.3', '40.23', '89.39', '757.32', '927.3', '816.11', '903.5', 'E919.6', '79.34', '38.43', '82.41', '86.86', '823.92', '956.2', '162.0', '750.3', '747.5', '754.89', 'V54.16', '726.5', '471.0', '707.04', '44.15', '811.03', 'E938.4', '595.81', '666.24', '670.04', '648.24', '665.34', '615.1', '654.04', '442.89', '666.34', '320.3', '362.34', '150.3', 'E931.3', '46.72', '54.99', '91.72', '41.03', '130.7', '007.4', '054.19', '972.4', '763.5', '40.9', '800.15', '404.91', '996.75', '773.2', '379.91', '786.2', '917.1', 'E917.9', '446.4', '031.2', '202.40', '12.', '808.53', '865.02', 'E825.7', '928.01', '902.89', '79.19', '52.11', '296.00', '805.08', '959.09', '78.13', '85.2', '86.9', '52.4', '710.3', '807.5', 'E953.8', '31.64', '748.0', 'E940.1', '46.43', '46.13', '404.93', '310.0', '907.0', '435.3', '88.79', '032.85', '70.0', '97.71', '425.8', '37.25', '37.28', '15.3', '642.51', '674.02', '652.61', '663.31', '656.51', '658.01', '659.61', 'V27.2', '69.59', '802.28', '442.81', '356.1', 'E850.4', '237.70', '924.11', '250.20', '648.21', '659.51', '669.21', '75.61', '73.4', '73.09', '730.26', '290.43', '80.39', '493.21', '295.73', '070.31', '280.8', '459.89', '473.1', '780.94', '52.22', '812.40', '036.41', '272.6', 'E944.7', '99.78', '787.29', '942.14', '686.9', '905.4', 'E989', '806.16', '823.21', '800.31', '38.68', '996.68', '250.31', '238.2', '173.7', '787.4', '532.01', '153.1', '706.2', 'V01.79', '250.32', '405.99', '780.65', '254.1', '250.33', '426.6', '38.29', '88.61', '836.50', '79.76', '277.1', 'V45.72', '722.90', '530.11', '873.51', '873.53', '11.51', '88.5', '11.49', '353.6', '38.38', '461.0', '590.2', '56.41', '751.62', '286.0', '296.30', '053.20', '440.9', '902.87', '873.61', '374.30', '362.11', '54.75', 'V01.89', '34.26', 'E818.0', '151.3', '755.55', '10.9', 'E888.1', '715.31', '81.8', '782.61', '772.8', '77.47', 'E929.9', 'E927.8', '78.66', '55.0', '305.23', '027.2', '571.9', 'V59.6', '472.0', '99.74', '932', '22.2', '22.41', '154.8', '305.22', '079.51', 'V44.59', '804.75', '804.85', '641.11', '669.11', '74.0', '68.39', '715.09', '79.11', '282.4', '196.6', '87.64', 'V58.0', '141.0', '788.32', '778.9', '86.26', '756.14', '682.1', '27.0', '866.10', '868.14', '868.12', 'E965.4', '766.0', '242.81', '803.32', '477.0', '213.2', '201.98', '534.90', '883.0', '161.2', '148.1', '161.3', 'V64.3', '782.2', '642.24', '255.8', '255.10', '161.9', '575.4', '50.51', '277.00', '52.09', '744.9', '525.50', 'V45.77', '277.87', '331.9', '277.9', '698.1', '746.86', '49.45', '693.1', 'V15.02', '863.29', '801.42', '823.10', '983.1', 'E864.1', '806.39', '307.1', '680.2', '705.83', '009.1', '333.82', '782.5', '34.23', '923.11', '852.41', '806.62', '355.2', '863.52', 'V85.37', '278.03', 'E878.5', '77.6', '86.03', '794.5', '704.1', '327.26', '581.81', '767.2', '729.39', '053.0', '786.04', '924.10', '777.9', '758.2', '442.83', '355.3', '728.87', '237.6', '781.1', '427.9', 'E932.8', '242.20', '434.10', '78.49', '97.01', '728.71', '763.82', '337.22', '646.83', '648.23', '659.63', '654.23', '863.0', '188.0', '34.4', '734', '81.57', '83.75', '291.3', '97.37', '755.31', 'V10.01', '93.91', '382.9', '745.10', '569.86', '48.36', 'V15.51', '425.7', '448.9', '270.4', '728.13', '800.61', '077.99', '372.39', '338.21', 'V13.51', '526.9', '733.16', '96.52', '142.9', '724.4', '85.21', '85.96', '975.3', '806.20', 'E987.1', '79.12', '860.5', '451.83', '236.91', '800.75', '362.31', '903.3', '836.51', '881.10', '955.2', '836.0', '38.83', '80.6', '789.40', '616.10', '227.1', '252.08', '68.1', '245.2', '702.0', '779.9', '996.40', '293.89', '535.11', '211.2', '238.77', '55.53', '39.99', '938', '648.61', '647.81', '647.61', '642.01', '646.21', '648.81', '669.42', '674.82', '648.42', '580.9', '478.0', '470', '21.62', '21.88', '21.03', '695.3', 'E825.1', '792.0', '921.0', '379.40', 'E891.8', '344.60', '37.81', 'E917.4', '304.73', '750.4', '38.34', '780.93', '39.53', '320.1', '972.0', '962.7', '041.83', '850.9', '863.45', '814.02', '879.7', '81.79', '761.0', '830.0', 'E953.0', '76.93', '791.5', '98.51', '480.1', '711.03', '83.42', '82.21', 'E884.3', 'E988.9', '289.51', '142.0', '26.32', '995.3', '686.09', '49.46', '770.4', '383.02', '383.21', '383.1', '381.00', '20.41', '20.01', '555.1', '729.6', 'V31.1', '38.16', '239.0', '040.0', '453.83', '21.1', '76.78', 'E932.5', '805.03', '791.0', '56.71', '656.41', '641.31', '279.3', '461.8', '771.1', '737.19', '48.69', '225.4', '783.5', '041.82', '33.29', '294.0', '140.1', '27.42', '27.57', '813.52', '83.64', '71.0', '770.12', 'V45.09', '908.1', '634.91', '463', '464.31', 'E940.8', '229.8', '998.51', '682.5', '172.5', '296.22', '814.08', '564.2', '813.33', '268.2', 'V85.1', '850.0', '346.00', '333.85', 'V88.12', '79.14', '259.9', '251.9', '34.52', '924.8', '862.29', '767.1', '45.11', '812.10', '823.12', '813.92', '930.1', '79.61', '79.22', '16.81', '10.6', '868.01', '763.6', '35.28', '81.81', 'V16.8', '174.4', '85.43', '85.7', '85.0', '53.00', '018.94', '004.9', '783.3', '84.14', '909.3', '996.31', '111.9', '852.56', 'E930.4', '096', '094.9', '854.02', '872.02', '690.10', '790.94', 'V09.81', '227.3', '765.05', '305.40', '840.9', '571.3', '304.02', 'V69.8', '133.0', '535.30', '305.52', '906.4', 'E929.1', '33.2', '999.89', '39.66', 'E941.2', '532.70', '921.2', '535.31', 'E935.4', '45.22', '438.21', '48.49', '70.92', 'E878.3', '85.94', 'E943.8', '336.9', '758.39', '754.60', '753.19', '97.12', '123.1', '362.89', '765.06', '57.81', '522.6', '483.0', '815.09', '48.71', '813.21', '780.31', '745.60', '753.6', '58.6', '902.41', '788.8', '904.2', '88.65', '958.2', '959.12', '23.3', '45.72', '405.91', 'E942.5', '788.21', '874.4', '754.0', '333.5', '297.9', '438.12', '864.04', '571.40', '200.80', '483.8', '097.1', '127.2', '161.1', '813.43', '719.65', '39.91', '730.88', '015.04', '99.84', 'E812.9', '528.01', '601.9', '60.5', '550.11', 'V13.5', '866.01', '51.59', '37.87', '553.8', '722.73', '10.1', '762.8', '52.', '54.', '320.0', '414.19', '29.1', '146.3', '98.14', '87.69', '149.0', '389.7', '700', '703.8', '86.27', 'E814.0', 'V15.41', '455.9', '49.93', '49.03', 'V51', '34.', '864.12', '879.2', '53.01', '81.38', '200.21', '289.4', '35.31', 'E879.3', '94.27', '45.28', 'V14.2', '69.5', '60.2', '34.59', '209.20', '747.62', '39.55', '862.0', '711.09', '992.0', '741.93', '967.0', 'E950.1', '214.3', '453.9', '99.95', '218.2', '556.6', '88.01', '282.62', '517.3', '722.52', 'E942.2', '249.11', '783.40', '794.15', '844.9', '44.93', '150.4', '44.22', '96.27', '994.1', 'E910.8', '916.1', 'E920.8', '720.0', '097.0', 'V14.0', '45.63', '757.0', '57.17', '977.9', 'E980.5', '151.1', '342.82', '87.72', '696.3', '795.79', '771.4', '254.0', '254.8', '801.09', '175.9', '344.04', '851.41', '755.66', '289.8', '813.82', 'V54.23', '56.2', '48.75', 'E823.1', '784.49', '730.29', 'E930.7', '861.22', '79.29', '821.31', 'E965.0', 'V10.81', '719.49', '282.9', '793.0', '426.9', '242.91', '574.31', '99.63', '55.02', '808.1', '77.82', '84.55', '38.98', '764.05', '751.7', '45.33', '922.2', '425.9', '012.15', '263.8', '428.9', '018.80', '131.02', '36.31', '38.22', '52.51', 'V58.11', '900.81', '45.12', '208.00', '780.55', '50.69', '968.5', 'E855.2', 'V45.61', '361.9', '360.00', '361.05', '14.34', '478.9', '461.2', 'E958.9', '018.03', 'V10.49', '696.2', 'V15.29', '42.55', '42.11', '748.5', '202.90', '446.20', '295.40', '344.30', 'E941.0', '041.2', '789.03', '269.8', '55.04', 'V50.3', '747.29', '35.53', '710.8', '796.4', '996.84', '710.4', '110.0', '697.9', 'E871.4', '184.4', '451.19', '69.09', '71.11', '40.54', '37.85', '37.76', '395.1', '998.30', '744.3', '736.70', '94.63', '648.13', '363.62', '521.9', '758.7', '46.62', '813.11', '820.01', '839.42', '57.89', 'E821.9', '369.70', 'V50.41', '85.42', '85.89', '768.3', '821.32', '600.90', '438.84', '738.10', '803.21', '996.43', '912.0', '244.3', '331.83', '618.1', '852.29', '783.1', '89.69', '800.20', '666.04', '747.69', '615.9', '372.75', '79.07', '729.71', '82.22', '55.54', '32.', '338.0', '478.34', '507.8', '359.1', '753.9', '890.1', '851.40', '851.89', '902.34', '902.33', '125.1', '24.3', '646.63', '68.16', 'V10.62', '45.82', '46.2', 'V16.41', '746.7', '959.7', '79.16', '94.2', 'V19.2', '634.51', '674.51', '726.69', '172.0', '756.89', '453.87', '581.1', '48.25', 'V14.8', '370.00', '376.30', '388.69', '85.1', '934.8', '202.85', '760.2', '84.18', '77.65', '77.85', '357.6', '881.20', '815.04', '927.10', '881.12', '812.52', '38.33', '82.57', '79.13', '282.2', '238.72', '018.96', '61.1', '97.53', '756.6', 'V04.0', 'V06.6', '45.71', '753.13', '344.09', '87.79', '757.39', '34.71', '376.33', '727.51', '31.73', '31.98', '692.4', '574.11', '253.8', '76.3', '86.94', '96.18', 'E858.1', '053.13', '283.11', '31.0', '767.11', '659.71', '641.21', '69.49', '309.4', '054.10', '295.20', '404.03', '451.0', '964.2', '35.7', '745.11', '729.4', '873.59', '806.22', '375.01', '374.89', 'V26.51', '38.99', '153.0', 'V54.12', '571.42', '85.41', 'E884.5', '51.43', '528.09', '373.00', '287.1', '952.00', '902.9', '273.9', '273.1', '821.20', '959.14', '198.6', '49.04', '831.04', '802.32', 'E870.5', '887.1', '84.23', '82.12', '372.73', '303.92', '751.4', '747.22', '48.32', '596.9', '114.0', '151.9', '151.4', '70.23', '959.9', '300.14', 'V60.8', 'V63.8', '865.13', '923.01', '708.8', '540.9', '552.1', '47.01', 'V10.84', '16.63', '975.2', '595.1', '823.22', '595.2', '878.2', '61.41', '694.8', '251.3', 'V42.84', 'E968.8', '307.6', '110.1', 'V06.2', '755.00', '344.40', '81.15', '748.2', '851.46', '200.10', '602.3', '723.5', '737.32', '34.84', '746.84', '862.22', '263.1', 'E917.8', '358.01', '027.0', '164.0', 'V10.29', '584.6', '307.49', '054.72', '215.4', '004.1', 'E862.4', '322.0', '702.8', '38.66', '851.25', '812.50', '34.82', '480.8', '756.4', '726.71', '726.19', '908.9', '305.83', 'E881.1', '078.10', '28.2', '331.19', '89.44', '337.29', '39.4', '788.1', '864.03', '802.7', '351.9', '635.02', '872.01', '281.8', '377.75', '488.1', '238.8', '96.38', '230.1', 'E817.1', '756.17', '608.89', '781.8', '78.0', '377.02', 'V26.52', '764.91', '754.1', '433.91', '53.41', '22.01', 'V12.52', '654.51', '666.02', '669.41', '69.96', '68.3', '68.0', '780.59', '89.48', '271.0', 'V12.42', 'V74.1', '404.01', '296.32', '701.2', '475', '887.0', '815.10', '955.3', '718.44', '78.64', '39.58', '324.9', '214.1', '716.99', '750.0', '750.26', '25.92', '26.21', '482.84', '89.37', '442.0', '345.00', '903.1', '239.6', '923.03', 'E968.7', '722.71', '701.8', '85.6', '531.10', 'V16.49', '51.32', '390', '233.7', '721.7', 'E888.0', 'E920.9', '87.44', '93.94', '130.0', '031.9', '32.41', '241.9', '344.02', 'E917.0', 'E849.4', '42.32', '31.71', '788.99', 'E814.6', '53.10', '237.71', '778.0', '812.41', '474.12', '277.6', '614.3', '57.83', '536.2', '781.99', '822.1', '372.03', '44.69', '812.31', '596.3', '808.8', '996.54', '726.90', '817.0', '850.2', '788.69', '79.25', '204.01', '80.14', '80.13', '411.81', 'E930.3', '902.20', '756.71', 'E988.8', '375.56', '464.30', '44.99', '709.01', 'V62.6', '244.2', '202.83', '814.12', '815.19', '844.2', '836.1', '77.73', '79.63', '80.83', '78.04', '82.44', 'V58.49', '815.01', '814.05', '814.06', '814.07', '284.89', '757.33', '161.0', '294.21', '761.3', '534.00', '421.9', '55.86', '88.75', '59.93', '798.1', '478.11', '455.6', '33.78', '598.9', '438.13', '005.81', '45.', '669.32', '642.92', '654.41', '646.82', '46.85', '385.89', '873.21', '078.89', '478.79', '478.19', '806.09', '806.24', '814.00', '701.5', '44.67', '89.32', '53.04', '648.93', '44.97', '604.0', '695.2', '99.72', '55.', '39.93', '995.63', '83.94', '44.49', '300.22', '305.80', '282.7', '791.2', '239.1', '42.54', '596.6', '753.21', '388.61', '225.1', '40.1', '768.6', '763.1', '716.80', '726.73', 'V46.3', 'V76.51', '714.9', '952.15', 'E828.2', '694.5', '356.2', '33.92', '747.41', '156.8', '803.24', '922.31', '775.8', 'V58.62', '290.41', '746.87', '96.01', '583.1', '342.10', 'E821.2', '041.01', '279.4', 'V15.09', '704.09', '880.13', '903.2', '44.9', '77.45', 'E850.3', '78.47', '844.0', '619.0', '57.84', '341.9', '358.0', '97.54', '349.81', 'V18.8', '200.05', '253.7', '345.41', 'V12.41', '788.91', '378.00', '345.51', '994.7', '78.45', '333.72', '146.4', '767.3', '765.08', '779.2', 'E929.3', '342.00', '415.12', '852.25', '440.32', '170.7', '84.56', '777.1', '296.99', '747.61', '729.2', '48.79', '274.11', 'V56.0', '674.03', '31.62', '212.2', '212.3', '996.78', '259.4', '812.03', '97.33', '727.05', '600.10', '60.11', '742.8', '562.02', '552.00', '620.3', '53.21', '713.1', 'E876.1', '65.0', '801.15', '23.1', '751.69', '863.20', '801.05', '873.30', '952.14', '305.02', '580.4', '88.66', '29.3', '718.31', '752.9', '327.21', '275.8', '42.7', '97.59', '863.30', '803.75', 'E938.2', '909.0', 'E929.2', '604.90', '228.1', '801.35', '85.54', '438.6', '48.9', 'V02.9', '535.00', '712.38', '389.08', '212.6', '800.76', '812.51', '20.5', '16.52', '535.70', '272.8', '735.4', '379.21', '780.99', 'V02.4', '216.4', '749.20', '218.0', '29.2', '478.21', '719.03', 'V02.3', '37.62', '194.1', '25.02', 'E885.4', '952.9', '744.1', '743.65', '23.11', '53.03', '56.84', '88.62', '866.11', 'V19.8', '46.82', '199.0', '35.72', '754.81', '743.62', '524.00', 'V43.61', '511.89', '656.13', '474.8', '140.0', '820.20', '780.66', 'E930.6', '307.51', '755.50', '755.60', '761.1', '258.9', '97.14', '250.21', '76.2', '286.52', '999.5', '581.89', '787.99', '885.0', 'E920.1', '84.21', 'V53.09', '754.33', 'V10.88', '627.1', '344.2', '87.09', '596.59', '763.3', '732.1', '83.88', '806.31', '861.31', '900.89', '170.2', '985.8', '619.8', '983.9', '976.7', 'E950.7', 'E958.1', '989.9', '553.00', '727.04', '804.23', '901.1', '828.1', '78.07', '671.53', '349.9', '32.2', '625.8', 'V58.12', '537.81', '447.9', 'V65.3', '794.9', '593.5', '141.4', '25.3', '76.64', '777.4', '783.41', '748.1', '743.63', '351.8', '995.27', 'E944.5', '897.0', '91.63', '13.2', '141.8', 'E919.2', '903.8', '80.84', 'E812.6', '50.', '711.02', 'V54.81', '528.3', 'V42.1', '752.49', '96.22', '350.2', '50.19', '184.0', '70.24', '535.20', '620.5', '65.39', 'V54.9', '784.59', 'E936.4', 'V09.50', '762.0', '635.72', '635.22', '635.12', '69.41', '801.24', '851.84', '816.00', '784.69', '97.51', '621.0', '67.39', '820.19', '813.31', '307.81', '61.49', '56.42', '346.20', '863.93', '864.13', 'E922.0', '55.81', '058.89', '269.0', '338.18', '758.81', '54.71', '205.02', 'V10.91', '882.1', '722.11', '727.61', '80.81', '80.21', '80.89', '96.53', '305.93', '389.00', '202.81', '45.3', '395.2', 'E947.0', '801.60', 'E965.1', '815.12', '788.63', 'V10.69', '692.82', '288.9', '443.23', '410.32', '719.01', '454.1', '77.09', '787.60', '474.11', '28.7', 'E814.1', '808.9', '806.60', '861.32', '541', '114.9', '288.61', '084.0', 'E931.4', '53.12', '362.30', '342.81', '746.09', 'V13.69', '35.25', '35.95', '601.1', '792.9', '424.3', '893.0', '59.95', '331.5', '47.2', '34.2', 'E813.1', '896.0', '284.0', '464.50', '81.65', '171.5', '567.81', '935.2', '343.2', '754.2', '616.0', '40.59', '560.32', '64.96', 'E938.5', '442.82', '801.96', '808.51', 'V18.2', '721.8', '48.82', '36.91', '778.5', '146.7', '26.30', '724.03', '379.90', 'V58.42', '004.8', '44.38', '952.4', '733.3', '38.89', '80.98', '582.2', '521.09', '611.6', '709.09', '803.06', 'E818.1', '726.33', '78.1', '723.8', '99.75', '327.27', '146.0', '863.1', '96.02', '800.25', '825.23', '455.1', '49.49', '238.0', '812.49', '996.77', 'E883.0', '850.4', 'E823.2', '736.89', '528.5', '853.10', '88.9', '256.1', '38.15', '748.69', '379.50', '219.1', '617.0', '221.0', '826.1', '79.68', '88.92', 'E917.3', '365.00', '365.65', '078.11', 'V44.8', '438.40', '82.11', '82.01', '904.53', '844.1', '861.03', '344.81', 'E819.6', '803.41', '69.52', '154.3', '484.8', '802.23', '788.34', '529.8', '779.1', '959.11', '51.13', '86.02', '995.2', '626.4', '304.30', '78.33', '81.72', '551.29', '874.9', '208.90', '201.92', '293.84', '305.41', '309.3', '781.7', '955.7', '34.02', '735.8', '202.11', '202.18', '173.6', '52.8', 'V45.87', '306.1', '191.0', '763.89', 'V16.59', 'E831.8', '556.2', '495.8', '38.63', '810.01', '523.9', '391.0', '728.2', '747.89', '903.01', '953.4', '57.93', '887.3', 'V42.5', '709.3', '71.09', '89.26', '271.8', '741.00', '825.22', '754.30', '847.1', '750.16', '755.22', '752.65', '532.10', '084.6', '914.0', '493.02', '279.03', '377.30', 'V49.84', '478.20', '998.9', '801.36', '453.81', '121.1', 'V16.6', '78.16', '44.14', '617.1', '233.0', '614.1', 'V09.71', '575.6', '618.5', '70.5', '256.4', '24.4', '852.11', 'E824.1', '556.8', '853.09', '77.62', '930.8', '98.21', '617.3', '69.01', '746.01', '902.22', '749.02', '31.92', '810.02', '21.00', '838.05', '79.78', '569.9', '863.99', '811.10', '278.02', '780.62', '249.00', '464.11', '815.03', '904.0', '815.11', '891.2', 'E970', '495.9', '85.34', '924.01', '464.00', 'V62.0', '552.9', '363.20', 'V58.64', '37.86', '758.9', '353.0', '902.26', '87.73', '722.83', '581.2', '867.7', '839.79', '878.0', '926.0', '96.39', '49.79', '79.79', '62.69', '96.24', '729.82', '50.13', '51.', '34.06', '83.63', '533.00', '804.73', '728.9', '774.39', '20.09', '417.1', '737.12', '722.51', '738.5', '81.36', '594.2', '594.1', '285.3', '164.1', '256.39', '719.16', '715.16', '158.8', '997.72', '765.11', '438.31', '664.01', '73.59', '75.69', '790.2', '154.2', '60.62', '45.15', '97.85', '564.0', '976.0', '812.12', 'E957.9', 'V46.9', 'V69.4', '279.11', '995.62', 'E938.9', '70.79', '474.00', '611.8', '85.85', '85.95', '415.0', 'V49.65', '629.89', '601.8', '57.11', '41.08', '753.20', '389.12', '97.61', 'V84.01', 'V49.60', '79.56', '793.2', '814.19', '814.18', '903.4', '77.84', '82.36', '86.7', '596.55', '526.89', '900.01', 'E986', '277.88', '284.19', '008.47', '238.76', '27.22', '524.62', '926.19', '58.5', '618.2', '866.03', '429.1', '048', '671.54', '426.50', '45.31', '719.11', '85.31', '762.7', '728.4', '251.5', '171.6', '905.0', '531.20', '854.00', '275.1', '426.51', '646.23', '88.40', '78.08', '380.10', '56.75', '187.4', '64.3', '58.0', '145.8', '76.31', '304.22', '301.7', '80.75', '957.1', '814.03', '599.60', '279.52', '374.52', '451.11', '265.1', '735.5', '786.30', '564.81', '924.5', '608.2', '811.09', '750.5', '43.3', '422.92', '952.3', 'E820.0', '643.13', '364.41', '996.53', '160.8', '22.62', '21.32', '770.9', '527.7', '80.15', 'E939.1', '332.1', 'E821.6', '97.35', 'E813.3', '77.66', '852.39', '900.9', '852.19', '873.74', '38.81', '25.51', 'V10.86', '294.20', '803.50', '800.00', '523.8', '267', '683', '443.1', '362.17', '429.6', '596.51', '718.49', '618.04', '70.52', '70.8', '77.87', '592.9', '733.40', 'V49.71', '768.5', '800.06', '556.4', '63.81', '512.2', '84.94', '786.39', '787.21', '787.23', '747.40', '35.91', '63.3', '715.97', '44.11', '44.41', '614.0', '65.89', '011.90', '011.36', '802.25', '873.54', 'V10.12', '35.05', '294.11', '576.9', '277.89', '215.5', '560.31', '233.3', '71.61', '49.39', '082.40', '816.02', '923.3', '45.51', '458.1', '47.9', '961.8', '961.4', '112.9', 'V16.43', '81.66', '800.30', '811.02', '282.60', 'V18.3', '772.2', '053.71', '42.22', '120.9', '813.08', '695.9', 'V90.89', '92.3', '865.14', '52.95', '308.3', '626.6', 'E901.8', '945.34', '991.3', '512.84', '92.39', '620.1', '733.49', '350.9', '746.85', '941.28', '941.27', '41.43', '32.1', '624.8', '76.5', '758.1', '757.5', '747.63', '715.37', '12.91', '378.87', 'E819.9', 'V18.59', '727.03', '83.01', '81.33', '806.8', '79.64', '211.7', '17.42', '64.91', '590.00', '65.29', '66.61', '312.9', '164.3', '944.21', '989.3', '344.03', '767.6', '453.0', '172.4', '36.09', '414.03', '359.2', '691.8', '33.9', '158.9', '052.9', '326', '12.2', '746.81', '883.1', '518.52', '997.49', '735.0', '81.12', '81.14', '77.59', '77.58', '727.1', '77.56', '83.13', '761.8', '517.2', '89.67', '831.01', 'E849.6', '438.50', '712.33', '718.87', '794.02', '29.6', '752.69', '645.11', '73.6', '68.49', '692.0', '692.89', '245.4', '396.1', '99.53', '861.12', '901.41', '38.47', '35.82', 'V18.9', '078.19', '583.2', '57.6', '76.39', '14.22', '376.35', '627.3', '460', '013.25', '454.0', '171.4', '801.45', '432.0', '12.6', '258.8', 'E870.2', '81.99', '648.03', 'V23.9', '88.35', '413.1', '946.2', '948.40', '56.74', 'E880.0', '719.26', '80.36', '110.8', '38.36', '202.43', '174.3', '85.48', '054.5', '53.62', '752.0', '705.21', '806.06', '115.99', '779.6', 'V44.50', '851.05', '603.8', '550.93', '45.21', '336.0', '354.8', '93.52', '804.00', 'V61.8', '802.39', '813.23', '235.7', 'V58.3', '66.69', '756.83', 'nan', '383.9', '558.2', '079.53', '839.08', '607.89', '78.09', '900.02', '874.2', '301.9', '395.9', '528.00', 'E834.1', '863.49', '200.01', '345.11', 'E937.0', 'V61.0', '53.81', '996.83', '251.0', '422.93', '15.', '813.54', '832.02', '79.72', '939.0', '192.0', 'E908.1', '139.8', '471.9', '120.8', '348.2', '447.5', '528.2', 'E917.5', 'E801.2', '84.07', '84.04', '800.82', 'E840.5', 'V46.8', 'E854.8', '85.36', '213.0', '794.6', 'V58.73', 'E929.4', '955.9', '903.9', '82.46', '804.12', '873.60', '27.52', 'E813.2', '368.41', '623.5', '21.86', '130.8', 'E934.5', 'V85.22', '45.30', '530.6', '905.5', '235.3', '716.16', '794.09', 'E826.0', '441.9', '333.90', '686.01', '377.00', 'V54.27', '152.9', '43.42', '945.32', 'V54.26', '780.50', '805.8', '233.9', '599.69', '380.15', '916.2', '22.11', '377.10', '377.16', '272.7', '852.42', '410.90', '83.5', '176.9', '277.8', '534.41', '89.50', '692.6', 'E865.4', '816.12', '880.20', '96.41', '794.4', '682.9', '342.02', '191.4', 'V32.00', '727.43', '359.0', '38.57', '719.13', '719.17', '36.2', '404.13', '323.81', '427.42', '287.0', '721.2', '40.69', '355.5', '576.0', '309.89', '649.44', '648.44', '052.7', '077.8', '211.0', '908.0', '618.8', 'E886.0', '590.9', '008.5', '803.11', '33.34', '824.3', 'E812.7', '523.4', '588.0', '88.38', '802.36', '29.39', '755.21', '68.23', '333.7', '37.96', '233.4', '96.19', '40.4', '258.1', '82.0', '582.89', '070.9', '621.3', '091.81', '617.5', 'V54.11', '733.81', '905.2', '99.16', '897.2', '79.27', '386.9', '648.33', '649.03', '989.5', 'E905.3', '453.77', '453.75', '453.76', '013.00', '854.03', '349.1', 'E823.8', '754.32', '011.94', '730.13', '31.61', 'V18.69', 'V64.43', '880.19', '525.12', '864.14', '299.00', '765.10', '874.12', '14.54', 'E945.2', '59.11', '674.84', '646.64', 'E967.4', '203.80', '84.0', '99.36', '58.93', '173.4', '775.89', '65.51', '32.49', '573.1', '800.71', '801.51', '958.0', '039.8', '29.11', '35.42', '719.42', '875.1', '814.01', '879.3', '772.1', 'E920.4', 'V10.61', '164.9', '173.5', '184.8', '71.79', '40.52', '153.7', '892.2', '863.40', '352.2', '159.9', '786.51', '995.83', 'E967.7', 'V62.89', '369.8', '147.8', '886.1', '816.03', '82.86', '774.31', '87.59', '862.1', '716.98', '373.13', '342.01', '906.3', '83.44', '010.85', '018.95', '018.05', '99.24', '305.71', '838.09', '710.5', '410.00', '118', '39.3', '34.81', '306.9', '901.9', '444.9', '551.3', '427.2', '764.99', '307.89', '861.02', '641.33', '69.93', '79.08', '661.11', '647.82', '343.4', '433.80', '41.98', '44.03', '81.55', '327.25', '236.2', '52.83', '367.1', '31.45', '22.6', '30.09', '785.9', '171.8', '763.81', '776.8', '43.91', 'V85.42', 'V58.31', '989.89', 'E861.3', '57.87', '812.43', '482.81', '907.5', '803.15', '17.33', '87.78', '366.41', '44.63', '301.81', '68.31', '65.91', '54.29', '864.19', '388.70', '680.5', '787.1', '172.8', 'E949.9', 'E901.1', '976.6', 'E858.7', '776.3', '369.9', '359.71', '216.9', '807.3', '836.3', '81.44', '764.16', '341.20', '44.19', '851.00', '839.20', 'E885.3', '176.3', 'V15.08', 'V16.2', '85.45', '646.61', '656.71', '145.0', '112.85', '900.1', '29.51', '279.9', '202.12', '202.13', '163.9', '595.9', '341.8', '714.1', '78.56', '801.76', 'E955.9', '13.41', '13.71', 'V43.82', '435.1', '804.76', 'V22.2', '511.81', '354.9', '61.2', '951.4', '956.9', '389.03', '600.2', '60.3', '070.53', '37.68', '289.50', '952.06', '047.0', '205.11', '607.9', 'E959', '308.0', '203.10', '98.26', 'E917.7', '67.12', 'E928.3', '76.73', '707.9', '648.64', '37.2', '793.3', '779.85', '144.0', '76.43', '78.53', '282.40', 'V14.5', '414.05', '300.16', '38.61', '83.87', '77.35', 'V85.30', '617.8', '68.12', '173.9', '801.65', 'E955.0', '854.06', '718.15', '879.1', '584.7', '471.8', '473.2', '334.9', '276.61', '758.3', '576.3', '29.0', 'E946.3', '516.0', '585.1', '031.0', '801.52', '502', '315.9', '81.82', '851.45', '404.90', '851.96', '813.07', '738.8', 'V54.01', '78.03', '71.3', '516.9', '804.66', '388.30', '57.59', '57.33', '270.6', 'V56.1', '872.8', '777.3', '947.1', '743.30', '743.20', '37.63', '674.34', '789.34', '863.31', '521.01', '846.0', '306.8', '24.5', 'E962.0', '376.01', 'E876.4', '376.9', '446.7', '49.71', 'V71.6', 'V85.34', '847.9', '594.9', '718.43', '727.9', '78.63', 'E935.7', '237.1', '800.10', '906.1', '086.0', '39.92', '031.1', '44.12', '748.61', 'E936.0', '707.00', '701.1', '635.52', '655.83', '69.29', '904.3', '904.8', '759.2', 'V54.15', '800.01', '719.09', '38.42', '598.2', '756.12', '928.00', '926.12', '242.10', '788.31', '880.23', '927.8', '832.00', '83.79', '79.82', '78.12', '062.2', '736.29', '453.1', '813.81', '31.49', '665.11', '99.76', '81.95', '379.24', '365.70', '438.81', '365.23', '885.1', '238.6', '812.42', '842.09', '414.9', '620.0', '292.89', '111.8', '901.82', '726.60', '742.0', '738.19', '520.0', '68.51', '445.89', '880.00', '79.38', '79.88', '41.01', '380.22', '909.9', '112.83', '192.2', '919.1', '991.1', '813.03', '863.43', '914.9', '354.5', '40.53', '674.14', '654.44', '648.94', '669.44', '717.6', '800.23', '84.08', '85.47', '86.21', '171.2', '516.36', '70.51', '901.42', '374.9', '553.9', '552.02', '53.31', 'E858.8', '754.61', '81.16', '53.29', '996.41', '374.82', '52.53', '52.92', '738.0', '21.83', '379.09', '382.00', '567.0', '172.3', '744.41', '759.0', '750.19', '616.4', '92.05', '202.87', '803.16', '79.18', '760.1', '296.44', '836.52', '85.91', '323.51', '284.12', 'V17.5', 'E870.3', 'V15.04', '963.1', '342.11', '345.81', '96.55', '368.12', '519.9', '582.0', '583.0', '46.96', '784.42', '338.12', '674.04', '672.04', '524.69', '863.55', '879.5', '878.7', '878.5', '788.37', '18.09', '804.70', '76.0', '246.2', '38.02', '18.29', '320.7', '71.22', 'V70.3', '841.1', '804.41', '801.41', '88.77', '216.6', '437.9', '744.89', 'V40.3', '556.0', '79.85', '307.23', '839.06', '955.1', 'V53.99', '730.06', '81.11', '81.13', '008.63', '996.89', '807.10', '93.93', '580.81', '362.13', 'V13.89', '998.01', '803.26', '288.09', '642.04', '851.31', '880.01', 'V07.4', '143.1', '536.1', '922.0', '23.01', '789.51', '279.50', '703.0', '45.01', '551.20', '374.31', '712.16', '041.05', '608.0', '60.71', '62.41', '960.5', '508.8', '75.4', '75.32', '804.32', '811.01', '951.5', '95.25', '80.46', 'V15.89', '642.64', '648.14', '868.09', 'E884.1', '493.81', '81.83', '801.80', 'E967.8', '995.81', '872.61', 'V04.81', 'V60.4', '79.21', 'E871.8', '094.0', 'V12.50', '172.6', '943.32', '942.34', 'V62.82', '958.93', '448.0', '736.72', '795.09', '079.4', '622.10', 'V61.11', '58.31', 'V33.1', '746.00', '78.38', '601.2', '447.3', '749.12', 'V28.9', '896.1', 'E920.0', '789.33', '921.9', '966.4', '88.97', '352.6', '951.3', '951.0', '425.18', '236.0', '274.89', '88.96', '801.75', '89.18', '331.89', '758.5', '371.40', '527.3', '952.02', '806.02', '052.0', '627.8', 'E825.0', '790.8', '090.2', '377.49', '144.9', '38.62', '27.56', '706.8', '919.8', '813.02', '83.03', '755.20', '532.30', '971.0', '697.0', '733.01', '195.0', '26.11', '521.08', 'E821.1', '65.52', '300.7', '744.23', '851.90', 'E985.0', '357.3', '375.15', '379.42', '958.5', '842.00', '634.11', '634.01', '327.13', '339.89', '378.81', 'E814.2', '642.54', '743.49', '242.30', '712.22', '79.52', '840.8', '804.20', '870.1', '574.80', '152.8', '727.82', '814.09', '334.0', '97.38', '81.40', '84.19', '862.32', '972.1', '974.4', 'E858.5', '618.3', '382.01', '616.2', '71.23', '782.8', '35.62', 'E841.5', '92.15', 'V40.0', '491.8', '81.84', '77.43', '76.97', '530.9', '680.6', '039.2', '756.11', '300.12', '078.0', '873.22', '902.51', '599.72', '37.1', '42.10', '790.22', '22.52', '488.01', '996.88', 'E906.3', '733.12', '378.42', '99.88', '347.00', '770.18', '812.59', '881.21', '652.21', '86.84', '008.61', '714.32', '97.89', '097.9', '622.1', '770.87', '461.1', '991.2', '265.2', '87.77', '673.11', '668.11', '871.0', '871.6', '304.81', '841.8', '78.52', '626.9', 'E901.9', '995.80', 'E967.3', '200.03', '616.8', '85.84', 'E851', '51.96', 'E811.0', '851.06', '607.3', 'V43.0', 'V15.59', '944.20', '714.89', '84.03', '82.81', '82.89', '812.02', '81.47', '44.2', '49.02', '041.03', '79.04', '759.5', '525.3', '232.5', 'V64.0', 'E835.3', '799.2', 'E854.0', '813.32', '749.23', '368.11', '385.82', 'E876.9', '20.59', '365.10', '800.85', '210.1', '237.0', '99.57', '77.33', '70.12', '806.03', 'E879.7', '711.55', '730.12', '131.01', '782.62', '754.40', '852.10', '89.41', '865.12', '945.22', 'E924.2', '861.30', '873.50', '952.03', '952.08', '86.1', '15.7', '836.61', '79.86', '93.16', '846.9', '881.11', '80.82', 'V15.52', '759.3', '57.34', '378.9', 'V17.2', '098.0', '312.39', '790.1', '755.11', '20.7', '011.93', '386.10', '42.84', '851.75', 'V70.8', '924.9', 'V72.81', '300.09', '50.24', '214.2', '289.1', '852.31', '727.89', '719.70', '701.0', '070.0', '999.6', '906.8', '230.2', '45.29', 'E885.0', '774.30', '190.5', '98.13', '740.2', '360.03', '12.7', '693.8', '53.80', '079.0', '435.0', '618.4', 'V12.61', '749.10', 'V16.7', 'V49.89', 'E919.3', '361.01', '14.75', '80.17', '77.78', '49.42', '621.4', '388.72', '600.9', '160.2', '16.59', '85.53', '70.72', '730.02', '77.02', '785.1', '719.40', '37.98', '726.72', '255.42', '279.06', '370.9', '803.60', '21.21', '97.32', '633.11', '66.62', '343.1', '378.10', '282.1', '730.89', '844.8', '308.9', '287.2', '438.52', '852.15', '48.35', '12.92', '12.73', '14.9', '365.22', '823.42', '15.2', '283.10', '804.30', '804.10', '953.9', '88.68', '302.50', 'V18.1', '806.08', '275.40', '364.03', '600.11', '86.65', '804.36', '738.6', '77.01', 'E924.9', '44.95', '837.0', '953.0', '37.92', '297.2', '92.30', 'V42.89', '279.8', '627.2', '715.30', '681.02', '301.50', '995.67', '323.4', '747.32', 'V85.45', '201.91', '648.31', 'V29.2', '282.8', '730.97', '770.16', '335.10', '334.8', '89.52', '21.0', '453.51', '744.42', '446.21', 'V85.36', 'V55.8', '43.49', '163.8', '727.42', '88.58', '51.86', '719.90', '794.01', '341.0', '323.82', 'V85.38', '202.82', '719.66', '852.24', '355.79', '803.36', '31.3', '80.73', '37.29', '171.0', '145.2', '27.31', '376.03', '904.50', '836.60', '715.15', '614.9', '305.43', '852.03', '665.24', '085.9', '47.91', '712.26', '673.33', '644.03', '96.54', 'E876.2', 'V64.05', '780.64', '202.01', 'E932.2', '774.1', '52.19', '379.56', 'E933.8', '343.8', '053.29', 'V16.51', '718.56', '93.26', '904.42', '847.2', '56.89', '673.24', '451.9', '371.43', 'V43.1', 'E870.9', '717.84', '837.1', '86.51', '377.01', '854.04', '957.8', '422.0', '397.1', '533.11', '210.4', '27.72', '21.61', '84.71', 'E800.2', '863.42', '599.84', 'E922.5', '376.89', 'V66.0', '718.88', '76.46', '607.2', '180.9', '878.6', '145.3', '73.0', '681.11', '756.13', '35.35', '202.20', '751.60', 'E858.2', '756.9', '417.0', 'E939.5', '820.02', '230.6', '569.44', '616.50', '733.02', '815.14', '279.53', '054.43', '98.12', 'E963', 'E964', '741.03', '211.8', '873.73', '239.4', '275.5', '252.8', '833.02', '813.93', '851.09', '79.73', '27.53', '801.74', '853.14', '801.64', '816.13', 'E923.8', '82.51', '287.33', '800.32', '524.60', '988.1', 'E865.5', '815.13', '743.59', '18.21', '478.70', '29.33', '29.59', '80.88', '799.01', '722.2', '590.11', '753.17', '369.01', '788.38', '670.02', '642.31', '793.7', '791.6', 'E900.0', '95.03', '70.62', '70.75', '969.1', '290.10', '851.73', 'E955.4', '188.5', '008.62', 'E945.1', '091.2', '354.1', '698.4', '980.2', 'E860.3', '312.89', '85.25', 'E938.7', '45.49', '524.06', '755.69', '208.91', '204.90', '205.90', '757.2', '288.62', '994.8', '909.4', '593.70', '93.46', '594.0', '985.1', 'E866.3', '360.40', 'E945.7', '733.20', '402.00', '726.0', '54.97', '070.59', '304.80', '153.5', '45.26', '614.2', '866.13', '874.11', '876.1', '615.0', '774.4', '96.57', 'V11.0', '292.82', '77.63', '35.70', '56.62', 'V10.50', '322.2', '18.0', '901.2', '78.11', '80.71', '80.31', '32.24', '747.11', '924.3', 'E943.3', '55.39', '607.1', 'V44.9', '800.16', '763.4', '24.11', 'E818.2', '523.40', '216.7', '172.7', '44.96', 'E812.3', '85.82', '434.00', '39.75', '459.10', '80.79', '313.81', '361.07', 'E954', '230.9', '233.1', 'V86.0', '728.3', '658.21', '660.21', '654.11', '672.02', '833.05', '78.54', '81.28', '78.29', '54.73', '802.35', 'V58.63', '373.11', '188.4', '22.9', '201.58', '718.28', 'V58.81', '776.0', '349.2', '786.01', '77.7', 'E858.9', '71.1', '99.26', '200.70', '942.03', 'E924.0', '27.92', '728.6', '335.22', '999.33', 'E885.2', '730.19', '335.23', '627.0', '307.42', '879.8', '49.2', '86.95', '730.01', '910.8', '780.54', '32.59', '701.3', '003.0', '023.9', '33.42', '200.40', '200.42', '971.2', '828.0', '57.31', '453.84', '735.9', '070.52', '958.91', '864.11', '42.1', '22.5', '21.22', 'V50.49', 'V84.09', '46.22', '812.30', '813.91', '338.28', '94.4', '200.30', '789.02', '995.64', 'V11.8', 'V62.4', '786.00', '560.30', '307.59', '312.34', '831.09', '764.01', '996.47', '250.23', '995.7', '537.1', '236.7', '621.8', '759.7', '41.06', '65.2', '021.8', '378.71', '360.43', '230.0', '536.40', '76.99', 'V02.0', '746.83', '80.12', '188.3', '81.85', '388.8', '642.41', '648.01', '648.92', '250.91', '674.12', 'V65.49', '768.7', '651.01', '74.4', 'V10.59', '64.11', '60.91', 'V17.41', '013.04', '011.64', '374.20', '84.59', 'E824.2', '112.81', '038.41', 'V10.9', '820.9', '806.5', '867.9', '231.2', 'V18.51', '389.15', 'E957.2', '525.11', 'E874.4', 'V60.2', 'E825.2', '669.34', '380.39', '327.42', '760.70', '438.10', '752.62', '484.3', '033.8', '455.4', '49.47', '478.22', '861.10', 'E871.7', '911.2', '868.10', '379.8', 'E871.6', '736.09', '598.1', '908.2', '833.11', '789.39', '349.31', '727.41', '235.4', '186.9', '304.93', '851.35', '944.25', '944.23', 'V15.86', '523.31', '365.12', '76.01', '76.11', 'V02.52', '800.24', '519.00', '806.23', '933.0', '839.03', '839.04', '879.9', '831.03', '40.19', '602.8', '60.0', '64.2', '230.8', '429.81', 'V85.23', '625.5', '96.7', '360.12', '39.21', '258.01', 'V18.11', '454.8', '870.3', '902.23', '333.6', '255.2', '46.64', '335.11', '288.4', '733.96', '718.45', 'V85.39', '755.64', '48.29', '531.01', '365.20', '115.90', '692.3', 'E957.0', 'E860.9', '203.02', '906.7', '44.92', '989.0', '477.8', '621.30', '053.12', '44.02', '642.61', '648.41', '755.10', '770.88', '404.00', 'E806.2', '716.97', '85.22', '793.99', '686.1', '50.14', '901.40', 'V18.19', '791.3', '111.0', '289.52', '157.4', '80.5', 'V85.32', '887.5', '812.19', '955.8', 'V46.14', '373.2', '296.53', '839.69', '93.29', '425.11', '480.2', '60.21', '44.91', '42.91', '323.61', '51.99', '666.22', '65.79', '611.71', '793.80', '49.74', 'V10.20', '300.15', '952.8', '533.41', '897.7', 'E804.2', 'V10.60', '617.2', '59.0', '633.80', '752.3', '93.99', '535.21', '302.9', 'V13.65', '200.22', '77.0', '330.8', 'V72.19', '249.81', '301.3', 'V85.44', '783.43', '690.12', 'E938.0', '159.8', '200.71', '42.4', '863.53', '31.72', '958.99', '32.9', '610.1', '384.20', '783.6', '784.61', 'V85.33', '200.23', '738.3', '722.30', '532.51', '727.3', '12.8', '736.6', '81.22', '51.49', 'E863.7', '49.11', '61.42', '642.34', '521.81', '239.2', '352.9', '361.06', '374.43', '736.71', '417.9', '775.3', 'V61.42', '296.51', '519.11', '367.4', '749.22', '743.53', '013.54', 'V85.24', '12.1', '274.00', '211.9', '296.54', '147.1', '832.09', '98.27', '807.6', 'E910.9', 'V61.10', '897.3', '821.39', '362.03', '362.07', '831.14', '716.93', '366.16', '355.1', '296.81', '918.0', '41.39', '942.04', 'E899', '359.21', '839.21', '754.31', '315.8', '879.0', '85.81', '716.87', 'E944.1', '216.3', '649.33', '642.93', '384.01', '542', '873.71', 'E965.9', 'E943.0', '972.2', '597.0', '598.00', '58.91', '41.91', 'E838.4', '97.87', '14.41', '42.62', '86.82', 'E834.3', '75.1', '624.9', '365.89', '745.12', '723.7', '76.77', '374.10', 'E831.1', '334.3', '147.9', '57.79', '80.0', '665.61', '655.71', '656.61', '659.21', '75.99', '530.87', '084.4', '45.52', '531.11', '288.66', '323.62', '410.22', '306.2', '712.13', '341.1', '924.20', '726.12', '375.00', '754.82', '959.3', '253.4', '433.81', '438.85', 'E816.3', '614.5', '012.05', '16.51', '942.09', '200.11', '204.12', '343.0', '315.31', '60.1', '603.1', '608.20', '176.4', '176.8', '35.98', '732.5', 'E855.5', '288.02', '523.33', '146.8', '18.39', '62.2', '63.1', 'V58.44', '97.52', '730.20', '56.91', '523.30', '151.6', '202.70', '99.45', '733.29', '304.40', '150.1', '195.2', '437.6', '757.1', '163.0', '429.2', '995.89', '014.05', '447.71', '275.9', '716.94', '27.55', '53.71', '17.36', '716.15', 'E838.1', 'E945.3', '53.42', '444.09', '69.3', '833.00', '813.13', '787.24', '969.05', '925.2', '53.83', '297.8', '839.07', '202.05', '38.37', 'E985.4', 'E030', '93.51', 'V45.88', '45.81', 'V85.25', '646.81', '83.77', '724.6', '912.2', '389.22', '279.41', '82.19', '999.41', 'E906.0', '209.21', '249.60', '344.5', '732.0', '200.18', '057.8', '795.39', '969.03', '564.3', '819.1', '249.01', '362.06', '362.05', '803.72', '362.83', '51.93', '70.61', '56.61', '255.12', '788.33', '796.9', '788.64', '368.47', '627.9', '209.51', '87.66', '793.19', '17.41', 'E980.1', '48.52', '368.15', '803.76', '803.66', '132.0', '132.9', '780.02', '790.95', 'E001.0', '793.81', '85.11', '518.53', '908.6', '79.89', '35.99', '305.73', '784.92', 'V85.31', '213.7', '046.71', '711.08', '389.17', '55.21', '737.33', '737.11', '634.12', 'E871.0', '249.91', '299.90', '819.0', '049.1', '058.81', '295.12', '619.2', '642.94', '649.14', '80.95', '890.2', '969.71', '973.3', 'E858.4', '998.00', '32.27', '567.1', '83.0', '188.1', '298.0', '840.6', '355.0', '302.85', '18.11', '71.6', '85.74', '715.33', '911.6', '945.04', '339.00', 'E000.9', '616.9', '512.89', '008.43', '654.14', '665.54', '998.09', '83.62', '82.43', '902.25', 'V83.89', '290.42', '378.53', '35.1', '793.89', '270.2', 'E999.1', 'E855.4', 'E927.4', '959.2', '780.33', '003.8', '801.71', '999.82', '916.5', 'E906.4', '338.22', '53.72', '947.3', '947.2', '998.02', 'E002.0', '320.89', '845.09', '84.82', '969.72', 'E874.0', '453.50', '530.13', 'E000.0', '054.12', '361.2', '666.14', '669.14', '983.2', '209.29', '209.72', '209.74', '209.79', '380.21', '998.33', '649.04', 'E950.5', '347.10', '524.4', '943.20', 'E873.5', '39.76', '825.32', '825.24', '80.91', '415.13', '795.10', 'V49.85', '716.66', '907.4', '854.01', '781.6', 'V86.1', '284.11', '789.60', '207.80', '344.42', '405.11', 'E816.9', '718.55', '386.19', '712.35', '733.5', '695.12', '695.50', '97.15', '863.84', '846.1', 'V54.10', '78.14', '474.9', '551.8', '596.89', '151.5', '52.98', '333.81', '42.59', '003.9', '642.63', '728.79', '523.10', '274.03', '999.71', '309.1', '65.62', '275.02', '202.78', '801.50', '802.34', '195.1', '959.19', 'E002.6', '146.1', '76.09', '569.71', '402.10', '816.10', '836.4', '249.80', 'E938.6', '44.01', '670.14', '235.8', '282.44', '531.91', 'V61.41', '853.16', '323.72', 'V07.8', '812.54', '281.3', '349.39', '209.61', '843.8', 'E003.2', 'E805.8', '288.1', 'V51.0', '718.48', '76.45', '21.4', '50.23', '873.23', 'E029.1', '861.20', '348.82', 'V61.09', '696.5', '381.4', '923.9', '922.9', '616.89', '478.71', 'E006.9', '575.2', '599.5', '851.83', 'E006.0', '800.13', '801.13', '848.8', '531.60', '973.5', '750.29', '951.7', '81.37', '200.50', '330.0', '362.52', '551.1', '33.79', '176.1', '796.71', '795.51', '719.12', '53.84', 'V01.6', '42.69', '999.84', '852.16', '680.9', '880.29', '525.40', '865.10', '863.94', '800.36', '355.71', '26.31', '171.7', '014.02', '60.99', '17.56', '348.81', '295.22', '995.85', 'E967.0', '375.9', '516.34', '41.93', '573.5', '719.96', 'E933.5', '42.65', '032.9', '894.0', '516.33', '275.01', '84.05', '33.71', 'V15.05', '68.69', '347.01', '368.59', 'V61.29', 'V90.81', '531.30', 'E920.5', '695.13', '698.2', '368.00', '045.90', '131.09', '89.34', 'E976', '054.13', '852.04', '695.14', '362.51', '274.02', '293.81', '851.21', '840.0', '453.71', '602.1', '39.78', '840.7', '714.2', '491.0', '282.3', '46.63', '372.40', '88.19', '551.21', 'V60.1', '189.8', '359.22', '379.99', '333.84', '082.49', '334.2', '817.1', '955.6', 'E003.9', '307.52', '974.7', '204.02', '550.00', '45.9', '24.32', '569.87', '64.98', '64.49', '709.00', '17.35', '55.22', '799.29', '97.11', '801.44', '716.50', '677', 'V10.71', '200.33', '977.3', '200.78', 'V53.6', '755.30', '388.5', '212.0', '389.21', '19.9', '149.8', '173.22', '26.3', '20.61', '523.5', '752.2', '752.19', '14.73', 'E913.2', '448.1', '813.53', '801.46', '834.11', '64.41', '79.84', '648.11', '550.01', 'E008.9', 'E874.8', '300.89', '070.49', '972.5', '627.4', 'E861.9', '122.8', '259.8', '359.5', '46.86', '727.09', '715.00', '902.10', 'V10.90', '209.43', 'E927.0', 'E013.8', 'E019.0', '621.2', '60.4', '72.9', '799.23', 'V40.31', '288.63', '293.82', '291.89', '997.32', '253.3', '77.1', '626.0', '27.61', '41.99', '851.62', '296.25', '277.31', '64.97', 'E879.5', '730.24', '013.30', '287.32', '79.42', '833.09', '233.31', '70.33', 'E000.8', 'V15.01', 'V85.21', '780.72', '664.11', '648.52', '715.93', '453.74', 'V56.8', 'V14.3', '288.51', 'V09.1', '426.54', '205.12', '923.21', '312.30', '305.31', '69.7', '173.72', '81.93', '629.81', 'V87.2', '202.08', '26.99', '526.5', '079.83', '745.9', '806.10', '195.8', '611.9', '007.1', '176.0', '375.30', '665.74', '917.3', '750.8', '380.23', '69.0', 'V49.61', '516.4', '49.23', 'E824.9', '269.3', '282.64', '379.55', 'E920.3', '863.51', '902.42', '952.07', '719.58', '642.74', '78.43', '362.32', '143.0', '752.89', '346.93', '995.61', '512.83', '194.5', '39.8', '116.0', '306.0', '695.15', '618.00', 'E910.0', 'E831.4', '536.9', '200.60', '639.0', '531.41', 'E818.9', '378.55', '47.4', '266.9', 'E817.0', '813.14', 'E960.1', '835.03', '150.0', '948.50', '943.25', '945.19', '48.51', '593.73', '91.46', '43.82', '362.33', '788.65', '282.46', '295.02', '032.89', '53.43', '246.8', '365.73', '953.5', '265.0', '912.5', '066.41', '066.42', '321.2', '736.05', '784.43', '953.1', '86.96', '36.17', '625.4', '35.55', '569.60', '377.51', '378.20', '296.14', '649.43', '646.53', '649.13', '88.04', '42.83', '736.30', 'V54.82', 'V88.21', '68.25', '374.32', '599.3', '939.3', 'E913.8', 'V25.41', '726.32', 'E933.4', '410.40', '259.2', '209.36', '209.75', '784.91', '526.2', '863.83', 'E007.1', '60.95', 'E016.1', '214.4', '53.14', '206.00', '339.20', '736.81', 'E813.8', '996.80', 'V02.53', '396.9', '722.70', '345.2', 'E818.8', '14.79', '904.51', '84.73', '76.1', '76.4', '80.54', '85.73', 'E874.2', '31.95', 'E945.5', '388.71', '527.9', '18.12', '85.71', '538', '88.3', '527.8', '378.82', '339.3', '674.01', '788.7', 'V54.22', '398.99', 'V15.06', 'V10.22', '333.79', '93.08', '943.23', '011.23', '21.5', '21.3', '744.01', '795.52', '800.34', '874.5', '89.5', '814.11', '357.1', '414.4', '901.81', '238.74', '304.43', 'V03.89', '369.10', '342.12', 'E825.8', 'V87.45', 'V06.5', '852.45', 'E006.1', '959.5', '360.02', '789.37', '115.05', '799.55', '371.82', '361.00', '714.81', '365.11', '581.0', '209.73', '534.01', '136.1', '79.46', '593.1', '737.29', 'V48.9', 'V87.09', '611.79', '642.33', '81.0', '054.0', '801.70', '719.50', '296.04', 'V13.52', '923.09', '269.2', 'E006.2', '148.8', '148.9', '695.0', '209.25', '209.71', '099.3', '789.47', '712.37', '787.04', 'V62.85', '362.41', '760.76', '337.01', '789.32', '719.18', 'E977', '146.9', '518.6', 'V65.5', '368.44', '27.69', '960.0', 'E941.9', '801.61', 'V13.81', '805.3', '805.5', '388.60', '202.52', '44.64', '44.68', '295.24', '825.20', 'E921.1', '729.90', '917.0', 'E008.0', '715.80', '727.63', '316', '43.6', '008.04', '87.34', '404.11', '477.2', '058.29', '053.14', '686.8', '943.35', '96.58', '803.62', '746.82', '366.46', '257.1', '364.70', '003.1', '315.2', '711.98', '721.91', '237.72', '77.2', '37.73', '380.00', '330.1', 'E876.7', 'E003.0', '237.9', 'V53.01', 'V25.01', '359.3', '659.53', '031.8', '362.9', '941.08', '529.6', '529.1', '611.89', '362.75', '301.13', '944.01', '82.35', 'E824.0', '87.65', '053.11', 'V61.03', '362.53', '344.41', '41.1', '709.4', '539.89', '534.10', '310.8', '389.05', '202.03', '307.47', '145.5', '21.89', '242.21', '799.53', '005.9', '362.04', '327.37', '382.4', '451.81', 'V16.52', '295.64', 'E804.1', '810.12', 'E946.6', '628.9', 'E932.4', '582.4', '339.12', '660.01', '652.51', '668.22', '288.2', '453.79', '24.7', '92.32', '801.23', '583.4', 'E900.1', 'E006.4', '334.4', '174.5', '201.48', '29.32', '127.3', '200.37', '49.31', '203.12', '807.18', '823.31', '820.30', '996.70', '790.21', '838.01', '447.73', 'E824.8', '249.90', 'V48.6', '93.21', '308.1', '410.10', 'E982.1', '753.14', '620.9', '942.32', '447.72', '202.06', '711.91', 'E925.0', '48.76', '69.98', '995.69', '727.62', '80.41', '799.59', '916.4', '80.96', '84.48', 'V13.64', '695.81', 'V15.42', '551.00', '424.91', '245.8', '60.12', '275.09', '909.5', 'E932.9', '732.3', '937', '23.73', 'V67.2', 'V67.1', '363.72', '215.0', 'E946.0', '995.65', '209.69', '845.12', '209.57', '054.40', '173.8', 'E007.3', '337.09', '906.0', '647.93', '646.73', '030.9', '788.62', '209.03', '648.83', '642.43', '389.06', 'V81.2', '45.83', '649.31', 'V27.1', '997.00', 'V07.39', '52.99', '190.6', '589.0', '780.53', 'V15.53', '834.00', '084.9', '736.20', '27.23', '238.73', '718.65', '80.35', '681.01', '941.00', '295.75', '373.12', '379.22', '371.89', '76.63', '802.30', '525.63', '327.24', '872.69', '295.44', '380.03', '221.8', '65.11', '322.1', '015.05', '728.82', '685.0', '756.15', '78.62', '526.0', '451.2', '523.42', '558.41', 'V43.63', '913.2', 'V88.11', '727.06', '80.33', 'E869.4', '954.0', '945.24', '25.59', '173.1', 'E866.9', '759.83', 'E934.0', '626.1', '965.69', '726.64', '46.4', '786.9', '851.50', '173.42', '173.32', '173.31', 'E926.2', '800.02', '495.7', '905.6', '46.95', '794.7', '532.11', '718.97', '641.03', '372.14', '389.18', '81.49', '80.47', 'E905.1', '906.5', '961.0', 'E857', '718.91', '910.2', '237.73', '78.02', '516.1', 'E007.6', '969.01', '239.89', '733.45', '32.09', '623.2', 'V53.7', 'V65.42', '756.16', 'E003.1', '724.09', '961.7', '961.9', '641.01', '670.12', '75.8', '732.4', '176.5', '115.19', '189.3', '58.23', '372.89', '81.1', '531.71', '715.92', '97.04', '296.31', '305.92', 'E814.5', '210.2', '911.4', '378.86', '373.9', '718.42', '808.59', 'E805.2', '784.99', '814.04', '959.4', '079.98', '98.05', '852.23', '200.48', 'E958.3', 'E975', '93.35', '379.93', 'E929.5', 'V03.7', 'E001.1', 'E013.9', '327.20', '209.17', '83.86', '93.03', '52.21', '372.74', '14.59', '726.79', '952.19', '969.79', '377.33', '011.86', '59.99', '851.94', '365.44', '288.59', '625.70', '55.12', '564.6', '625.1', '713.8', 'V09.90', '388.42', '091.9', '50.94', '008.41', 'E827.8', '200.24', '712.96', '304.72', '718.21', '327.19', '346.70', '661.31', '665.31', '665.41', '75.51', '33.49', '372.10', '80.49', '204.80', '282.43', '284.2', '726.65', '711.95', '719.85', '789.61', '508.2', 'E890.8', '852.32', '807.11', 'E967.9', '81.29', '695.10', '800.80', '376.52', 'E883.9', '704.01', '375.20', 'V61.04', '393', '719.30', '77.98', '918.9', '77.95', '194.3', '352.4', 'V64.06', '868.00', '994.9', '225.3', '272.5', '487.8', '32.22', '529.3', '646.71', '642.52', '657.01', 'V91.03', 'V49.66', '027.9', '995.86', '825.34', '719.04', '919.6', '747.20', '170.3', '277.09', '277.02', 'E029.9', '996.51', 'V78.9', '712.39', '365.52', '12.64', '88.4', 'V53.02', '81.97', '352.3', '249.50', '833.13', '833.03', 'E016.2', '77.12', 'V04.5', '62.99', '63.09', '50.93', '869.1', 'V90.10', '729.99', '200.47', '800.11', '79.83', '692.79', '207.22', '711.20', '215.3', '968.3', '337.21', '378.50', '642.14', '38.35', '33.73', '51.42', '203.82', '927.01', '110.6', '23.0', '38.69', '607.85', 'V42.6', 'V44.51', '209.63', '733.93', '753.11', '387.9', '930.9', '39.89', '457.2', '79.55', 'V84.89', '532.61', '624.02', '910.4', 'V15.85', '941.26', '943.21', '719.44', '282.41', '534.30', '706.9', '17.7', '86.19', '649.34', '648.84', '372.9', '443.82', '493.82', '718.35', '379.52', '368.01', '179', '014.85', '29.31', '870.4', '82.33', '82.79', '438.14', '48.73', '694.0', '339.02', '853.04', '209.30', '402.11', '525.79', '800.60', 'V90.39', 'E967.1', '282.0', '695.51', '634.21', '380.14', '018.90', '996.87', '344.31', '718.40', '46.14', '556.5', '296.21', '590.01', '370.24', '386.2', '915.2', 'E007.0', '183.2', '928.21', '26.0', 'E939.9', '908.3', '38.01', '838.13', '878.3', '62.61', 'E856', 'E817.8', '524.89', '851.03', '363.15', '963.5', '315.4', '315.39', '833.01', '997.41', '058.21', '39.77', '872.64', '56.72', '995.82', '728.83', '346.91', '607.81', '091.52', '825.39', '618.82', '729.91', '801.19', '214.0', 'V61.07', '183.8', '173.41', '298.2', '999.4', '564.4', '17.71']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tls_y.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90dd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8922"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tls_y.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cde10",
   "metadata": {},
   "source": [
    "There are 3 ways of decoding this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86bd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#15) ['038.9','785.59','584.9','427.5','410.71','428.0','682.6','425.4','263.9','96.04'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tls_y.decode(a_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53981b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "038.9;785.59;584.9;427.5;410.71;428.0;682.6;425.4;263.9;96.04;99.62;89.64;96.72;38.93;96.6\n"
     ]
    }
   ],
   "source": [
    "show_at(tls_y.train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03700c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#15) ['038.9','785.59','584.9','427.5','410.71','428.0','682.6','425.4','263.9','96.04'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tls_y.vocab[torch.where(a_label == 1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4127d6",
   "metadata": {},
   "source": [
    "#### 3. Making the `Datasets` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d82d5d-1b65-4c87-a8a2-1d2405a46be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_vocab = torch.load(lm_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d230d2c-ff84-4a59-b24f-08289f4f34af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#57376) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(lm_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms = [Tokenizer.from_df('text'), attrgetter(\"text\"), Numericalize(vocab=lm_vocab)]\n",
    "y_tfms = [ColReader('labels', label_delim=';'), MultiCategorize(vocab=lbls), OneHotEncode()]\n",
    "tfms = [x_tfms, y_tfms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388164b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dsets = Datasets(df, tfms, splits=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b18b3",
   "metadata": {},
   "source": [
    "Let's now check if our `Datasets` got created alright:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a3676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49354, 3372)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsets.train), len(dsets.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361970b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fdf398",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.decode(dsets.train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.show(dsets.train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26659eb0",
   "metadata": {},
   "source": [
    "Looks fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9c79a",
   "metadata": {},
   "source": [
    "#### 4. Making the `DataLoaders` object:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eabc06",
   "metadata": {},
   "source": [
    "We need to pick the sequence length and the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0bdf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, sl = 16, 72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cd7b7",
   "metadata": {},
   "source": [
    "We will use the `dl_type` argument of the `DataLoaders`. The purpose is to tell `DataLoaders` to use `SortedDL` class of the `DataLoader`, and not the usual one. `SortedDL` constructs batches by putting samples of roughly the same lengths into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_type = partial(SortedDL, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cde48-5633-48b1-a7a3-40b2cae52193",
   "metadata": {},
   "source": [
    "**Crucial:** \n",
    "- We will use **`pad_input_chunk`** because our encoder `AWD_LSTM` will be wrapped inside `SentenceEncoder`. \n",
    "- A `SenetenceEncoder` expects that all the documents are padded, \n",
    "- with most of the padding at the beginning of the document, with each sequence beginning at a round multiple of bptt\n",
    "- and the rest of the padding at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92169157",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = dsets.dataloaders(bs=bs, seq_len=sl, \n",
    "                        dl_type=dl_type,\n",
    "                       before_batch=pad_input_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d755e2",
   "metadata": {},
   "source": [
    "Creating the `DataLoaders` object takes considerable amount of time, so let's save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c3853",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dls_clas, path_model/'caml_dls_clas_full_8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = torch.load(path_model/'caml_dls_clas_full_16.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94995576",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fef3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deb/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:475: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "/home/deb/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:475: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "/home/deb/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:475: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "/home/deb/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:475: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "/home/deb/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:475: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "/home/deb/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:475: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos admission date discharge date service cardiothoracic allergies patient recorded as having no known allergies to drugs attending first name3 lf chief complaint doe fatigue and decreased exercise tolerance major surgical or invasive procedure redo sternotomy avr 21 mm company mosaic porcine valve mvr mm company mosaic porcine valve history of present illness yo female with prior cabg in since then she has developed increasing doe fatigue and decreased ability to tolerate activity echo revealed severe as and mr initially seen by dr last name stitle and now presents for surgical repair past medical history cabg x dr last name stitle cad as mr last name titles lipids htn gerd oncogenic osteomalacia lv diastolic dysfunction hereditary hypophosphatemic rickets bil hip surgeries with left thr prior bladder suspension prior tah prior cataract surgery social history retired homemaker never used tobacco denies etoh lives with husband family history not known physical exam</td>\n",
       "      <td>401.9;V43.64;V45.81;272.4;599.0;96.6;530.81;39.61;780.39;041.04;997.02;35.21;396.2;35.23;348.39;89.6;270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>96.71;38.93;96.04;428.0;507.0;599.0;96.6;440.24;88.48;496;99.15;14.0;43.11;482.41;39.29;707.15;38.18;997.2;444.21;97.01;38.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>427.89;305.1;599.0;785.52;995.92;584.9;296.80;042;304.01;041.6;V12.04;008.45;332.0;038.8;348.39;304.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_clas.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1a7fd-60be-4a18-83a0-48dbf170b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = dls_clas.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a3782-9308-41cb-8b22-907df336b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8d1db-e24a-477e-9e84-87564aff9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dls_clas.vocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2256e5c-2e96-4fb4-8805-fa498bba6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[20], len(x[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3541dd6-f3e7-4151-92b4-92e8b0653ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(' '.join([dls_clas.vocab[0][o] for o in x[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d47ce-2525-497d-b0d4-de09c8306795",
   "metadata": {},
   "source": [
    "In this section we will unwrap fastai's `text_classifier_learner` and manually write all the steps (ofcourse, shamelessly stealing) from fastai's source code!\n",
    "\n",
    "To create a `Learner` from scratch we need two things:\n",
    "- `DataLoaders` `dls`\n",
    "- an architecture `arch` for the `model`\n",
    "\n",
    "We already have our `dls` ready from the previous section, so let's focus on creating our architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad6867",
   "metadata": {},
   "source": [
    "## `Learner` for Multi-Label Classifier Fine-Tuning (Frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87d173-a05d-4192-87be-091c9f341471",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = torch.load(path_model/'dls_clas_full_64.pkl')\n",
    "# dls_clas = torch.load(path_model/'dls_clas_sample_64.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7730d2b5-79c2-4c84-a7a6-1db42b5d8d6e",
   "metadata": {},
   "source": [
    "### Architecture Creation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c616826-5167-4f8a-a7b4-3bcd3c81707b",
   "metadata": {},
   "source": [
    "To create an architecture that we can use to build a `model` we need to unwrap fastai's `get_text_classifier`. Let's do that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82b7ec-8f54-41f2-832c-aecbde0920e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_text_classifier(AWD_LSTM,\n",
    "#                             len(vocab),\n",
    "#                             n_out,\n",
    "#                             seq_len=seq_len,\n",
    "#                             config=config,\n",
    "#                             y_range=y_range,\n",
    "#                             drop_mult=drop_mult,\n",
    "#                             lin_ftrs=lin_ftrs,\n",
    "#                             max_len=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a304d-1799-4c89-98b4-553064c8b84f",
   "metadata": {},
   "source": [
    "The following is a dictionary where keys are the well known (possibly pretarianed) NLP architectures and values are the configurations, urls (where we can download the pretrained weights from) and other info we need to create that architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd780362-5dba-43ee-b783-dcb385411318",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_meta = {\n",
    "    AWD_LSTM: {'hid_name':'emb_sz', 'url':URLs.WT103_FWD, 'url_bwd':URLs.WT103_BWD,'config_lm':awd_lstm_lm_config, 'split_lm': awd_lstm_lm_split,            \n",
    "                'config_clas':awd_lstm_clas_config, 'split_clas': awd_lstm_clas_split\n",
    "              }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93f61b-7a9c-41ad-9906-6036b8a599d2",
   "metadata": {},
   "source": [
    "Let's define an architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ece6c37-74a4-42ad-85de-bd7adfd5dcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.models.awdlstm.AWD_LSTM"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch = AWD_LSTM\n",
    "arch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503d013-3482-423e-84f3-0321a1a5b20a",
   "metadata": {},
   "source": [
    "Let's get the value (information) of that architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d5ef8-a692-4987-9e02-9fa6ecfb3ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hid_name': 'emb_sz',\n",
       " 'url': 'https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd.tgz',\n",
       " 'url_bwd': 'https://s3.amazonaws.com/fast-ai-modelzoo/wt103-bwd.tgz',\n",
       " 'config_lm': {'emb_sz': 400,\n",
       "  'n_hid': 1152,\n",
       "  'n_layers': 3,\n",
       "  'pad_token': 1,\n",
       "  'bidir': False,\n",
       "  'output_p': 0.1,\n",
       "  'hidden_p': 0.15,\n",
       "  'input_p': 0.25,\n",
       "  'embed_p': 0.02,\n",
       "  'weight_p': 0.2,\n",
       "  'tie_weights': True,\n",
       "  'out_bias': True},\n",
       " 'split_lm': <function fastai.text.models.awdlstm.awd_lstm_lm_split(model)>,\n",
       " 'config_clas': {'emb_sz': 400,\n",
       "  'n_hid': 1152,\n",
       "  'n_layers': 3,\n",
       "  'pad_token': 1,\n",
       "  'bidir': False,\n",
       "  'output_p': 0.4,\n",
       "  'hidden_p': 0.3,\n",
       "  'input_p': 0.4,\n",
       "  'embed_p': 0.05,\n",
       "  'weight_p': 0.5},\n",
       " 'split_clas': <function fastai.text.models.awdlstm.awd_lstm_clas_split(model)>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = _model_meta[arch]\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b035d4-6e20-4b6c-bce8-1c64d42db569",
   "metadata": {},
   "source": [
    "Let's now get the default configuration required to build that architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1a331-9f20-4e4e-81fa-afda5a0e30cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emb_sz': 400,\n",
       " 'n_hid': 1152,\n",
       " 'n_layers': 3,\n",
       " 'pad_token': 1,\n",
       " 'bidir': False,\n",
       " 'output_p': 0.4,\n",
       " 'hidden_p': 0.3,\n",
       " 'input_p': 0.4,\n",
       " 'embed_p': 0.05,\n",
       " 'weight_p': 0.5}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = None\n",
    "config = ifnone(config, meta['config_clas']).copy()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd7d66-374b-4050-9fdc-947d2634f800",
   "metadata": {},
   "source": [
    "Alternatively, we could have just grabbed the configuration provided by a fastai convenienece dictionary called `awd_lstm_clas_config`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e6a60-0311-40a2-a53a-9a38bbeb7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = awd_lstm_clas_config.copy()\n",
    "# config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58bd7bb-cbf7-415b-aa5d-3fbeaff5c0e8",
   "metadata": {},
   "source": [
    "**Dropout**:  \n",
    "As we can see the the default configuration has some default dropout probabilities. We can use a multiplier to multiply with each of these probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6fb7f-3b77-40ae-af13-c0de0ceaa0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_mult = 0.1\n",
    "for k in config.keys():\n",
    "    if k.endswith('_p'): config[k] *= drop_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a1573-4c9e-4612-8361-01e0d6b4d81d",
   "metadata": {},
   "source": [
    "NOT SURE (Find out later!) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f462b4-09e7-42b8-be7f-601aa35d7f11",
   "metadata": {},
   "source": [
    "Note: In fastai `ps` was $0.1$, we made it $0.01$ (because in extreme multi-label classification we want to drop out less activations, but need to investigate this later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b58f6-892b-44da-a083-3259976f650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.1], [50])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_ftrs, ps = None, None\n",
    "if lin_ftrs is None: lin_ftrs = [50]\n",
    "if ps is None: ps = [0.1] * len(lin_ftrs)\n",
    "ps, lin_ftrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba952c-fb63-4d44-8bfc-333e024f44ea",
   "metadata": {},
   "source": [
    "Let's find out the total number of classes from dls (becuase we need this info to build the architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd7aab-dbb2-4798-bef6-54436d3a5d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8922"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_out = get_c(dls_clas)\n",
    "n_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fbb399-a743-4c5f-9780-cebaf24a7fd2",
   "metadata": {},
   "source": [
    "The embedding size as per the default config is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9285724-f6c3-4a5e-994d-2ced8d98901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_sz = config.get('emb_sz')\n",
    "emb_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb01f97-9429-4256-8644-69ca11ebceb2",
   "metadata": {},
   "source": [
    "The decoder for text classification will be a `PoolingLinearClassifier` with two linear layers:\n",
    "- the first layer will have number of input features 1200 and number of output features 50\n",
    "- the second layer will have number of input features 50 and number of output features equal to `n_out`\n",
    "\n",
    "Let's make these layer configuarion in a list called `layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3639b68-4d93-43ef-a11d-b9dc9acef5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1200, 50, 8922]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [emb_sz * 3] + lin_ftrs + [n_out]\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4bdb99-8da0-47a2-b8a5-63062250e920",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65463d-0406-47f9-9bdd-17aa1aa92b7a",
   "metadata": {},
   "source": [
    "**IMPORTANT**: \n",
    "\n",
    "In fastai the number of output features coming out was 400. Then we did a `masked_concat_pool` so the final output fed to the `PoolingLinearClassifier` was 1200. In order to implement **attention** we concat a context of 400 to those previous 1200 features. So the total number output features with attention is 1200+400=1600. Hence the change below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73db0fe-b8b8-45bd-9a08-22f4c02d207a",
   "metadata": {},
   "source": [
    "If using attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7b2c9-0f58-43e4-aa6d-56ecaf30c4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14275200, 50, 8922]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_attn = [n_out * (emb_sz * 3 + emb_sz) ] + lin_ftrs + [n_out]\n",
    "layers_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166adb1-61ff-49dd-bf3e-b99c1b7cd4bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ba145-5c72-4477-bd23-769620fa89bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1]\n",
      "[0.04000000000000001, 0.1]\n"
     ]
    }
   ],
   "source": [
    "print(ps)\n",
    "ps = [config.pop('output_p')] + ps\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b45863-8725-48b0-96b6-0ecb7d8fdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = config.pop('init') if 'init' in config else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1ccf4-c31e-4f86-afe3-894012c1a52c",
   "metadata": {},
   "source": [
    "Now we have reached a point where we can make our Encoder (which in this case will be an AWD_LSTM that is the architecture that we are using). To make the `AWD_LSTM` module we need to pass the `vocab_sz` and the `config` dictionary (after peeling off `output_p` and `init`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f064f4-cee6-4ffb-ae25-dfbee1d1a375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60008"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = dls_clas.vocab[0]\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e8f7d-445e-4e56-8f11-b2cd660401ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch = <class 'fastai.text.models.awdlstm.AWD_LSTM'>, \n",
      "\n",
      " config = {'emb_sz': 400, 'n_hid': 1152, 'n_layers': 3, 'pad_token': 1, 'bidir': False, 'hidden_p': 0.03, 'input_p': 0.04000000000000001, 'embed_p': 0.005000000000000001, 'weight_p': 0.05}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{arch = }, \\n\\n {config = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95d77e-2d29-41a8-a245-18b4b5ffe01c",
   "metadata": {},
   "source": [
    "Let's take a look at how our AWD_LSTM model looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2995aa3-b27b-4b2f-b5c9-0852fa0470f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.models.awdlstm.AWD_LSTM"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587f7b8-ac5c-4b33-8f73-833f789ed511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.update({'bidir': True})\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868050ae-75a6-4806-8edd-768c2d4daa74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AWD_LSTM(\n",
       "  (encoder): Embedding(60008, 400, padding_idx=1)\n",
       "  (encoder_dp): EmbeddingDropout(\n",
       "    (emb): Embedding(60008, 400, padding_idx=1)\n",
       "  )\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDropout(\n",
       "      (module): LSTM(400, 1152, batch_first=True)\n",
       "    )\n",
       "    (1): WeightDropout(\n",
       "      (module): LSTM(1152, 1152, batch_first=True)\n",
       "    )\n",
       "    (2): WeightDropout(\n",
       "      (module): LSTM(1152, 400, batch_first=True)\n",
       "    )\n",
       "  )\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0): RNNDropout()\n",
       "    (1): RNNDropout()\n",
       "    (2): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch(vocab_sz=len(vocab), **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88175c53-ad17-42f1-a5f8-c88a825b7531",
   "metadata": {},
   "source": [
    "**An important thing (of immense practical importance) to note:**  \n",
    "Since we do not have infinite amount of GPU memory so we need to wrap our AWD_LSTM module inside a `SentenceEncoder`.  \n",
    "Need to elaborate further (later on!)\n",
    "\n",
    "fastai's `SentenceEncoder` takes the following positional and keyword arguments:\n",
    "- `bptt` (this is chunk size, mostly `seq_len`, the text document gets broken into)\n",
    "- `module` (this the module that we want to wrap a `SentenceEncoder` around)\n",
    "- `pad_idx` (has a default value of 1 everywhere in fastai)\n",
    "- `max_len` (has a default value 72 * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb2a45-9b36-4810-aac1-f05f1bbad878",
   "metadata": {},
   "source": [
    "We don't really need to write our own `SentenEncoder` we can just use fastai's. But it is exteremely important to understand this source code. So for the sake of completion we include it below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3a259-6e9e-48a8-bc13-765ae1119410",
   "metadata": {},
   "source": [
    "```python\n",
    "def _pad_tensor(t, bs):\n",
    "    if t.size(0) < bs: return torch.cat([t, t.new_zeros(bs-t.size(0), *t.shape[1:])])\n",
    "    return t\n",
    "\n",
    "class OurSentenceEncoder(Module):\n",
    "    def __init__(self, bptt, module, pad_idx=1, max_len=None):\n",
    "        store_attr('bptt,module,pad_idx,max_len')\n",
    "        \n",
    "    def reset(self): getattr(self.module, 'reset', noop)()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        bs, sl = input.size()\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.reset()\n",
    "        mask = input == self.pad_idx\n",
    "        outs, masks = [], []\n",
    "        for i in range(0, sl, self.bptt):\n",
    "#             import pdb; pdb.set_trace()\n",
    "            #Note: this expects that sequence really begins on a round multiple of bptt\n",
    "            real_bs = (input[:, i] != self.pad_idx).long().sum()\n",
    "            o = self.module(input[:real_bs, i: min(i+self.bptt, sl)])\n",
    "            if self.max_len is None or sl-i <= self.max_len:\n",
    "                outs.append(o)\n",
    "                masks.append(mask[:, i:min(i+self.bptt, sl)])\n",
    "        outs = torch.cat([_pad_tensor(o, bs) for o in outs], dim=1)\n",
    "        mask = torch.cat(masks, dim=1)\n",
    "        return outs, mask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe88bc-c8f2-4f80-bbdd-2eec21dbbf35",
   "metadata": {},
   "source": [
    "**IMPORTANT:** To make sure we look at the hidden state for each of the token in the entire document (meaning we want to be looking at `bs,seq_len,nh` where `seq_len` is the entire document length and not just `max_len`) before classifying, we need to let `SentenceEncoder` work with its default `max_len` (instead of passing something like $72*20$ which is what we would do for sentiment analysis)... [Elaborate this later explaining the difference between extreme multilabel classification and sentiment analysis...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5bd65-db5d-4a38-a3d4-22a2689636b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceEncoder(\n",
       "  (module): AWD_LSTM(\n",
       "    (encoder): Embedding(60008, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60008, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 72\n",
    "# encoder = OurSentenceEncoder(seq_len, arch(vocab_sz=len(vocab), **config), pad_idx=1, max_len=seq_len*20)\n",
    "encoder = SentenceEncoder(seq_len, arch(vocab_sz=len(vocab), **config), pad_idx=1, max_len=seq_len*20)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca29950-8589-435c-b531-5b4f3054af4a",
   "metadata": {},
   "source": [
    "So now have the encoder and ready to make the decoder. The decoder in this case would be a `PoolingLinearClassifier`. We will make it using the following information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508f151-91a8-4be5-b684-26994419f963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1200, 50, 8922], [0.04000000000000001, 0.1], 72)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_range = None\n",
    "layers, ps, seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7ab18-e599-415c-9dda-1a25b75f1d5a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7dfff8-b958-408c-8b23-c88f8a72059a",
   "metadata": {},
   "source": [
    "If using attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de5c3c-6788-48de-bd43-4a31308ff520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14275200, 50, 8922], [0.04000000000000001, 0.1], 72)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_attn, ps, seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb2217d-845c-4052-a7bc-7501383f55fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f4b72-e97d-4995-bc18-dc41b37c05e6",
   "metadata": {},
   "source": [
    "Eventhough now we can just use fastai's `PoolingLinearClassifier`, but later on we will need to modify this module (to incorporate *attention*). So for the purpose of customizability let's copy the source code from fastai and (shamelessly) call it `OurPoolingLinearClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383b6f5-054a-48aa-a526-4d1006174358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurPoolingLinearClassifier(Module):\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        if len(ps) != len(dims)-1: raise ValueError(\"Number of layers and dropout values do not match.\")\n",
    "        acts = [nn.ReLU(inplace=True)] * (len(dims) - 2) + [None]\n",
    "        layers = [LinBnDrop(i, o, p=p, act=a) for i,o,p,a in zip(dims[:-1], dims[1:], ps, acts)]\n",
    "        if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.bptt = bptt\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, mask = input\n",
    "        x = masked_concat_pool(out, mask, self.bptt)\n",
    "        x = self.layers(x)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff30bdf0-bb46-4501-b3fa-27bb938c76a3",
   "metadata": {},
   "source": [
    "#### Breaking down the `OurPoolingLinearClassifier.__init__`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05181fa3-c34d-4a05-9ab6-465158e96f72",
   "metadata": {},
   "source": [
    "Note that in the `__init__` while creating `OurPoolingLinearClassifier` `dims` is `layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca50ba7-601c-463e-957f-292ba63c8754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims = [1200, 50, 6594]\n"
     ]
    }
   ],
   "source": [
    "dims = layers\n",
    "print(f\"{dims = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7dfca9-017a-445c-a173-5b6aa1610728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps = [0.04000000000000001, 0.1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ps = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed6775-e6fa-41c7-bf40-a898d5b935ef",
   "metadata": {},
   "source": [
    "Also note that `bptt` is `seq_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c622186-bb58-44ea-8992-61dc700c4858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bptt = 72\n"
     ]
    }
   ],
   "source": [
    "bptt = seq_len\n",
    "print(f\"{bptt = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bc179-6f0c-4251-b239-af297d0aff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100328b-e2fc-4c89-941b-41e42c7ba125",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ps) != len(dims) - 1: raise ValueError(\"Number of layers and dopout values do not match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf72060-c6e3-4b0c-bfac-21f5ec742c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ReLU(inplace=True), None]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts = [nn.ReLU(inplace=True)] * (len(dims) - 2) + [None]\n",
    "acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f771f-0fde-4c02-b2e8-5c097f364fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1200, o = 50, p = 0.04000000000000001, a = ReLU(inplace=True)\n",
      "i = 50, o = 6594, p = 0.1, a = None\n"
     ]
    }
   ],
   "source": [
    "for i, o, p, a in zip(dims[:-1], dims[1:], ps, acts):\n",
    "    print(f\"{i = }, {o = }, {p = }, {a = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd9385-4bf2-4811-9e0e-26653e59af6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LinBnDrop(\n",
       "   (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (1): Dropout(p=0.04000000000000001, inplace=False)\n",
       "   (2): Linear(in_features=1200, out_features=50, bias=False)\n",
       "   (3): ReLU(inplace=True)\n",
       " ),\n",
       " LinBnDrop(\n",
       "   (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (1): Dropout(p=0.1, inplace=False)\n",
       "   (2): Linear(in_features=50, out_features=6594, bias=False)\n",
       " )]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [LinBnDrop(i, o, p=p, act=a) for i, o, p, a in zip(dims[:-1], dims[1:], ps, acts)]\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48a48d-d230-416a-ba88-7cd21a8720e7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfefdcf1-e3aa-4146-877e-4e60c6e8d8ca",
   "metadata": {},
   "source": [
    "CAUTION: Work in Progress!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252ceaa-2506-4008-8330-6f39af3cdd4d",
   "metadata": {},
   "source": [
    "#### Attempt #1: Implementing [CAML](https://arxiv.org/pdf/1802.05695.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f63074-f26a-40e4-bd5d-dcdafcf60129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAML1(Module):\n",
    "    def __init__(self, dims, ps, bptt):\n",
    "        layers = [LinBnDrop(dims[0], 50, p=ps, act=nn.ReLU(inplace=True))]\n",
    "        layers.append(LinBnDrop(50, n_out, p=0.1, act=None))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.bptt = bptt\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, mask = input\n",
    "        x = masked_concat_pool(out, mask, self.bptt)\n",
    "        x = self.layers(x)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51aa55-960d-4c1f-86be-4e096bf92d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAML1(\n",
       "  (layers): Sequential(\n",
       "    (0): LinBnDrop(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.04, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): LinBnDrop(\n",
       "      (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): Linear(in_features=50, out_features=6594, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAML1(dims=[1200, 6594], ps=0.04, bptt=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa4ea9-78c9-4148-bed5-1a2c2dce996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAML2(Module):\n",
    "    def __init__(self, dims, ps, bptt):\n",
    "        self.layer = LinBnDrop(dims[0], dims[1], p=ps, act=None)\n",
    "        self.bptt = bptt\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, mask = input\n",
    "        x = masked_concat_pool(out, mask, self.bptt)\n",
    "        x = self.layer(x)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641c519-7cb8-4a9e-8847-647e61b49536",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = CAML2(dims=[1200, 8922], ps=0.04, bptt=seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbeb380-3fb8-4828-aa1a-2c1a63ae3adf",
   "metadata": {},
   "source": [
    "Note: Also try `CAML2` w/o dropouts and batch normalization (Verify this, but as far as what I found it does not work well as compared to /w batch normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da88737-22bf-4e8a-a8c2-da8a717c50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinBn2Drop(nn.Sequential):\n",
    "    \"Module grouping `BatchNorm2d`, `Dropout` and a `Linear` layer with just one output feature\"\n",
    "    def __init__(self, n_lbs, n_fts, bn=True, p=0., act=None, lin_first=False):\n",
    "        layers = [BatchNorm(n_lbs, ndim=1)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        lin = [nn.Linear(n_fts, 1, bias=not bn)]\n",
    "        if act is not None: lin.append(act)\n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc65d47-ef12-4dc2-9066-026c83507cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAML3(Module):\n",
    "    def __init__(self, ps, bptt):\n",
    "        self.layers = LinBn2Drop(n_out, emb_sz, p=ps, act=None) # deb\n",
    "        self.bptt = bptt\n",
    "        self.emb_label = nn.Embedding(n_out, emb_sz) # deb\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        out, _ = input\n",
    "        # breakpoint()\n",
    "        # x = masked_concat_pool(out, mask, self.bptt)\n",
    "        attn_wgts = out @ self.emb_label.weight.transpose(0, 1) # deb\n",
    "        attn_wgts = F.softmax(attn_wgts, 1) # deb\n",
    "        ctx = attn_wgts.transpose(1,2) @ out # deb\n",
    "        \n",
    "        x = self.layers(ctx)\n",
    "        x = x.view(x.shape[0], x.shape[1])\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa01025-e307-4d7b-8712-eaf379b0d388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAML3(\n",
       "  (layers): LinBn2Drop(\n",
       "    (0): BatchNorm1d(6594, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.04, inplace=False)\n",
       "    (2): Linear(in_features=400, out_features=1, bias=False)\n",
       "  )\n",
       "  (emb_label): Embedding(6594, 400)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAML3(ps=0.04, bptt=seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436a6d7-05b5-437a-8653-e904c2f34e2a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf28d0-6fc7-4b9b-a81b-360c3b813ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1406, 400]), device(type='cuda', index=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.randn((128, 1406, 400)).cuda()\n",
    "out.shape, out.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef165fe3-2731-4b05-925d-3945d087d71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1406, 6594]), device(type='cuda', index=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_wgts = torch.randn((128, 1406, 6594)).cuda()\n",
    "attn_wgts.shape, attn_wgts.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272301d7-d62a-4f01-96cc-a3fc1c5a273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out[:, :, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109b081-cdc1-4595-8cce-07e6172a50fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 1406])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_wgts.transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0250d-ab3c-4225-9a3f-61acf5c515c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = attn_wgts.transpose(1,2) @ out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97092628-e48d-4a0d-b4b0-74f61730bf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc593063-29f6-4c2b-87fa-695fcaa71979",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = CAML3(ps=0.04, bptt=seq_len).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b94e4e-fb01-434c-9fd8-77133e52db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, *_ = test_model((out, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246428d7-d707-4168-99aa-c14359afcfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f094b-29cd-4a9b-b7e0-b04629cac9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f6655-0d04-49d7-8df1-fc0ac82dd606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90ee9611-d923-487d-8893-87b55dc53af8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a62886-f042-417f-b68d-1b5ee8c44ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAML4(Module):\n",
    "    def __init__(self, dims, ps, bptt):\n",
    "        self.layer = LinBnDrop(dims[0], dims[1], p=ps, act=None)\n",
    "        self.bptt = bptt\n",
    "#         self.emb_label = nn.Embedding(n_out, emb_sz) # deb\n",
    "#         self.linear_final = nn.Linear(emb_sz) \n",
    "\n",
    "    def forward(self, input):\n",
    "        out, mask = input\n",
    "        x = masked_concat_pool(out, mask, self.bptt)\n",
    "#         attn_wgts = x @ self.emb_label.weight.transpose(0, 1) # deb\n",
    "#         attn_wgts = F.softmax(attn_wgts, 1) # deb\n",
    "#         ctx = out[:, :, None] * attn_wgts[..., None] # deb\n",
    "#         ctx = ctx.sum(1)\n",
    "        x = self.layer(x)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168b162-cc40-436f-9fe5-af607e680789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAML3(\n",
       "  (layers): LinBn2Drop(\n",
       "    (0): BatchNorm1d(6594, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.04, inplace=False)\n",
       "    (2): Linear(in_features=400, out_features=1, bias=False)\n",
       "  )\n",
       "  (emb_label): Embedding(6594, 400)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = CAML3(ps=0.04, bptt=seq_len)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a366ee-712a-4f2a-aec1-e666c7fc2de3",
   "metadata": {},
   "source": [
    "#### Attempt #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1685d-ccb6-4bd0-a3a5-fb76c6db1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurPoolingAttentionClassifier(Module):\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        if len(ps) != len(dims)-1: raise ValueError(\"Number of layers and dropout values do not match.\")\n",
    "        acts = [nn.ReLU(inplace=True)] * (len(dims) - 2) + [None]\n",
    "        layers = [LinBnDrop(i, o, p=p, act=a) for i,o,p,a in zip(dims[:-1], dims[1:], ps, acts)]\n",
    "        if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.bptt = bptt\n",
    "        \n",
    "        # new\n",
    "        label_emb_sz=49\n",
    "        self.emb_label = nn.Embedding(n_out, emb_sz)\n",
    "        self.lin = nn.Linear(emb_sz, emb_sz)\n",
    "        self.lin_for_tok_red = nn.Linear(seq_len*20, 50)\n",
    "        self.lin_for_rep_compress = nn.Linear(emb_sz, label_emb_sz)\n",
    "        self.lin_for_rep_decompress = nn.Linear(label_emb_sz, emb_sz)\n",
    "        self.V = self._init_param(label_emb_sz)\n",
    "        # new\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out, mask = input\n",
    "        x = masked_concat_pool(out, mask, self.bptt)\n",
    "        \n",
    "        # new\n",
    "        num_tok = out.shape[1]\n",
    "        out = F.pad(out, (0,0,0,seq_len*20-num_tok))\n",
    "        bs = out.shape[0]\n",
    "        label_indices = torch.arange(n_out, device=out.device)\n",
    "        labels = label_indices.repeat(bs, 1)\n",
    "        after_grabbing_label_embedding = self.emb_label(labels)\n",
    "        after_first_matmul = self.lin(after_grabbing_label_embedding)\n",
    "        \n",
    "        out = self.lin_for_rep_compress(out)\n",
    "        after_first_matmul = self.lin_for_rep_compress(after_first_matmul)\n",
    "        \n",
    "        out = out.permute(0,2,1).contiguous()\n",
    "        out = self.lin_for_tok_red(out)\n",
    "        out = out.permute(0,2,1).contiguous()\n",
    "        after_nonlinearity = torch.tanh(out[:, :, None] + after_first_matmul[:,None])\n",
    "        attn_wgts = (after_nonlinearity @ self.V)\n",
    "        ctx = (out[:, :, None] * attn_wgts[..., None])\n",
    "        ctx = ctx.sum(1)\n",
    "        \n",
    "        ctx = self.lin_for_rep_decompress(torch.relu(ctx))\n",
    "        \n",
    "        x = x[:, None]\n",
    "        x = x.repeat(1, n_out, 1)\n",
    "        x = torch.cat((x, ctx), dim=-1)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # new\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        return x, out, out\n",
    "    \n",
    "    def _init_param(self, *sz): \n",
    "        return nn.Parameter(torch.randn(sz)/math.sqrt(sz[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec42aa6",
   "metadata": {},
   "source": [
    "CAUTION ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e377ae-4397-451d-b66e-bb9c1bc94b5c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02468265-c1a9-4e94-a771-c5ed5eb29bd9",
   "metadata": {},
   "source": [
    "#### Creating the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b2782-747e-48c8-9cb2-16af730132d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = OurPoolingLinearClassifier(layers, ps, bptt=seq_len, y_range=y_range)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f8c0a-c82a-4b11-80b1-19ef8e53e518",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8bbbe3-d05a-4819-a455-5d1acaaa3704",
   "metadata": {},
   "source": [
    "If using attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e31d41-3013-4f1f-8ca7-1d9322dfa3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OurPoolingAttentionClassifier(\n",
       "  (layers): Sequential(\n",
       "    (0): LinBnDrop(\n",
       "      (0): BatchNorm1d(10550400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.04000000000000001, inplace=False)\n",
       "      (2): Linear(in_features=10550400, out_features=50, bias=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): LinBnDrop(\n",
       "      (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): Linear(in_features=50, out_features=6594, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (emb_label): Embedding(6594, 400)\n",
       "  (lin): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (lin_for_tok_red): Linear(in_features=1440, out_features=50, bias=True)\n",
       "  (lin_for_rep_compress): Linear(in_features=400, out_features=49, bias=True)\n",
       "  (lin_for_rep_decompress): Linear(in_features=49, out_features=400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = OurPoolingAttentionClassifier(layers_attn, ps, bptt=seq_len, y_range=y_range)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8d0f1-16c0-480b-b5a2-17dfa71dba9e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f406d-f27a-4e1d-9261-63c967c75b93",
   "metadata": {},
   "source": [
    "#### Creating the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cb1c6c-6a42-4e9a-afc4-643f828c1700",
   "metadata": {},
   "source": [
    "The last thing that we need to do in order to create our architecture module is stack the `encoder` and the `decoder` using `SequentialRNN` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93a65e-3dde-4d96-b06f-d0a37ebc22e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60008, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60008, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): CAML2(\n",
       "    (layer): LinBnDrop(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.04, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=8922, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialRNN(encoder, decoder)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9e226-c693-4d07-8ab0-ba5fdae75318",
   "metadata": {},
   "outputs": [],
   "source": [
    "if init is not None: model = model.apply(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463e7ad-aafa-487e-b883-ef243d5354f5",
   "metadata": {},
   "source": [
    "At this point we are done creating the `model` (that is have replicated every step inside `get_text_classifier`). Next, let's move on to `text_classifier_learner`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f9c72-725c-41fa-91a7-009d29b03da3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7349c-3541-4f3b-b6f1-99e073790771",
   "metadata": {},
   "source": [
    "#### Scratchpad: (Build the pieces to implement attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d1d36-5e9d-43e7-9e41-b4ecad7a0588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 18398]),\n",
       " torch.Size([128, 6594]),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = dls_clas.one_batch()\n",
    "x.shape, y.shape, x.device, y.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f359e1-b495-46fc-b571-a5196e7dce96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SentenceEncoder(\n",
       "   (module): AWD_LSTM(\n",
       "     (encoder): Embedding(60008, 400, padding_idx=1)\n",
       "     (encoder_dp): EmbeddingDropout(\n",
       "       (emb): Embedding(60008, 400, padding_idx=1)\n",
       "     )\n",
       "     (rnns): ModuleList(\n",
       "       (0): WeightDropout(\n",
       "         (module): LSTM(400, 1152, batch_first=True)\n",
       "       )\n",
       "       (1): WeightDropout(\n",
       "         (module): LSTM(1152, 1152, batch_first=True)\n",
       "       )\n",
       "       (2): WeightDropout(\n",
       "         (module): LSTM(1152, 400, batch_first=True)\n",
       "       )\n",
       "     )\n",
       "     (input_dp): RNNDropout()\n",
       "     (hidden_dps): ModuleList(\n",
       "       (0): RNNDropout()\n",
       "       (1): RNNDropout()\n",
       "       (2): RNNDropout()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " OurPoolingAttentionClassifier(\n",
       "   (layers): Sequential(\n",
       "     (0): LinBnDrop(\n",
       "       (0): BatchNorm1d(10550400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (1): Dropout(p=0.04000000000000001, inplace=False)\n",
       "       (2): Linear(in_features=10550400, out_features=50, bias=False)\n",
       "       (3): ReLU(inplace=True)\n",
       "     )\n",
       "     (1): LinBnDrop(\n",
       "       (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (1): Dropout(p=0.1, inplace=False)\n",
       "       (2): Linear(in_features=50, out_features=6594, bias=False)\n",
       "     )\n",
       "   )\n",
       "   (emb_label): Embedding(6594, 400)\n",
       "   (lin): Linear(in_features=400, out_features=400, bias=True)\n",
       "   (lin_for_tok_red): Linear(in_features=1440, out_features=50, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder, decoder = model[0], model[1]\n",
    "encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266325d-a054-4a83-a9d0-86f1c0e34576",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = encoder.cuda(), decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4fc6cb-6bcf-40bc-b913-2b4e1f6679e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cccec-43cb-4b81-b236-2c5ba5b240e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1406, 400]), torch.Size([2, 1406]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, mask = encoder(x[:2])\n",
    "out.shape, mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf8887-f670-49b4-92e3-06f40ff0a34b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc5c67-d8a0-4517-883e-81344a087851",
   "metadata": {},
   "source": [
    "After getting the output from the `encoder` we will do a masked concat pooling as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff6b5f-6b2a-403a-bf7b-0300f7f802c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_concat_pool = masked_concat_pool(out, mask, bptt=seq_len)\n",
    "# out_concat_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07ce8f-44ae-4e08-a891-72010902485f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cde9e1-c552-468a-a84f-5f8532d88cdd",
   "metadata": {},
   "source": [
    "Testing the decoder which is `OurPoolingAttentionCalssifier`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70cba84-87cd-4d20-97fe-3e5cdfc53b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# out, mask, preds, model, encoder, decoder, x, y, dls_clas, learn, m, out_concat_pool = None, None, None, None, None, None, None, None, None, None, None, None\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c203e8c-4d84-4363-8ada-c5fa8de3629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, _, _ = decoder((out, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6256c9-9fc3-4da7-beff-73c413dc1521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6594])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bfa54-0cb8-40bb-8114-340a85274a5d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732def1-038e-4c24-804a-ac5f51e47d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var = locals().get('out_concat_pool', 'Shit!!!')\n",
    "# print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb917d-90c5-4a91-97f5-3408714f3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, encoder, dls_clas, out, mask, ind_var = None, None, None, None, None, None, None\n",
    "# import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef118554-9522-4a44-92d8-201627cab2b9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021fd3e9-b533-4d14-a10a-b1f8b14a7351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 400, 6594)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs, emb_sz, n_out = 128, 400, get_c(dls_clas)\n",
    "bs, emb_sz, n_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddf867-81e8-4a42-b3d3-82cdcb13314c",
   "metadata": {},
   "source": [
    "`out` is the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d9ee4-876e-4b99-99ef-dca63b291566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1406, 400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "out = torch.randn(bs, 1406, emb_sz, dtype=torch.float)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c1c82-d0ca-41f0-a207-08ada45359bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659ac13-2b44-4c67-96d3-4b47e507ccd4",
   "metadata": {},
   "source": [
    "- The `out` is the memory: `(bs, sl, nh)`\n",
    "- meaning of the dimensions:\n",
    "    * For each of the 128 documents we are focussing on the last 1406 tokens each having a hidden vector of length 400\n",
    "- Let us say before predicting a label we want to know which part of the memory we want to pay attention to for that particular label\n",
    "- We need to compute the attention weights for each of the `sl` many `(bs,nh)`\n",
    "- The shape of the attention weights would be `bs,sl`\n",
    "---\n",
    "- To compute the attention weights we will use a mini \"neural network\"\n",
    "- **What would be the input to this mini-nn? The input to this neural network would be that particular label embedding that we want to predict**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b54c3-3db6-4fee-89a5-1183e0eab0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "n_labels = n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427469cf-122a-4395-af77-6e32e378ecbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "label_emb_sz = 49\n",
    "emb_label = nn.Embedding(n_labels, emb_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81839ad3-2963-45da-8fb1-b256293f7416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 1.0915, -0.2077, -0.4202,  ..., -0.1955, -0.0613, -0.1809],\n",
       "         [ 0.6431, -0.7658, -0.9118,  ...,  0.7211, -0.7179,  0.5732],\n",
       "         [-0.0820, -0.8173, -0.0974,  ..., -1.7775, -1.6814, -2.2647],\n",
       "         ...,\n",
       "         [ 0.3422, -0.4851, -0.4024,  ...,  0.2219,  0.8045,  0.0061],\n",
       "         [-1.0910, -0.1145,  2.0704,  ..., -0.0486,  0.2683, -0.0494],\n",
       "         [ 1.3110, -0.1406,  1.5179,  ..., -2.2415,  0.9447,  0.8938]], requires_grad=True),\n",
       " torch.Size([6594, 400]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_label.weight, emb_label.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a6389-861b-4f7c-a502-4eb9a2532330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0915,  0.6431, -0.0820,  ...,  0.3422, -1.0910,  1.3110],\n",
       "        [-0.2077, -0.7658, -0.8173,  ..., -0.4851, -0.1145, -0.1406],\n",
       "        [-0.4202, -0.9118, -0.0974,  ..., -0.4024,  2.0704,  1.5179],\n",
       "        ...,\n",
       "        [-0.1955,  0.7211, -1.7775,  ...,  0.2219, -0.0486, -2.2415],\n",
       "        [-0.0613, -0.7179, -1.6814,  ...,  0.8045,  0.2683,  0.9447],\n",
       "        [-0.1809,  0.5732, -2.2647,  ...,  0.0061, -0.0494,  0.8938]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_label.weight.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ec9d4-0469-47e1-9f2e-d6cbe68bdb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 6594])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_label.weight.transpose(0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfccb095-479c-4f03-80b5-f193eee1d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_label.weight.data = emb_label.weight.type(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0cf09f-731e-4f62-be53-1b0617088f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_label.weight.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65750da8-c7d0-444d-aca9-591f732340ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_label.__dict__\n",
    "# [o for o in dir(emb_label) if not o.startswith(\"__\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa30a2-c608-40d8-93df-0e70e14feee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(6594, 400)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d87ca9-b07a-4ac3-921b-1b1c53a42e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randint(0,5,(10,5))\n",
    "# x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0ea45-0c4d-440a-a3f9-a813cd69c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_label(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c0629-8653-44bf-81e0-627bcd235a9f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e85231-a914-40cf-8bc0-f75e678b869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.arange(5)[:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710a119-0626-4236-b485-e6427bf9c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.ones(5, bs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060ee0f-25ba-4b9f-bf23-575fcafb1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = torch.ones(bs, 5, dtype=torch.int64) * torch.arange(5)\n",
    "# labels.shape, labels.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6299b1e-813e-404d-9899-e13c0b0bda66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "label_indices = torch.arange(n_labels)\n",
    "labels = label_indices.repeat(128, 1)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6a792-d7f2-427a-9cb4-ec29e6e81bef",
   "metadata": {},
   "source": [
    "Note: In `labels`,\n",
    "- 0th axis is bs\n",
    "- 1st axis is label index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9d3d9-bccd-44b5-b6a2-3f2a98f29a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    2,  ..., 6591, 6592, 6593],\n",
       "        [   0,    1,    2,  ..., 6591, 6592, 6593],\n",
       "        [   0,    1,    2,  ..., 6591, 6592, 6593],\n",
       "        ...,\n",
       "        [   0,    1,    2,  ..., 6591, 6592, 6593],\n",
       "        [   0,    1,    2,  ..., 6591, 6592, 6593],\n",
       "        [   0,    1,    2,  ..., 6591, 6592, 6593]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels[:10, :]\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12446676-dde2-463f-a73b-dd87e97f34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_label = labels[:,0]\n",
    "# a_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb40aee-791d-48dc-89e3-34980c768030",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ee777-8833-4f56-a788-859035cd5de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "after_grabbing_label_embedding = emb_label(labels)\n",
    "after_grabbing_label_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db87f9-6e87-476d-bf5e-1bb29b22da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.getsizeof(after_grabbing_label_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40591db4-7a6d-4126-a6f5-d53435922bec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfbd58c-90ff-424f-8f58-d650ddc90315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.randint(0, 5, size=(3,2,4))\n",
    "# b = torch.randint(0, 5, size=(4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ccb93-0ece-472f-aaaa-e3c21f5787a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24738d52-c8ec-443a-a79a-b3e35fd74eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816775e-ad67-42db-8765-19a9f17c7bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a@b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d797f899-4108-447e-ba24-0388dcd9a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a@b).permute(1,0,2).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a85b56-87f2-4a11-93ec-b8ccfcde5590",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b4fe5-5e0d-4e50-8689-024b875a2b9b",
   "metadata": {},
   "source": [
    "*Step 0 of mini-nn:* `after_grabbing_label_embedding` is the input to the mini-nn which will answer the following question:   \n",
    "\n",
    "**Which part of the memory `out` should I pay attention before making a prediction about a label**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cb457-2ba7-4617-b119-8c4dc088e0f6",
   "metadata": {},
   "source": [
    "*Step 1 of mini-nn: Doing the first matrix multiply:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3cf029-feff-4636-b1f0-cc36aadd4b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=400, out_features=400, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "lin = nn.Linear(emb_sz, emb_sz)\n",
    "lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62ae17-d8b3-46d7-9b5f-2f369c1df2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin.weight.data = lin.weight.type(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc9c04-ddf7-4b99-b38e-8a93544075ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 6594, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "print(after_grabbing_label_embedding.shape)\n",
    "after_first_matmul = lin(after_grabbing_label_embedding)\n",
    "after_first_matmul.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb57842-993f-42c9-9d80-992124cca0d6",
   "metadata": {},
   "source": [
    "At this point the axes mean the following:\n",
    "- 0th axis is the bs\n",
    "- 1st axis is label\n",
    "- 2nd axis is emb_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea1d649-fefa-43ea-ab7d-f8ff80ec6dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after_first_matmul = after_first_matmul.permute(1,0,2).contiguous()\n",
    "# after_first_matmul.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0146a77-1a0b-4a50-88fd-c948b40798e0",
   "metadata": {},
   "source": [
    "Think of it this way:  \n",
    "In each of the 128 documents, each of the 6594 labels has a representation vector of length 400."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83004f66-044d-4ec4-af4d-cfee287b26a6",
   "metadata": {},
   "source": [
    "*Step 2 of mini-nn: Applying non-linearity:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4f2a9-6f24-420a-beb9-6ec89aaccf53",
   "metadata": {},
   "source": [
    "But before applying a non-linearity we want to add the `after_first_matmul` to the entire memory `out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5623d1d-8d5a-4a49-9a09-e1e9524eb922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1406, 400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b68a6-e4f1-4a3f-9a85-90cfe14a3bd4",
   "metadata": {},
   "source": [
    "To this memory we will add a singleton dimension at the 2nd position (think of this as adding a label specific dimension to our memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c62f5-d3c8-4314-900d-960f0ecf0c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out[:, :, None].shape = torch.Size([128, 1406, 1, 400])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{out[:, :, None].shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b4a11-d5f8-4b67-b6ef-78a610d3b486",
   "metadata": {},
   "source": [
    "We want to literally add to our memory the `after_first_matmul`. So to line things up perfectly we add a singleton dimension at the 1st position in `after_first_matmul` (think of this as the token specific dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744381b5-ce24-4bb6-9b77-491e680fbe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_first_matmul[:,None].shape = torch.Size([128, 1, 6594, 400])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{after_first_matmul[:,None].shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e1dc5-cff0-4c81-ace6-c41b3b268809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.type(torch.half), after_first_matmul.type(torch.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3a663-0bb8-437a-a954-213838b6ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = after_nonlinearity = torch.tanh(out[:, :50, None] + after_first_matmul[:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed88cbd8-f591-47b4-8bea-d0126235f7d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7772e7d5-e0d0-4f5a-89c5-8d5febf5a4f8",
   "metadata": {},
   "source": [
    "At this point we are faced with a realistic issue:  \n",
    "We cannot afford to \n",
    "- consider all the 1406 tokens in our memory (stored in `out`), and \n",
    "- have those tokens have a representation length of 400.   \n",
    "Solution: (Like always) We we will have to do a matrix multiplication to preform compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be8705-595c-482f-8e5a-a1bf3ea91af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1406, 400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f79543-e277-4b7f-9839-fee8cb7d9aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=400, out_features=49, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "lin_for_rep_red = nn.Linear(emb_sz, label_emb_sz)\n",
    "lin_for_rep_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e0183-054c-4e0d-8935-68999fdb9f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1406, 49])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "out = lin_for_rep_red(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1055f6a4-d746-45fa-a95c-159f1337a0a9",
   "metadata": {},
   "source": [
    "Let's also compress the representation length of the the last dimension in `after_first_matmul`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ffab8-811a-481e-af41-58a675a853ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_first_matmul.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4625f-482f-4fbc-aeca-24c0b7433042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 49])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "after_first_matmul = lin_for_rep_red(after_first_matmul)\n",
    "after_first_matmul.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf30c1a-a0c3-4211-b1c7-d75f3d8bd5df",
   "metadata": {},
   "source": [
    "Now, let's compress the number of tokens in `out` from 1406 to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef4aef-0898-4069-acee-30f842fe504c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 49, 1406])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "out = out.permute(0,2,1).contiguous()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f44c5-71fd-4d38-8927-952d25e31ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1406, out_features=50, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "lin_for_tok_red = nn.Linear(1406, 50)\n",
    "lin_for_tok_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f895fe7-efdf-4a52-abb8-49aadc9d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 49, 50])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "out = lin_for_tok_red(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ccfee-5160-4207-b496-890cbf4e62eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 50, 49])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "out = out.permute(0,2,1).contiguous()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a6e0a-f474-4d0a-9854-8bad43117709",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b3709-f02b-48c3-81c2-fb42a2772a19",
   "metadata": {},
   "source": [
    "Now we are good to add `after_first_matmul` to our compressed memory (stored in `out`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f7d97-3a17-49f6-b1f3-998da1e655be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape = torch.Size([128, 50, 49]), after_first_matmul.shape = torch.Size([128, 6594, 49])\n",
      "after_nonlinearity.shape = torch.Size([128, 50, 6594, 49])\n"
     ]
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "print(f\"{out.shape = }, {after_first_matmul.shape = }\")\n",
    "after_nonlinearity = torch.tanh(out[:, :, None] + after_first_matmul[:,None])\n",
    "print(f\"{after_nonlinearity.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356937d1-5d32-4c3f-9f8d-0748fd86e165",
   "metadata": {},
   "source": [
    "Note what each of the dimension of `after_nonlinearity` means:  \n",
    "In each of the **128 documents**, we are focussing on the last **50 (compressed from 1406) tokens**. Each one of those tokens has a **hidden (compressed) represenation vector of length 49** for each of the **6594 labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f048cc-e446-4fe3-b6ab-617154cc5f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_nonlinearity.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed80beb-8975-4ed5-b928-5a1d9e0605c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10336495-29bb-4b33-b924-c60edc9e0441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "def init_param(*sz): print(f\"{sz = }\"); return nn.Parameter(torch.randn(sz)/math.sqrt(sz[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca503b28-1950-4065-9a7a-4f2e97994774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_param(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec104dd-ef7f-4c55-8968-74f51ef1042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_param(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825253f-00d6-452d-b419-d60e1a59ee41",
   "metadata": {},
   "source": [
    "`V` is for the second matrix multiply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d8f02-7975-43f3-94dc-d9515ab70123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sz = (49,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([49])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "V = init_param(label_emb_sz)\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ede53-accc-491a-8aff-ed11ff4f852c",
   "metadata": {},
   "source": [
    "*Step 3 of mini-nn: Doing a second matrix multiply:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f8f6a-89c3-4889-9c15-f1aa99255382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 50, 6594])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "attn_wgts = (after_nonlinearity @ V)\n",
    "attn_wgts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a18273-bdd1-448a-9ab0-749d4e8c2453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5042, -0.4067,  0.2481,  ...,  0.4743,  0.6855,  0.3966],\n",
       "         [ 0.3884, -0.3913,  0.1412,  ...,  0.3579,  0.4673,  0.2750],\n",
       "         [ 0.3630, -0.3703,  0.1130,  ...,  0.3201,  0.4854,  0.1989],\n",
       "         ...,\n",
       "         [ 0.5318, -0.3458,  0.2158,  ...,  0.4740,  0.6765,  0.3409],\n",
       "         [ 0.0388, -0.7875, -0.2704,  ...,  0.0134,  0.2340, -0.1294],\n",
       "         [ 0.2823, -0.5718, -0.0493,  ...,  0.2117,  0.4972,  0.1334]],\n",
       "\n",
       "        [[ 0.1241, -0.7451, -0.1825,  ...,  0.0953,  0.2921, -0.0203],\n",
       "         [ 0.2909, -0.5416,  0.0215,  ...,  0.2523,  0.4723,  0.1960],\n",
       "         [ 0.1338, -0.5911, -0.0852,  ...,  0.2431,  0.3780,  0.0520],\n",
       "         ...,\n",
       "         [ 0.7050, -0.1777,  0.4014,  ...,  0.7176,  0.8624,  0.6345],\n",
       "         [ 0.4314, -0.4561,  0.2013,  ...,  0.4348,  0.5315,  0.3565],\n",
       "         [ 0.0490, -0.7727, -0.2656,  ..., -0.0220,  0.2699, -0.0791]],\n",
       "\n",
       "        [[ 0.9006,  0.1438,  0.7388,  ...,  1.0048,  1.0604,  0.8983],\n",
       "         [-0.1268, -0.8799, -0.4262,  ..., -0.1184, -0.0549, -0.2492],\n",
       "         [ 0.3554, -0.4827,  0.0140,  ...,  0.3103,  0.4728,  0.2540],\n",
       "         ...,\n",
       "         [ 0.2301, -0.5883,  0.0694,  ...,  0.3032,  0.3562,  0.1801],\n",
       "         [ 0.5709, -0.1793,  0.3390,  ...,  0.6076,  0.7982,  0.4759],\n",
       "         [ 0.0333, -0.8503, -0.3262,  ..., -0.0097,  0.2433, -0.1648]]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_wgts[:3, :7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4573156-44a4-4e66-8971-2dc27230d9a7",
   "metadata": {},
   "source": [
    "Note that the \n",
    "- 0th axis is for the bs\n",
    "- 1st axis is actual attention wgts\n",
    "- 2nd axis is the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b4368-7881-4d79-8ee3-323bc1963ad6",
   "metadata": {},
   "source": [
    "At this point what we have is the following:  \n",
    "- In all the **128 documents** we have **50 (compressed from 1406) numbers** for each of the **6594 labels**.\n",
    "- These 50 numbers are presumably the amount of *attention* we need to pay to   \n",
    "those **50 tokens** each of which have a **49 (compressed) length** hidden represenation vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfee5b-1966-431b-b479-b32a269954cf",
   "metadata": {},
   "source": [
    "Taking linear combination of the memory (`out`) based on the `attn_wgts` to get `ctx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcf55d-0114-4734-92ce-ae28e87001de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 50, 49]), torch.Size([128, 50, 6594]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, attn_wgts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90094f-043c-4a4b-8eab-4d57f80dfd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 50, 1, 49]), torch.Size([128, 50, 6594, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out[None,...].shape, attn_wgts[...,None].shape\n",
    "out[:, :, None].shape, attn_wgts[..., None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8392b1-2630-4108-ba75-0cb79a0db5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 50, 6594, 49])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "ctx = (out[:, :, None] * attn_wgts[..., None])\n",
    "ctx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087bb9e-f541-4869-8daa-8f1ced1f62f4",
   "metadata": {},
   "source": [
    "At this point the meaning of `ctx` is:  \n",
    "For each of the **128 documents**, for each of the **50 (compressed from 1406)** tokens, for each of the **6594 labels** we have a **hidden compressed representation of length 49**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1245f1-2b8d-4a74-8fc6-245389ee88d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.7465e-03,  6.1149e-02, -2.2250e-01,  ..., -1.0547e-02, -8.3051e-02, -7.1283e-02],\n",
       "          [-1.4087e-03, -4.9323e-02,  1.7947e-01,  ...,  8.5075e-03,  6.6989e-02,  5.7497e-02],\n",
       "          [ 8.5931e-04,  3.0086e-02, -1.0947e-01,  ..., -5.1895e-03, -4.0863e-02, -3.5073e-02],\n",
       "          ...,\n",
       "          [ 1.6427e-03,  5.7516e-02, -2.0928e-01,  ..., -9.9206e-03, -7.8116e-02, -6.7048e-02],\n",
       "          [ 2.3742e-03,  8.3127e-02, -3.0247e-01,  ..., -1.4338e-02, -1.1290e-01, -9.6903e-02],\n",
       "          [ 1.3736e-03,  4.8094e-02, -1.7500e-01,  ..., -8.2956e-03, -6.5320e-02, -5.6065e-02]],\n",
       "\n",
       "         [[ 1.9480e-01, -1.0085e-01, -6.0489e-02,  ..., -1.6415e-02,  7.1366e-02, -7.5854e-02],\n",
       "          [-1.9623e-01,  1.0160e-01,  6.0934e-02,  ...,  1.6536e-02, -7.1891e-02,  7.6413e-02],\n",
       "          [ 7.0810e-02, -3.6661e-02, -2.1989e-02,  ..., -5.9670e-03,  2.5942e-02, -2.7574e-02],\n",
       "          ...,\n",
       "          [ 1.7948e-01, -9.2922e-02, -5.5732e-02,  ..., -1.5124e-02,  6.5753e-02, -6.9889e-02],\n",
       "          [ 2.3434e-01, -1.2133e-01, -7.2768e-02,  ..., -1.9747e-02,  8.5853e-02, -9.1252e-02],\n",
       "          [ 1.3789e-01, -7.1392e-02, -4.2820e-02,  ..., -1.1620e-02,  5.0519e-02, -5.3696e-02]],\n",
       "\n",
       "         [[ 2.0679e-01, -1.9689e-01,  7.2514e-02,  ..., -7.1185e-03,  4.9860e-02, -1.0741e-01],\n",
       "          [-2.1099e-01,  2.0088e-01, -7.3985e-02,  ...,  7.2628e-03, -5.0871e-02,  1.0959e-01],\n",
       "          [ 6.4356e-02, -6.1273e-02,  2.2567e-02,  ..., -2.2153e-03,  1.5517e-02, -3.3427e-02],\n",
       "          ...,\n",
       "          [ 1.8236e-01, -1.7363e-01,  6.3948e-02,  ..., -6.2775e-03,  4.3970e-02, -9.4721e-02],\n",
       "          [ 2.7656e-01, -2.6331e-01,  9.6979e-02,  ..., -9.5201e-03,  6.6682e-02, -1.4365e-01],\n",
       "          [ 1.1330e-01, -1.0787e-01,  3.9731e-02,  ..., -3.9002e-03,  2.7319e-02, -5.8850e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.4640e-01,  1.5364e-01, -8.2277e-02,  ...,  5.0683e-02,  5.1475e-02,  2.4160e-02],\n",
       "          [-2.2529e-01, -9.9923e-02,  5.3511e-02,  ..., -3.2963e-02, -3.3478e-02, -1.5713e-02],\n",
       "          [ 1.4057e-01,  6.2346e-02, -3.3388e-02,  ...,  2.0567e-02,  2.0889e-02,  9.8043e-03],\n",
       "          ...,\n",
       "          [ 3.0877e-01,  1.3695e-01, -7.3340e-02,  ...,  4.5178e-02,  4.5884e-02,  2.1536e-02],\n",
       "          [ 4.4071e-01,  1.9547e-01, -1.0468e-01,  ...,  6.4483e-02,  6.5490e-02,  3.0738e-02],\n",
       "          [ 2.2204e-01,  9.8483e-02, -5.2740e-02,  ...,  3.2488e-02,  3.2996e-02,  1.5487e-02]],\n",
       "\n",
       "         [[ 2.8909e-02,  2.0587e-03,  6.7105e-04,  ..., -7.5653e-03,  5.1221e-03, -2.0617e-02],\n",
       "          [-5.8716e-01, -4.1812e-02, -1.3629e-02,  ...,  1.5366e-01, -1.0403e-01,  4.1874e-01],\n",
       "          [-2.0164e-01, -1.4359e-02, -4.6806e-03,  ...,  5.2768e-02, -3.5727e-02,  1.4380e-01],\n",
       "          ...,\n",
       "          [ 1.0019e-02,  7.1343e-04,  2.3255e-04,  ..., -2.6218e-03,  1.7751e-03, -7.1448e-03],\n",
       "          [ 1.7445e-01,  1.2423e-02,  4.0495e-03,  ..., -4.5653e-02,  3.0909e-02, -1.2441e-01],\n",
       "          [-9.6473e-02, -6.8699e-03, -2.2394e-03,  ...,  2.5246e-02, -1.7093e-02,  6.8800e-02]],\n",
       "\n",
       "         [[-3.7299e-03,  1.8332e-01, -4.7049e-02,  ..., -3.2168e-02, -9.8785e-02,  1.9971e-02],\n",
       "          [ 7.5544e-03, -3.7129e-01,  9.5291e-02,  ...,  6.5153e-02,  2.0008e-01, -4.0450e-02],\n",
       "          [ 6.5156e-04, -3.2024e-02,  8.2189e-03,  ...,  5.6194e-03,  1.7257e-02, -3.4888e-03],\n",
       "          ...,\n",
       "          [-2.7972e-03,  1.3748e-01, -3.5284e-02,  ..., -2.4124e-02, -7.4083e-02,  1.4977e-02],\n",
       "          [-6.5683e-03,  3.2283e-01, -8.2853e-02,  ..., -5.6648e-02, -1.7396e-01,  3.5170e-02],\n",
       "          [-1.7621e-03,  8.6607e-02, -2.2227e-02,  ..., -1.5197e-02, -4.6669e-02,  9.4351e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.7754e-02,  3.0815e-02,  2.6581e-02,  ...,  1.4434e-03, -3.0631e-04, -4.1400e-02],\n",
       "          [ 1.6667e-01, -1.8505e-01, -1.5963e-01,  ..., -8.6683e-03,  1.8395e-03,  2.4862e-01],\n",
       "          [ 4.0829e-02, -4.5331e-02, -3.9103e-02,  ..., -2.1234e-03,  4.5060e-04,  6.0902e-02],\n",
       "          ...,\n",
       "          [-2.1310e-02,  2.3660e-02,  2.0409e-02,  ...,  1.1083e-03, -2.3519e-04, -3.1788e-02],\n",
       "          [-6.5336e-02,  7.2541e-02,  6.2574e-02,  ...,  3.3980e-03, -7.2108e-04, -9.7459e-02],\n",
       "          [ 4.5340e-03, -5.0340e-03, -4.3423e-03,  ..., -2.3580e-04,  5.0040e-05,  6.7632e-03]],\n",
       "\n",
       "         [[-2.2598e-02,  5.4780e-03,  4.9353e-02,  ..., -4.6649e-02,  7.7002e-04,  1.2219e-01],\n",
       "          [ 4.2078e-02, -1.0200e-02, -9.1897e-02,  ...,  8.6863e-02, -1.4338e-03, -2.2752e-01],\n",
       "          [-1.6696e-03,  4.0473e-04,  3.6463e-03,  ..., -3.4466e-03,  5.6891e-05,  9.0275e-03],\n",
       "          ...,\n",
       "          [-1.9603e-02,  4.7521e-03,  4.2813e-02,  ..., -4.0468e-02,  6.6798e-04,  1.0600e-01],\n",
       "          [-3.6692e-02,  8.8946e-03,  8.0134e-02,  ..., -7.5744e-02,  1.2503e-03,  1.9840e-01],\n",
       "          [-1.5228e-02,  3.6915e-03,  3.3258e-02,  ..., -3.1436e-02,  5.1890e-04,  8.2339e-02]],\n",
       "\n",
       "         [[-1.1614e-02, -1.4625e-02,  2.2434e-02,  ..., -2.1929e-02, -3.3679e-02, -4.8604e-02],\n",
       "          [ 5.1314e-02,  6.4619e-02, -9.9125e-02,  ...,  9.6894e-02,  1.4881e-01,  2.1475e-01],\n",
       "          [ 7.3930e-03,  9.3100e-03, -1.4281e-02,  ...,  1.3960e-02,  2.1440e-02,  3.0941e-02],\n",
       "          ...,\n",
       "          [-2.1101e-02, -2.6572e-02,  4.0761e-02,  ..., -3.9844e-02, -6.1192e-02, -8.8309e-02],\n",
       "          [-3.2814e-02, -4.1323e-02,  6.3389e-02,  ..., -6.1962e-02, -9.5161e-02, -1.3733e-01],\n",
       "          [-4.5096e-03, -5.6789e-03,  8.7113e-03,  ..., -8.5152e-03, -1.3078e-02, -1.8873e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.7286e-02,  5.7431e-02,  2.1055e-01,  ...,  1.1118e-01,  2.3440e-01,  1.0904e-01],\n",
       "          [-1.9484e-02, -1.4478e-02, -5.3079e-02,  ..., -2.8028e-02, -5.9092e-02, -2.7488e-02],\n",
       "          [ 4.3999e-02,  3.2696e-02,  1.1986e-01,  ...,  6.3294e-02,  1.3344e-01,  6.2075e-02],\n",
       "          ...,\n",
       "          [ 7.8666e-02,  5.8456e-02,  2.1430e-01,  ...,  1.1316e-01,  2.3858e-01,  1.1098e-01],\n",
       "          [ 9.4541e-02,  7.0253e-02,  2.5755e-01,  ...,  1.3600e-01,  2.8673e-01,  1.3338e-01],\n",
       "          [ 6.9557e-02,  5.1688e-02,  1.8949e-01,  ...,  1.0006e-01,  2.1096e-01,  9.8133e-02]],\n",
       "\n",
       "         [[-5.9713e-02, -4.3760e-03,  1.2829e-01,  ...,  9.0876e-02, -7.9220e-02, -1.9887e-02],\n",
       "          [ 6.3125e-02,  4.6261e-03, -1.3562e-01,  ..., -9.6068e-02,  8.3747e-02,  2.1024e-02],\n",
       "          [-2.7866e-02, -2.0421e-03,  5.9870e-02,  ...,  4.2409e-02, -3.6969e-02, -9.2808e-03],\n",
       "          ...,\n",
       "          [-6.0179e-02, -4.4102e-03,  1.2929e-01,  ...,  9.1586e-02, -7.9839e-02, -2.0043e-02],\n",
       "          [-7.3566e-02, -5.3912e-03,  1.5806e-01,  ...,  1.1196e-01, -9.7599e-02, -2.4501e-02],\n",
       "          [-4.9341e-02, -3.6160e-03,  1.0601e-01,  ...,  7.5092e-02, -6.5461e-02, -1.6433e-02]],\n",
       "\n",
       "         [[ 1.2593e-02, -6.9963e-04, -1.8169e-02,  ..., -3.0383e-03,  8.6847e-03, -1.5792e-02],\n",
       "          [-1.9843e-01,  1.1024e-02,  2.8629e-01,  ...,  4.7876e-02, -1.3685e-01,  2.4883e-01],\n",
       "          [-6.8205e-02,  3.7892e-03,  9.8401e-02,  ...,  1.6456e-02, -4.7036e-02,  8.5528e-02],\n",
       "          ...,\n",
       "          [-5.6383e-03,  3.1324e-04,  8.1345e-03,  ...,  1.3603e-03, -3.8884e-03,  7.0704e-03],\n",
       "          [ 6.9322e-02, -3.8513e-03, -1.0001e-01,  ..., -1.6725e-02,  4.7807e-02, -8.6929e-02],\n",
       "          [-2.0307e-02,  1.1282e-03,  2.9297e-02,  ...,  4.8994e-03, -1.4004e-02,  2.5465e-02]]]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx[:2, :7, :, :9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d228ea-60ca-428c-88f8-74d09cb3fe96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 49])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "ctx = ctx.sum(1)\n",
    "ctx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84f911-b5f0-435b-b083-bfaeee799d25",
   "metadata": {},
   "source": [
    "Now, let's decompress the hidden representation length from 49 back to 400 by doing a non-linearity followed by matrix multiply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db654c-d058-4b24-9cb5-1e040099aebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "lin_for_rep_decompress = nn.Linear(label_emb_sz, emb_sz)\n",
    "ctx = lin_for_rep_decompress(torch.relu(ctx))\n",
    "ctx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd682da-f0e2-4c48-9cb0-1fb942c5aff7",
   "metadata": {},
   "source": [
    "Let's recap what we have so far `ctx`:  \n",
    "For each of the **128 documents** for each of the **6594 labels** we have a **represenation vector of lenth 400** (we obtained this by paying attention to specific parts of the memory).\n",
    "\n",
    "Now this `ctx` will get concatenated with whatever the decoder was previously using without attention (i.e., the output of `masked_concat_pool`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20a04e-676e-47ac-be70-6dfb46e282a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05321591-f331-48c9-9e91-e40876c2a541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1200])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_concat_pool = torch.randn((128,1200))\n",
    "out_concat_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb73a0-444e-4654-bed6-2aa192068d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 1200])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "out_concat_pool = out_concat_pool[:, None]\n",
    "out_concat_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a996a4-638c-4566-8d1b-8c9544becc45",
   "metadata": {},
   "source": [
    "Think of `out_concat_pool` as the label-agnostic **1200 length represenation** of each of the **128 documents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c2af5-7d71-4ca4-a797-263c43dc8f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 1200])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "out_concat_pool = out_concat_pool.repeat(1, n_labels, 1)\n",
    "out_concat_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f06b72-a2ce-4156-b4b9-c9fcde717eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ctx = ctx.permute(1,0,2).contiguous()\n",
    "ctx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9623a84-aa4f-4bef-a977-4c441b5e2871",
   "metadata": {},
   "source": [
    "Think of `ctx` as label-specific **400 length representation** of each of the **128 documents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd00e84-238b-45a1-983e-05a00d43c30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 1600])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "out_concat_pool = torch.cat((out_concat_pool, ctx), dim=-1)\n",
    "out_concat_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc1e6d-e011-4f90-bec4-c7ab5025d454",
   "metadata": {},
   "source": [
    "At this point we can flattan the features before feeding it into the Linear-BatchNorm-Droput layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf36463-6335-4e7d-9f3b-a21e67dbc63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 1600])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(128, 6594, 1600)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5fb32-ce8c-43a8-99c2-54f27fe17114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1350451200])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca5e78-4459-4b62-87b3-126517b5a93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10550400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(t.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b83d7-cc0d-4d14-a837-cc72b14054e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='yellow';this.parentNode.removeChild(this)\" style=\"display:none\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10550400])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_bg('yellow')\n",
    "out_concat_pool = out_concat_pool.view(out_concat_pool.shape[0], -1)\n",
    "out_concat_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e485f8d-c28c-4a2e-a13b-bb3a7d92e1c1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e27cde-85c2-4524-a838-ba2a5809f53d",
   "metadata": {},
   "source": [
    "Let's see how to perform batchnorm on this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc0b25-a56c-4fba-bc2a-9ec613f7b257",
   "metadata": {},
   "source": [
    "First let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40059eb9-175b-4eb2-963e-daa9ee31e77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.BatchNorm1d(100)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431b2f9-9e79-4800-8484-ab9cd7a57f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c02f0-1190-4fb6-9f37-2ff53b8fe2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2eab2-b223-4ebc-accf-1b0a0091063f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.BatchNorm1d(18)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a40f14-7ddd-46c7-865e-103272aebbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(20, 18, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e0920-fd33-4672-9771-464eaf2a216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 18, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977caac5-29ba-41f9-8ce5-a6319ecbc8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(6594, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.BatchNorm1d(n_out)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2341706-d497-4dfa-900c-da5aaf019375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6594, 1600])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(out_concat_pool).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b125f-d036-46e8-bc93-61d35893ffe6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72c912-8967-4633-b2c7-85a259e7208d",
   "metadata": {},
   "source": [
    "##### Practice some ufuncs in PyTorch and Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68506f-6a40-4589-a4d1-fbe156fbba38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90deaafc-7d4d-4e5f-b288-d1f619eaacb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1,2], [3,4]])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a286d-111c-44d1-a5a4-3b9c29dd2de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 3, 3, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(arr, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b04ad-993c-4ab7-b4e7-6f5d0fd95713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 2, 2, 2],\n",
       "       [3, 3, 3, 4, 4, 4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(arr, 3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2299454-2b0f-4dea-9703-cd6164e425e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(arr, [1, 2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe813a-a31b-45cd-b726-69364d4b88aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), 1, (3,))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0, 1, 2])\n",
    "a, a.ndim, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de2543f-eb7e-4745-9909-d298b15152fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416a2a1-36e2-43f3-9657-62e302870258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 0, 1, 2, 0, 1, 2],\n",
       "       [0, 1, 2, 0, 1, 2, 0, 1, 2]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(a, (2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3815c6-1023-45ba-8c99-652ffca70cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 2, 0, 1, 2]],\n",
       "\n",
       "       [[0, 1, 2, 0, 1, 2]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(a, (2,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a77dd-85b9-4a66-aaf8-d7f11e1862c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2],\n",
       "        [3, 4]]),\n",
       " (2, 2),\n",
       " 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([[1,2], [3, 4]])\n",
    "b, b.shape, b.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8afa99-7d82-4365-8a0e-ba9cd22e5914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 2],\n",
       "       [3, 4, 3, 4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(b,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df80114-cbdf-41a2-b9de-b6ce912949bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(b, (2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b2932-d89b-4162-80d1-d06144726e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 2, 1, 2],\n",
       "       [3, 4, 3, 4, 3, 4],\n",
       "       [1, 2, 1, 2, 1, 2],\n",
       "       [3, 4, 3, 4, 3, 4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(b, (2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c949d42-5532-4ff7-a65c-0df0cb8c0cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([1, 2, 3, 4])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a8eac-dfaa-46c1-9d39-45d5f8069706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],\n",
       "       [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],\n",
       "       [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],\n",
       "       [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(c, (4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77466149-4581-4935-bb7e-8f94cbcaf05c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c49c9c-a1f4-410d-bbec-0f99fc866154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 4, 0, 3, 1],\n",
       "         [3, 1, 3, 0, 3, 2],\n",
       "         [3, 2, 1, 0, 1, 0]]),\n",
       " torch.Size([3, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randint(0, 5, (3,6))\n",
    "t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9f19e-1039-4be8-b8df-9ddb8fac7fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 1, 4, 0, 3, 1]],\n",
       " \n",
       "         [[3, 1, 3, 0, 3, 2]],\n",
       " \n",
       "         [[3, 2, 1, 0, 1, 0]]]),\n",
       " torch.Size([3, 1, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t[:, None]\n",
    "t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ccecb7-3520-4f63-90ba-fbf4fbe92f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 4, 0, 3, 1],\n",
       "         [1, 1, 4, 0, 3, 1],\n",
       "         [1, 1, 4, 0, 3, 1],\n",
       "         [1, 1, 4, 0, 3, 1],\n",
       "         [1, 1, 4, 0, 3, 1]],\n",
       "\n",
       "        [[3, 1, 3, 0, 3, 2],\n",
       "         [3, 1, 3, 0, 3, 2],\n",
       "         [3, 1, 3, 0, 3, 2],\n",
       "         [3, 1, 3, 0, 3, 2],\n",
       "         [3, 1, 3, 0, 3, 2]],\n",
       "\n",
       "        [[3, 2, 1, 0, 1, 0],\n",
       "         [3, 2, 1, 0, 1, 0],\n",
       "         [3, 2, 1, 0, 1, 0],\n",
       "         [3, 2, 1, 0, 1, 0],\n",
       "         [3, 2, 1, 0, 1, 0]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.repeat(1, 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292d21a-04c1-49b5-b990-03245c448d13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22367468-375d-4187-8ea6-0f7c506dd456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([[1], [2], [3]])\n",
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d7d31-b1de-4a01-876e-4077028d8d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [2, 2, 2, 2],\n",
       "        [3, 3, 3, 3]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.expand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152da6b-d733-48a6-88cd-cb3f80354938",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (5) must match the existing size (3) at non-singleton dimension 0.  Target sizes: [5, 3].  Tensor sizes: [3, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4561/3650102684.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (5) must match the existing size (3) at non-singleton dimension 0.  Target sizes: [5, 3].  Tensor sizes: [3, 1]"
     ]
    }
   ],
   "source": [
    "b.expand(5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff63a55-1994-457a-b6d9-7daecea2dcf3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959236d5-4ec7-4d56-97c0-8123c1858da0",
   "metadata": {},
   "source": [
    "### `Learner` Creation: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a2e71-7875-4c9d-aad6-94424b045b3f",
   "metadata": {},
   "source": [
    "In this section we will replicate all the steps in `text_classifier_learner`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0dc836-6704-4b5e-8dbe-776240667074",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.01, metrics=PrecisionK).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b667a6-4fb0-4920-838d-fa8d98663826",
   "metadata": {},
   "source": [
    "There are three things that happens inside `text_classifier_learner`\n",
    "- create the `model` - we have already done this in the previous section\n",
    "- create a `Learner` using `TextLearner`\n",
    "    * `TextLearner` is a Basic class for a `Learner` in NLP.\n",
    "    * Adds a `ModelResetter` and an `RNNRegularizer` with `alpha` and `beta` to the callbacks, the rest is same as `Learner` init.\n",
    "    * Adding functionality to the base class, it has the methods - `load_pretrained`, `save_encoder` and `load_encoder`.\n",
    "- load the pretrained weights (grabbing that information from `_model_meta`) into the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4bdfd-44e2-40bd-9797-91ea687aece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = _model_meta[arch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08175f09-7fe8-4b21-bf93-dad7557b8f00",
   "metadata": {},
   "source": [
    "#### Metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4513f-c0a5-47dc-967b-b2d0175f018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(yhat_raw, y, k=15):\n",
    "    \"\"\"\n",
    "        Inputs: \n",
    "            yhat_raw: activation matrix of ndarray and shape (n_samples, n_labels)\n",
    "            y: binary ground truth matrix of type ndarray and shape (n_samples, n_labels)\n",
    "            k: for @k metric\n",
    "    \"\"\"\n",
    "    yhat_raw, y = to_np(yhat_raw), to_np(y)\n",
    "    # num true labels in the top k predictions / k\n",
    "    sortd = yhat_raw.argsort()[:,::-1]\n",
    "    topk = sortd[:, :k]\n",
    "    \n",
    "    # get precision at k for each sample\n",
    "    vals = []\n",
    "    for i, tk in enumerate(topk):\n",
    "        num_true_in_top_k = y[i,tk].sum()\n",
    "        vals.append(num_true_in_top_k / float(k))\n",
    "    \n",
    "    return np.mean(vals)\n",
    "\n",
    "def precision_at_r(yhat_raw, y):\n",
    "    \"\"\"\n",
    "        Inputs: \n",
    "            yhat_raw: activation matrix of ndarray and shape (n_samples, n_labels)\n",
    "            y: binary ground truth matrix of type ndarray and shape (n_samples, n_labels)\n",
    "    \"\"\"\n",
    "    yhat_raw, y = to_np(yhat_raw), to_np(y)\n",
    "    # num true labels in the top r predictions / r, where r = number of labels associated with that sample \n",
    "    sortd = yhat_raw.argsort()[:, ::-1]\n",
    "    \n",
    "    # get precision at r for each sample\n",
    "    vals = []\n",
    "    for i, sorted_activation_indices in enumerate(sortd):\n",
    "        # compute the number of labels associated with this sample\n",
    "        r = int(y[i].sum())\n",
    "        top_r_indices = sorted_activation_indices[:r] \n",
    "        num_true_in_top_r = y[i, top_r_indices].sum()\n",
    "        vals.append(num_true_in_top_r / float(r))\n",
    "    \n",
    "    return np.mean(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd13c3-90a8-44df-b443-488d58ae4ccc",
   "metadata": {},
   "source": [
    "#### `TextLearner`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27ef81-b8ea-4ec8-8983-ecf1a2f523af",
   "metadata": {},
   "source": [
    "Let's create the `Learner` (we can pass our metrics here:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7550c-daf7-4ec6-90d3-d8721d6cce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = TextLearner(dls_clas, model, splitter=meta['split_clas'], metrics=[precision_at_k, precision_at_r])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13b51a-d279-44cc-81f4-7ff8445d01d6",
   "metadata": {},
   "source": [
    "Note that in this case fastai was clever to figure out the correct loss (`BCEWithLogitsLoss`) from the `Dataloaders dls_clas`! We could overide this cleverness by specifically passing a `loss_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864cd17-5295-4562-af02-781b06b6935f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlattenedLoss of BCEWithLogitsLoss()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986c468-5cba-4a70-89f8-c822e095bb1a",
   "metadata": {},
   "source": [
    "#### Pretrained Weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d0fd7-b01e-4c59-98df-754cfd14f4cb",
   "metadata": {},
   "source": [
    "Now let's replace the random weights of the encoder using the pretrained weights (from the url in meta). Note that in this case we can skip this step, because we are anyway later on going to load the encoder weights from our finetuned language model. But let's not skip this step for completeness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf5848d-8cb4-4585-b0b9-9f94f5187e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backwards = False\n",
    "url = 'url_bwd' if backwards else 'url'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287221d-55c0-40bf-9c2e-e3ad282879d2",
   "metadata": {},
   "source": [
    "Let's check if we have pretrained weights for that `arch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf38318-d15a-4477-b3e6-818c53046e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if url not in meta: warn(\"There are no pretrained weights for that architecture yet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5747e764-a3a8-4ef6-adc1-f62086d0b3b7",
   "metadata": {},
   "source": [
    "Get the path which contains the pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305a4c1-cac1-4eb3-9ee0-0f52aeadf152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('/home/ubuntu/.fastai/models/wt103-fwd/lstm_fwd.pth'),Path('/home/ubuntu/.fastai/models/wt103-fwd/itos_wt103.pkl')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_path = untar_data(meta[url], c_key='model')\n",
    "pretrained_model_path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c797431-ccf3-47ac-9e0f-4b09f915865c",
   "metadata": {},
   "source": [
    "Let's get the weights of the pretrained model and the vocab that was used to train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1b3d0-de1c-47a2-88fb-c75edde22b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('/home/ubuntu/.fastai/models/wt103-fwd/lstm_fwd.pth'),\n",
       " Path('/home/ubuntu/.fastai/models/wt103-fwd/itos_wt103.pkl'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = [list(pretrained_model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "wgts_fname, vocab_fname = fnames\n",
    "wgts_fname, vocab_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5336d94-50b9-4757-bfc2-0e6dc4a9b484",
   "metadata": {},
   "source": [
    "Just for fun why don't we take a look at the pretrained weights and the vocab that got used to train that model:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758125d3-814d-4cc0-ab49-c00067963e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(collections.OrderedDict, list)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_wgts, old_vocab = torch.load(wgts_fname), load_pickle(vocab_fname)\n",
    "type(pretrained_wgts), type(old_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5614e-24ed-4e3e-b8b5-8299c4b97d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.encoder.weight  0.encoder_dp.emb.weight  0.rnns.0.weight_hh_l0_raw  0.rnns.0.module.weight_ih_l0  0.rnns.0.module.weight_hh_l0  0.rnns.0.module.bias_ih_l0  0.rnns.0.module.bias_hh_l0  0.rnns.1.weight_hh_l0_raw  0.rnns.1.module.weight_ih_l0  0.rnns.1.module.weight_hh_l0  0.rnns.1.module.bias_ih_l0  0.rnns.1.module.bias_hh_l0  0.rnns.2.weight_hh_l0_raw  0.rnns.2.module.weight_ih_l0  0.rnns.2.module.weight_hh_l0  0.rnns.2.module.bias_ih_l0  0.rnns.2.module.bias_hh_l0  1.decoder.weight  1.decoder.bias  "
     ]
    }
   ],
   "source": [
    "for key in pretrained_wgts: print(key, end='  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eca295-b858-4f51-aeb8-1e378a0f0970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#60000) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxmaj','xxup','xxrep','xxwrep','the',',','.','of','and','in','to','a','=','\"','was','on','-',\"'s\",'as','for','that','with','by','\\n ',')','(','\\n \\n ','is','his','at','he','it','from','were','an','had','which','be','this','but',\"'\",'are','not','first','their'...]\n"
     ]
    }
   ],
   "source": [
    "print(coll_repr(old_vocab, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef7eea5-eb46-44f7-918c-0922a314f6ef",
   "metadata": {},
   "source": [
    "Now we write that magic line which will replace the random weights in our `encoder` i.e., `learn.model[0]`, with the `pretrained_wgts`.  \n",
    "\n",
    "**IMPORTANT**: We need to pass the\n",
    "- `wgts_fname`,\n",
    "- and the `vocab_fname`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2389dd-f511-4986-9da3-20ce93c4bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a2c02-8fe0-4250-a4b8-2d6bd0751967",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_pretrained(wgts_fname, vocab_fname, model=learn.model[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e70b005-00a0-4d02-9d7b-167467e0657f",
   "metadata": {},
   "source": [
    "Additionally, we also need to freeze the `encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5889db8-8fc5-420d-97c1-65b14e355d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743145c-7d73-4665-9c05-b6f8e5efd61e",
   "metadata": {},
   "source": [
    "Also, to do mixed precision training, we need the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a79d8-493d-4bdc-a858-d416bddf5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa1734e-9c8a-43d4-bca1-2b64c9ea3896",
   "metadata": {},
   "source": [
    "#### LM weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897fca5-fb38-4124-b0fa-28909503091b",
   "metadata": {},
   "source": [
    "Additionally, we also need to freeze the `encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1550a3f-743f-459b-a676-45301d5526bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ceee0-8ab9-4fbb-8b32-af5f412d16e4",
   "metadata": {},
   "source": [
    "Also, to do mixed precision training, we need the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ef72f-2868-4cf8-b491-6ed35dcde758",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f3364-a146-4539-a1d8-205058abbe29",
   "metadata": {},
   "source": [
    "**IMPORTANT:**  \n",
    "This is where the \"Magic\" happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4e70b-a8e3-42d1-9260-ef6de0ea1ed5",
   "metadata": {},
   "source": [
    "The final step prior to training the classifier is to load the encoder from our fine-tuned language model. We will use `load_encoder` instead of `load` because we have only the pretrained weights available for the encoder; `load` by default raises an exception if an incomplete model is loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1cebd-3029-4dd9-8b5e-a4c79fc855de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lht {path_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5fbcb-13bb-49ff-8599-2392467c17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder(path_model/'lm_finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee01204-6e67-4d7e-8943-5ffcc37d7501",
   "metadata": {},
   "source": [
    "## `Learner` for Multi-Label Classifier Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d7375-cd24-47bf-bee8-b2838a7f5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = torch.load(path_model/'caml_dls_clas_full_16.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e7c3c-e026-4ea2-947d-99417569b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.1, max_len=72*40, metrics=PrecisionK).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c9ced-7036-4479-919f-dc2211a452db",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder(path_model/'caml_lm_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9cc0e4-1053-4ef4-bc4e-dc407bff07dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelAttentionClassifier(\n",
       "  (layers): LinBnDrop(\n",
       "    (0): BatchNorm1d(8922, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.04000000000000001, inplace=False)\n",
       "  )\n",
       "  (emb_label): Embedding(8922, 400)\n",
       "  (final_lin): Linear(in_features=400, out_features=8922, bias=True)\n",
       "  (final_lin2): Linear(in_features=400, out_features=8922, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c2740-07bf-4224-83a0-aa29f8f6d1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Same as `nn.Module`, but no need for subclasses to call `super().__init__`\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mLabelAttentionClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# self.layers = LinBnDrop(self.lbs, self.fts, p=ps, act=None) # wrong_deb\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# ps = 0.1 # deb\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinBnDrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mln\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# deb\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbptt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# deb\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_lin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_lin2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# x = masked_concat_pool(out, mask, self.bptt)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# ctx = out.new_zeros((bs, self.lbs, self.fts))\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# for out_split in torch.split(out, 1, dim=1):\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mattn_wgts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# deb\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mattn_wgts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_wgts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# deb\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_wgts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mout\u001b[0m \u001b[0;31m# deb\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# x = self.final_lin.weight.mul(x).sum(dim=2).add(self.final_lin.bias) #missed_deb\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_lin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_lin2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# x = x.view(x.shape[0], x.shape[1])\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/xcube/nbs/examples/mimic/xcube/text/models/core.py\n",
       "\u001b[0;31mType:\u001b[0m           PrePostInitMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder = (learn.model[1].__class__)\n",
    "decoder??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cc2b3-e8ff-4aae-bfd2-d29d1b7301b3",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53ac2f-0631-4b89-867e-96eed90b8adc",
   "metadata": {},
   "source": [
    "The last step (yes, the madness will end soon) is to train with:\n",
    "- *discriminative learning rates*: define here\n",
    "- *gradual unfreezing*: define here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f934f4-b1ba-4681-ad46-9320c724d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n",
    "lr_min, lr_steep, lr_valley, lr_slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d03f4-d4df-4d6f-ac38-e2b931e6f06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>PrecisionK</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.468901</td>\n",
       "      <td>20:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.007193739525973797.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr_max=0.131825673, moms=(0.8,0.7,0.8), wd=0.1, cbs=SaveModelCallback(fname=path_model/'caml_clas_full'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccbc19-c81e-480b-a350-bf51451a728f",
   "metadata": {},
   "source": [
    "This was the result after just three epochs. Now let's unfreeze the last two layers and do discriminative training:  \n",
    "\n",
    "Now we will pass -2 to `freeze_to` to freeze all except the last two parameter groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174876d-5895-460b-94d6-8138d0721644",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd157898-01bd-49ab-9afd-6d36ab5da8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [3/6 1:27:14<1:27:14]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>PrecisionK</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.005933</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>29:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.005940</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>0.467912</td>\n",
       "      <td>29:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.467675</td>\n",
       "      <td>29:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='29' class='' max='3084' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.94% [29/3084 00:16<29:27 0.0059]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.007206531707197428.\n",
      "Better model found at epoch 1 with valid_loss value: 0.007191396784037352.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_one_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSaveModelCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcaml_clas_full\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/callback/schedule.py:116\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    113\u001b[0m lr_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    114\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[38;5;241m/\u001b[39mdiv, lr_max, lr_max\u001b[38;5;241m/\u001b[39mdiv_final),\n\u001b[1;32m    115\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmom\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoms \u001b[38;5;28;01mif\u001b[39;00m moms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m moms))}\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:221\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:212\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:206\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:198\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:169\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:194\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    192\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/learner.py:172\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_one_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/xcube/nbs/examples/mimic/xcube/text/models/core.py:42\u001b[0m, in \u001b[0;36mSentenceEncoder.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, sl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbptt):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#Note: this expects that sequence really begins on a round multiple of bptt\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     real_bs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28minput\u001b[39m[:,i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_idx)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 42\u001b[0m     o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mreal_bs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbptt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m sl\u001b[38;5;241m-\u001b[39mi \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len:\n\u001b[1;32m     44\u001b[0m         outs\u001b[38;5;241m.\u001b[39mappend(o)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/text/models/awdlstm.py:105\u001b[0m, in \u001b[0;36mAWD_LSTM.forward\u001b[0;34m(self, inp, from_embeds)\u001b[0m\n\u001b[1;32m    103\u001b[0m new_hidden \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l, (rnn,hid_dp) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dps)):\n\u001b[0;32m--> 105\u001b[0m     output, new_h \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     new_hidden\u001b[38;5;241m.\u001b[39mappend(new_h)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m: output \u001b[38;5;241m=\u001b[39m hid_dp(output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/text/models/awdlstm.py:52\u001b[0m, in \u001b[0;36mWeightDropout.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# To avoid the warning that comes because the weights aren't flattened.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:679\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    682\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    683\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/fastai/torch_core.py:340\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _torch_handled(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opt, func): convert,types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m),(torch\u001b[38;5;241m.\u001b[39mTensor,)\n\u001b[0;32m--> 340\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert: res \u001b[38;5;241m=\u001b[39m convert(res)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, TensorBase): res\u001b[38;5;241m.\u001b[39mset_meta(\u001b[38;5;28mself\u001b[39m, as_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:1023\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> 1023\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert(ret, \u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(6, lr_max=lr_min, moms=(0.8,0.7,0.8), wd=0.1, cbs=SaveModelCallback(fname=path_model/'caml_clas_full'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4661d6-857c-45ff-a794-c20e922b4429",
   "metadata": {},
   "source": [
    "Now we will unfreeze a bit more, recompute learning rate and continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8eb628-85bf-4e0e-aea3-fa878d5b7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f7b03-b436-47ac-87d5-8b776c3637f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>PrecisionK</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>0.007010</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>39:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>0.502254</td>\n",
       "      <td>40:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.512021</td>\n",
       "      <td>40:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.525287</td>\n",
       "      <td>40:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.530921</td>\n",
       "      <td>40:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.007010351400822401.\n",
      "Better model found at epoch 1 with valid_loss value: 0.006781804841011763.\n",
      "Better model found at epoch 2 with valid_loss value: 0.006633494049310684.\n",
      "Better model found at epoch 3 with valid_loss value: 0.006457117851823568.\n",
      "Better model found at epoch 4 with valid_loss value: 0.006400250364094973.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, lr_max=1e-3, moms=(0.8,0.7,0.8), wd=0.2, cbs=SaveModelCallback(fname=path_model/'caml_clas_full'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbadf33-52e3-4e2d-bd93-d3bfd4208663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [0.006400250364094973,0.5309213127718464]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = learn.load(path_model/'caml_clas_full')\n",
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399b07fe-fd97-44d0-aacf-9c3ad8ae9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(5, lr_max=1e-3, moms=(0.8,0.7,0.8), wd=0.2, cbs=SaveModelCallback(fname=path_model/'caml_clas_full'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5101bd9-d2ca-40df-840d-2052ab42e52c",
   "metadata": {},
   "source": [
    "Finally, we will unfreeze the whole model, recompute learning rate and perform training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a9044-361b-4e24-a1f0-e6896a3e1438",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d137b-0386-4352-ad0b-419a5cba9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, lr_max=lr_valley, moms=(0.8,0.7,0.8), wd=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ad422-82c5-4996-84b8-e90d3af0ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbbb218-092c-41a9-9f43-9373d00fca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L(learn.recorder.values).itemgot(2));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
