{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Computation of gradient updates for the L2R models\n",
    "output-file: l2r.gradients.html\n",
    "title: L2R Gradients\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac293f3d-d29d-4e9b-bd1d-ced9606e5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastai # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d85e76f-6a44-4ece-8d36-d7c690ab399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp l2r.gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48864062-4d5a-4b6c-a9ae-cb703518daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.torch_imports import *\n",
    "from xcube.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a63a1598-330f-4aa8-bcbf-dbf72dbed711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6296c70-e60d-4f7b-9bc3-f904ad00576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c84c0-2b46-4650-adcf-c89173c37cb0",
   "metadata": {},
   "source": [
    "The following notations are borrowed from [From RankNet to LambdaRank to LambdaMART: An Overview](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf)\n",
    "\n",
    "\n",
    "Let $I$ denote the pair of indices $\\{i, j\\}$, for which we desire token_i to be ranked differently from token_j (for a given label group). Since we must include each pair just once, so it is convenient to consider pairs of indices $\\{i, j\\}$ for which token_i is more relevant than token_j.\n",
    "\n",
    "$$\\lambda_{ij} = \\sigma \\left\\{ \\frac{1}{2}(1 - S_{ij}) - \\frac{1}{1+e^{ \\sigma(p_i - p_j)}} \\right\\}  \\textsf{  Eq: 3},$$ where $\\sigma$ is a hyper-parameter which controls the shape of the sigmoid and $p_i, p_j$ are predictions made by the model for token_i and token_j respectively, and\n",
    "\n",
    "$$S_{ij} = \\begin{cases} \n",
    "                1, & \\text{if token_i is more relevant} \\\\ \n",
    "                0, & \\text{if token_i is as relevant as token_j} \\\\\n",
    "                -1, & \\text{if token_j is ore relevant} \n",
    "            \\end{cases}$$\n",
    "\n",
    "The weight update rule in gradient descent is given by:\n",
    "$$\\delta w_k = \\eta \\sum_{\\{i,j\\} \\in I} (\\lambda_{ij} \\frac{\\partial p_i}{\\partial w_k} - \\lambda_{ij} \\frac{\\partial p_j}{\\partial w_k}) = -\\eta \\sum_i \\lambda_i \\frac{\\partial p_i}{\\partial w_k},$$ where\n",
    "\n",
    "$$\\lambda_i = \\sum_{j: \\{i,j\\} \\in I} \\lambda_{ij} - \\sum_{j: \\{j,i\\} \\in I} \\lambda_{ji} \\textsf{  Eq: 4}.$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c7ceca-bcb0-46e9-9e05-f49b486887dd",
   "metadata": {},
   "source": [
    "**Implementing the above equations:**\n",
    "\n",
    "(Handcrfted Gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada7194-15e1-4af2-a63f-1cf274ebc9d0",
   "metadata": {},
   "source": [
    "We can think of the tensor returned by `_summation` as essentially the summation notation in eq:4 above. It has three dimension. The length of the zeroth dim is the number of tokens. And each token contains a 2d tensor. For each token the zeroth and the first dim of 2d tensor has the following interpretation.\n",
    "\n",
    "For each token in a sequence (i.e. the i's) it contains the information about the other tokens (i.e. the j's) that  \n",
    "1. The first column value tells us the row num we got to index in the pairs array.\n",
    "2. The last column value tells us whether i is more relevant or less relevant than j. In other words, it determines the sign while computing $\\lambda_i$ in eq: 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d440c42-3c7c-4a3c-86ff-1243318ef2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summation(sl, ij):\n",
    "    sumer = []\n",
    "    for i in range(sl):\n",
    "        _x = torch.nonzero(ij == i, as_tuple=False)\n",
    "        _x[:, -1] = torch.pow(-1, _x[:, 1])\n",
    "        sumer.append(_x)\n",
    "    return torch.stack(sumer, dim=0)\n",
    "\n",
    "def _idcg(xb, k=None, gain_fn=None):\n",
    "    # pdb.set_trace()\n",
    "    x = xb[:, :, :, -1]\n",
    "    ranks = x.argsort(dim=-1, descending=True).argsort(dim=-1) # ranking by the scores, highest score gets rank 0\n",
    "    dfs = 1/torch.log2(ranks + 2)\n",
    "    gains = torch.pow(2, x) if gain_fn == 'exp' else torch.pow(x, 3)\n",
    "    idg = gains * dfs\n",
    "    idcg = idg.sum(dim=-1)\n",
    "    \n",
    "    idcg_at_k = None\n",
    "    if k is not None:\n",
    "        topk, topk_idxs = torch.topk(x, k=k, dim=-1, largest=True)\n",
    "        # topk_relvs = torch.take_along_dim(x, topk_idxs, dim=-1)\n",
    "        dfs_at_k = 1/torch.log2(2 + torch.arange(k)).cuda()\n",
    "        gains_at_k = torch.pow(2, topk) if gain_fn == 'exp' else torch.pow(topk, 3)\n",
    "        idg_at_k = gains_at_k * dfs_at_k\n",
    "        idcg_at_k = idg_at_k.sum(-1)\n",
    "    \n",
    "    return idcg, idcg_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d8b6e02-4a55-4dd7-bc07-faf8ff9df441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_loss2(preds, xb, sigma=0.5, lambrank=False, gain_fn=None, k=6):\n",
    "    # In the following `ij` is essentially the set $I$\n",
    "    sl = xb.shape[2]\n",
    "    ij = torch.as_tensor(np.fromiter(itertools.combinations(np.arange(sl), 2), dtype=np.dtype((int,2))),\n",
    "                                device=xb.device)#.expand(xb.shape[0], xb.shape[1], -1, -1)\n",
    "    \n",
    "    # Sort the tokens by the model prediction scores so that we can compute the set $I$ defined above:\n",
    "    srtd_preds, srtd_idxs = preds[:, :, :,  0].sort(descending=True)\n",
    "    \n",
    "    srtd_ranks = srtd_preds.new_empty(srtd_preds.size())#srtd_idxs.argsort()\n",
    "    srtd_ranks[:,:] = torch.arange(preds.shape[2])\n",
    "    ri_rj = srtd_ranks[:, :, ij] # these are the ranks for token_i and token_j\n",
    "    dfi_dfj = 1.0 / torch.log2(ri_rj + 2)\n",
    "    dfi = dfi_dfj[:,:,:,0]\n",
    "    dfj = dfi_dfj[:,:,:,1]\n",
    "        \n",
    "    srtd_relvs = torch.take_along_dim(xb[:, :, :, -1], srtd_idxs, dim=-1)\n",
    "    pi_pj = srtd_preds[:, :, ij] # these are p_i and p_j \n",
    "    pi, pj = pi_pj[:, :, :, 0], pi_pj[:, :, :, 1]\n",
    "    exp_ij = torch.exp(sigma * (pi - pj))\n",
    "    si_sj = srtd_relvs[:, :, ij] # these are the relevance scores for token_i and token_j\n",
    "    si, sj= si_sj[:, :, :, 0], si_sj[:, :, :, 1]\n",
    "    gain_i, gain_j = ( torch.pow(2.0, si), torch.pow(2.0, sj) ) if gain_fn == 'exp' else ( torch.pow(si, 3.0), torch.pow(sj, 3.0) ) # cubic\n",
    "    signs = torch.sign(si - sj)\n",
    "    delta_dcg = torch.abs((gain_i - gain_j) * (dfi - dfj))\n",
    "    idcg, idcg_at_k = _idcg(xb, k=k, gain_fn=gain_fn)\n",
    "    delta_ndcg_at_k = delta_dcg / idcg_at_k.unsqueeze(-1)\n",
    "    \n",
    "    lambda_ij = sigma * (  0.5 * (1 - signs) -  1/(1 + exp_ij) )\n",
    "    if lambrank: lambda_ij *= delta_ndcg_at_k # use this for Lambda-Rank\n",
    "    \n",
    "    sumer = _summation(sl, ij)\n",
    "    idxr, signs = sumer[:, :, 0], sumer[:, :, -1]\n",
    "    # Now we can compute $\\lambda_i$ from eq: 4,\n",
    "    lambda_i = (lambda_ij[:, :, idxr] * signs).sum(dim=-1)\n",
    "    \n",
    "    return srtd_preds, lambda_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2912896b-4469-40cd-a72d-57f077f47714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_loss3(preds, xb, sigma=0.5, lambrank=False, gain_fn=None, k=6):\n",
    "    with torch.no_grad():\n",
    "        # pdb.set_trace()\n",
    "        x = xb[:, :, :, -1, None]\n",
    "        x_t = xb[:, :, :, -1, None].transpose(-1,-2)\n",
    "        preds_t = preds.transpose(-1,-2)\n",
    "        preds_rank = preds[:, :, :, 0].argsort(dim=-1, descending=True).argsort(dim=-1).unsqueeze(-1)\n",
    "        preds_rank_t = preds_rank.transpose(-1,-2)\n",
    "        \n",
    "        exp_ij= 1.0 + torch.exp(sigma* (preds - preds_t))\n",
    "        rel_diff = x - x_t\n",
    "        gain_diff = torch.pow(2.0, x) - torch.pow(2.0, x_t) if gain_fn == 'exp' else torch.pow(x, 3.0) - torch.pow(x_t, 3.0)\n",
    "        decay_diff = 1.0/torch.log2(preds_rank + 2.0) - 1.0/torch.log2(preds_rank_t  + 2.0)\n",
    "        idcg, idcg_at_k = _idcg(xb, k=k, gain_fn=gain_fn)\n",
    "        idcg_at_k = idcg_at_k[..., None, None]\n",
    "        # pdb.set_trace()\n",
    "        delta_ndcg_at_k = torch.abs(gain_diff * decay_diff * 1/idcg_at_k)\n",
    "        pos_pairs = (rel_diff > 0).float()\n",
    "        neg_pairs = (rel_diff < 0).float()\n",
    "        S_ij = pos_pairs - neg_pairs\n",
    "        lambda_update = sigma * (  0.5 * (1 - S_ij) -  1/exp_ij )\n",
    "        if lambrank: lambda_update *= delta_ndcg_at_k \n",
    "        lambda_update = lambda_update.sum(dim=-1, keepdim=True)\n",
    "        # free memory\n",
    "        del preds_t, preds_rank, preds_rank_t, exp_ij, rel_diff, gain_diff, decay_diff, idcg, idcg_at_k, delta_ndcg_at_k, pos_pairs, neg_pairs, S_ij\n",
    "        import gc; gc.collect(); torch.cuda.empty_cache()\n",
    "    return preds, lambda_update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f3b25-707e-4baa-92dc-6e8608eb585b",
   "metadata": {},
   "source": [
    "If we were to use a loss fuunction instead of hand creafted gradients:\n",
    "\n",
    "$$C = \\sum_{\\{i,j\\} \\in I} \\frac{1}{2}(1 - S_{ij})\\sigma(p_i-p_j) + \\log(1 + e^{-\\sigma(p_i - p_j)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea249a52-2071-470f-96fa-90ab0b9fe20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, xb, sigma=0.5):\n",
    "    \n",
    "    srtd_relvs, srtd_idxs = xb[:, :, :, -1].sort(descending=True)\n",
    "    srtd_preds = torch.take_along_dim(preds[:,:,:,0], srtd_idxs, dim=-1)\n",
    "\n",
    "    sl = torch.arange(xb.shape[2], device=xb.device)\n",
    "    ij = torch.cartesian_prod(sl, sl)\n",
    "    idxs, = torch.nonzero(ij[:, 0] < ij[:, 1], as_tuple=True)\n",
    "    ij = ij[idxs]\n",
    "    \n",
    "    si_sj = srtd_relvs[:, :, ij] # these are the relevance scores for token_i and token_j\n",
    "    si, sj= si_sj[:, :, :, 0], si_sj[:, :, :, 1]\n",
    "    signs = torch.sign(si - sj)\n",
    "    pi_pj = srtd_preds[:, :, ij]\n",
    "    pi, pj = pi_pj[:,:,:,0], pi_pj[:,:,:,1]\n",
    "    exp_ij = torch.exp(-sigma*(pi -pj))\n",
    "    exp_ij[exp_ij==torch.inf] = tensor(1e6)\n",
    "    C = ( 0.5*(1 - signs)*sigma*(pi -pj) + torch.log(1 + exp_ij) ) #shape (64, 2234, 64)\n",
    "    # C = C.sum(dim=-1) # shape (64, 2234)\n",
    "    C = C.mean(dim=-1)\n",
    "    return C#.mean()\n",
    "\n",
    "def loss_fn2(preds, xb, sigma=.5):\n",
    "    \"Computes average pairwise cross-entropy loss\"\n",
    "    sl = xb.shape[2]\n",
    "    rel_diff = xb[:, :, :, -1, None] - xb[:, :, :, -1, None].transpose(-1, -2)\n",
    "    pos_pairs = (rel_diff > 0).float()\n",
    "    neg_pairs = (rel_diff < 0).float()\n",
    "    S_ij = pos_pairs - neg_pairs\n",
    "    preds_diff = preds - preds.transpose(-1, -2)\n",
    "    C = .5 * (1 - S_ij) * sigma * preds_diff - F.logsigmoid(sigma * preds_diff)\n",
    "    C = torch.triu(C, diagonal=1) # to take each pair only once\n",
    "    C = C.sum((-1,-2)) / (C.new_ones(C.shape[-2:]).triu(diagonal=1).sum())\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b785a00-5112-4256-a3d9-c0431def8484",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3cc98c0-b181-48ce-9028-6ee2f18d0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
