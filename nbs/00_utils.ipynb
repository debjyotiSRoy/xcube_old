{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# skip\n",
    "! [ -e /content ] && pip install -Uqq fastai #upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from fastcore.all import *\n",
    "from xcube.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Utilities needed for little repititive tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def namestr(obj, namespace):\n",
    "    \"Returns the name of the object `obj` passed\"\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'some_var'\n",
    "test_eq(namestr(a, globals()), ['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def list_files(startpath):\n",
    "    \"\"\" simulates the linux tree cmd \n",
    "    https://stackoverflow.com/questions/9727673/list-directory-tree-structure-in-python\n",
    "    \"\"\" \n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_paths(path, prefix=None):\n",
    "    \"\"\"\n",
    "    with `path` as basedir, makes data and models dir and \n",
    "    returns a dictionary of relevant pathlib objects\n",
    "    \"\"\"\n",
    "    path_data = path/'data'\n",
    "    path_model = path/'models'\n",
    "\n",
    "    path_model.mkdir(exist_ok=True)\n",
    "    path_data.mkdir(exist_ok=True)\n",
    "\n",
    "    data = path_data/(prefix+'.csv')\n",
    "    dls_lm_path, dls_lm_r_path = path_model/f\"{prefix}_dls_lm.pkl\", path_model/f\"{prefix}_dls_lm_r.pkl\"\n",
    "    dls_lm_vocab_path, dls_lm_vocab_r_path = path_model/f\"{prefix}_dls_lm_vocab.pkl\", path_model/f\"{prefix}_dls_lm_vocab_r.pkl\"\n",
    "    lm_path, lm_r_path = path_model/f\"{prefix}_lm.pth\", path_model/f\"{prefix}_lm_r.pth\"\n",
    "    lm_finetuned_path, lm_finetuned_r_path = path_model/f\"{prefix}_lm_finetuned.pth\", path_model/f\"{prefix}_lm_finetuned_r.pth\"\n",
    "    dsets_clas_path, dsets_clas_r_path = path_model/f\"{prefix}_dset_clas.pkl\", path_model/f\"{prefix}_dset_clas_r.pkl\"\n",
    "    dls_clas_path, dls_clas_r_path = path_model/f\"{prefix}_dls_clas.pkl\", path_model/f\"{prefix}_dls_clas_r.pkl\"\n",
    "    clas_path, clas_r_path = path_model/f\"{prefix}_clas.pth\", path_model/f\"{prefix}_clas_r.pth\"\n",
    "    dls_colab_path = path_model/f\"{prefix}_dls_colab.pkl\"\n",
    "    collab_path = path_model/f\"{prefix}_collab.pth\"\n",
    "    plist = [path, path_data, path_model, \n",
    "             data, \n",
    "             dls_lm_path, dls_lm_r_path,\n",
    "             dls_lm_vocab_path, dls_lm_vocab_r_path,\n",
    "             lm_path, lm_r_path,\n",
    "             lm_finetuned_path, lm_finetuned_r_path,\n",
    "             dsets_clas_path, dsets_clas_r_path,\n",
    "             dls_clas_path, dls_clas_r_path,\n",
    "             clas_path, clas_r_path,\n",
    "             dls_colab_path,\n",
    "             collab_path]\n",
    "    pdir = {}\n",
    "    for o in plist:  pdir[namestr(o, locals())[0]] = o\n",
    "    return pdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created temporary dir: /tmp/tmpxux8x8mn\n",
      "tmpxux8x8mn/\n",
      "    data/\n",
      "        mimic3-9k.csv\n",
      "    models/\n",
      "        mimic3-9k_dls_clas.pkl\n",
      "        mimic3-9k_dls_lm.pkl\n",
      "        mimic3-9k_lm_r.pth\n",
      "        mimic3-9k_lm_finetuned_r.pth\n",
      "        mimic3-9k_dls_lm_vocab_r.pkl\n",
      "        mimic3-9k_clas.pth\n",
      "        mimic3-9k_dset_clas.pkl\n",
      "        mimic3-9k_dls_colab.pkl\n",
      "        mimic3-9k_dls_lm_vocab.pkl\n",
      "        mimic3-9k_dls_lm_r.pkl\n",
      "        mimic3-9k_dset_clas_r.pkl\n",
      "        mimic3-9k_lm.pth\n",
      "        mimic3-9k_clas_r.pth\n",
      "        mimic3-9k_lm_finetuned.pth\n",
      "        mimic3-9k_dls_clas_r.pkl\n",
      "        mimic3-9k_collab.pth\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tempdirname:\n",
    "    print(f\"created temporary dir: {tempdirname}\")\n",
    "    _paths = make_paths(Path(tempdirname), \"mimic3-9k\")\n",
    "    for v in _paths.values(): v.touch()\n",
    "    list_files(tempdirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def plot_reduction(X, tSNE=True, n_comps=None, perplexity=30, figsize=(6,4)):\n",
    "    \"\"\"\n",
    "    PCA on X and plots the first two principal components, returns the decomposition \n",
    "    and the explained variances for each directions,\n",
    "    if `tSNE` then does a tSNE after PCA.\n",
    "    \"\"\"\n",
    "    reduction = \"tSNE\" if tSNE else \"PCA\"\n",
    "    pca = PCA(n_components=n_comps, svd_solver=\"full\")\n",
    "    X_red = pca.fit_transform(X)\n",
    "    if tSNE:\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity)\n",
    "        X_red = tsne.fit_transform(X_red[:, :50])\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    plt.scatter(X_red[:, 0], X_red[:, 1], marker='x')\n",
    "    ax.set_xlabel(\"1st component\")\n",
    "    ax.set_ylabel(\"2nd component\")\n",
    "    ax.set_title(f\"{reduction} Decomposition\")\n",
    "    plt.show()\n",
    "    return X_red, pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_text.models.core.ipynb.\n",
      "Converted 03_text.learner.ipynb.\n",
      "Converted 04_metrics.ipynb.\n",
      "Converted 05_collab.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Step 1: Data Preprocessing- Normalization and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mto_np\u001b[49m(lbs_emb)\n\u001b[1;32m      2\u001b[0m X_copy \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_np' is not defined"
     ]
    }
   ],
   "source": [
    "X = to_np(lbs_emb)\n",
    "X_copy = X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set:\n",
    "* $x^1, \\cdots, x^m$ where $x^i \\in \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, std = X.mean(axis=0), X.std(axis=0)\n",
    "mu.shape, std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X-mu)/std\n",
    "X_copy = StandardScaler().fit_transform(X_copy)\n",
    "assert np.allclose(X, X_copy, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Step 2: Compute the Covariance Matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} x^i {x^i}^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "m,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1/m * X.T @ X\n",
    "sigma.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Step 3: Compute the Eigen Vectors of `Sigma` (using SVD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(sigma)\n",
    "test_eq((u.shape, s.shape, vh.shape), ((400, 400), (400,), (400, 400)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Step 4: Take the first $k$ columns of `u` -> these are the direction vectors (or mathematically, the first $k$ eigen vectors of `sigma`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comps = 300\n",
    "u_red = u[:, :n_comps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Step 5: Compute the projections, $z_i \\in \\mathbb{R}^k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^i = u_{\\textsf{red}}^Tx^i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X @ u_red "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Step 6: Choosing the number of principal components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total variation in the data: $\\frac{1}{m} \\sum_{i}^{m} ||x^i||^2$\n",
    "* Squared projection error: $\\frac{1}{m} \\sum_{i}^{m} ||x^i - z^i||^2$\n",
    "* Choose $k$ to be the samllest value such that $\\frac{\\frac{1}{m} \\sum_{i}^{m} ||x^i - z^i||^2}{\\frac{1}{m} \\sum_{i}^{m} ||x^i||^2} \\leq 0.01 (\\text{or } 0.05)$\n",
    "* **Shortcut:** $\\frac{\\sum_i^k s_{ii}}{\\sum_i^n s_{ii}} >= 0.99 (\\text{or } 0.95)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = np.sum(s[:n_comps])/np.sum(s)\n",
    "var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two principal components explains only ~18% of the variance, but let's cross our fingers and toss some matplotlib into the mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(s[:2])/np.sum(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.scatter(-Z[:, 0], -Z[:, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
