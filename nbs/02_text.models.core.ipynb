{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3b7cc-8ead-4343-a437-bc0fbfc28ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1d2dc-4959-4a79-b27a-77acbc54e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from fastai.data.all import *\n",
    "from fastai.text.models.core import *\n",
    "from fastai.text.models.awdlstm import *\n",
    "from xcube.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a2ebb-00ba-4295-9717-2f800a36ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4e9f7-bc4b-454a-abb3-d7af066b7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.models.core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb9980-a025-45bc-a614-0bcb621bea72",
   "metadata": {},
   "source": [
    "# Core text modules\n",
    "\n",
    "> Contain the modules common between different architectures and the generic functions to get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c319081-5bc4-4dca-a277-e725860f2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_model_meta = {AWD_LSTM: {'hid_name':'emb_sz', 'url':URLs.WT103_FWD, 'url_bwd':URLs.WT103_BWD,\n",
    "                          'config_lm':awd_lstm_lm_config, 'split_lm': awd_lstm_lm_split,\n",
    "                          'config_clas':awd_lstm_clas_config, 'split_clas': awd_lstm_clas_split},}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811afdb-22d7-4e86-b15a-77b63e1a511e",
   "metadata": {},
   "source": [
    "## Basic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c5748-ef36-401c-bc6b-72f0fcb4dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential pytorch module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children(): getattr(c, 'reset', noop)()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b84e3-067f-4110-9546-3500b69ceff2",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e098324-afd1-41ed-9d3e-a4f040f7a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _pad_tensor(t, bs):\n",
    "    if t.size(0) < bs: return torch.cat([t, t.new_zeros(bs-t.size(0), *t.shape[1:])])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff116dfc-1388-4dd6-81f1-3e618ccac43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SentenceEncoder(Module):\n",
    "    \"Create an encoder over `module` that can process a full sentence.\"\n",
    "    def __init__(self, bptt, module, pad_idx=1, max_len=None): store_attr('bptt,module,pad_idx,max_len')\n",
    "    def reset(self): getattr(self.module, 'reset', noop)()\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        self.reset()\n",
    "        mask = input == self.pad_idx\n",
    "        outs,masks = [],[]\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            #Note: this expects that sequence really begins on a round multiple of bptt\n",
    "            real_bs = (input[:,i] != self.pad_idx).long().sum()\n",
    "            o = self.module(input[:real_bs,i: min(i+self.bptt, sl)])\n",
    "            if self.max_len is None or sl-i <= self.max_len:\n",
    "                outs.append(o)\n",
    "                masks.append(mask[:,i: min(i+self.bptt, sl)])\n",
    "        outs = torch.cat([_pad_tensor(o, bs) for o in outs], dim=1)\n",
    "        mask = torch.cat(masks, dim=1)\n",
    "        return outs,mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d355c2-825a-4bb9-aae9-da6b3d56bb78",
   "metadata": {},
   "source": [
    "> Warning: This module expects the inputs padded with most of the padding first, with the sequence beginning at a round multiple of bptt (and the rest of the padding at the end). Use `pad_input_chunk` to get your data in a suitable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbbcff-6828-4e9e-9fed-2fe7ec1ab0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def masked_concat_pool(output, mask, bptt):\n",
    "    \"Pool `MultiBatchEncoder` outputs into one vector [last_hidden, max_pool, avg_pool]\"\n",
    "    lens = output.shape[1] - mask.long().sum(dim=1)\n",
    "    last_lens = mask[:,-bptt:].long().sum(dim=1)\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).sum(dim=1)\n",
    "    avg_pool.div_(lens.type(avg_pool.dtype)[:,None])\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    x = torch.cat([output[torch.arange(0, output.size(0)),-last_lens-1], max_pool, avg_pool], 1) #Concat pooling.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94171767-3926-4a8b-b13b-a304b366918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PoolingLinearClassifier(Module):\n",
    "    \"Create a linear classifier with pooling\"\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        if len(ps) != len(dims)-1: raise ValueError(\"Number of layers and dropout values do not match.\")\n",
    "        acts = [nn.ReLU(inplace=True)] * (len(dims) - 2) + [None]\n",
    "        layers = [LinBnDrop(i, o, p=p, act=a) for i,o,p,a in zip(dims[:-1], dims[1:], ps, acts)]\n",
    "        if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.bptt = bptt\n",
    "\n",
    "    def forward(self, input):\n",
    "        out,mask = input\n",
    "        x = masked_concat_pool(out, mask, self.bptt)\n",
    "        x = self.layers(x)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56bd99-2f56-4053-9792-7555b7893064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CAML2(Module):\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        self.layer = LinBnDrop(dims[0], dims[1], p=ps, act=None)\n",
    "        self.bptt = bptt\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, mask = input\n",
    "        x = masked_concat_pool(out, mask, self.bptt)\n",
    "        x = self.layer(x)\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c85678-04c1-4a62-83f4-f112d34c5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CAML3(Module):\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        self.lbs = dims[-1] \n",
    "        self.fts = dims[0]//3\n",
    "        self.layers = Lin1BnDrop(self.lbs, self.fts, p=ps, act=None) # deb\n",
    "        self.bptt = bptt\n",
    "        self.emb_label = nn.Embedding(self.lbs, self.fts) # deb\n",
    "\n",
    "    def forward(self, input):\n",
    "        out, _ = input\n",
    "        # breakpoint()\n",
    "        # x = masked_concat_pool(out, mask, self.bptt)\n",
    "        attn_wgts = out @ self.emb_label.weight.transpose(0, 1) # deb\n",
    "        attn_wgts = F.softmax(attn_wgts, 1) # deb\n",
    "        ctx = attn_wgts.transpose(1,2) @ out # deb\n",
    "        \n",
    "        x = self.layers(ctx)\n",
    "        x = x.view(x.shape[0], x.shape[1])\n",
    "        return x, out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f7b3e-f778-4758-bfb1-6eed725de8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_text_classifier(arch, vocab_sz, n_class, seq_len=72, config=None, drop_mult=1., lin_ftrs=None,\n",
    "                       ps=None, pad_idx=1, max_len=72*20, y_range=None):\n",
    "    \"Create a text classifier from `arch` and its `config`, maybe `pretrained`\"\n",
    "    meta = _model_meta[arch]\n",
    "    config = ifnone(config, meta['config_clas']).copy()\n",
    "    for k in config.keys():\n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "    if lin_ftrs is None: lin_ftrs = [50]\n",
    "    if ps is None: ps = [0.1]*len(lin_ftrs)\n",
    "#     layers = [config[meta['hid_name']] * 3] + lin_ftrs + [n_class]\n",
    "    layers = [config[meta['hid_name']] * 3] + [n_class]\n",
    "#     ps = [config.pop('output_p')] + ps\n",
    "    ps = config.pop('output_p')\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    encoder = SentenceEncoder(seq_len, arch(vocab_sz, **config), pad_idx=pad_idx, max_len=max_len)\n",
    "#     decoder = PoolingLinearClassifier(layers, ps, bptt=seq_len, y_range=y_range)\n",
    "    decoder = CAML3(layers, ps, bptt=seq_len, y_range=y_range)\n",
    "    model = SequentialRNN(encoder, decoder)\n",
    "    return model if init is None else model.apply(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a15e0-19b3-4d4b-9289-9f30566b4f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_text.models.core.ipynb.\n",
      "Converted 03_text.learner.ipynb.\n",
      "Converted 04_metrics.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ec094-cf9c-4553-98c9-e483e3ec21f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
