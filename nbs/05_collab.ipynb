{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d20673dc-20ba-4283-b0c9-e2e2e3ff999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# skip\n",
    "! [ -e /content ] && pip install -Uqq fastai # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "96433927-66e1-4824-a865-cb01441754e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp collab\n",
    "# default_class_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2e3c3cea-e9ef-460f-ac49-561e8af296bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.tabular.all import *\n",
    "from fastai.collab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "688c74c3-41a5-494a-aa7b-80a4674f70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41320ed0-06c5-49f3-9f75-e832e36cfa6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Collaborative filtering\n",
    "> Tools to add to the [fastai collab](https://docs.fast.ai/collab.html) to make the learning transferable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542e307-dbc8-4667-9e53-0c26c1237610",
   "metadata": {},
   "source": [
    "## Loading `users`/`items` embeddings from a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ab11e-6b6a-4ba5-a501-b53ea37ba7c0",
   "metadata": {},
   "source": [
    "In a collab model, to load a pretrained vocabulary, we need to adapt the embeddings of the  vocabulary used for the pre-training to the vocabulary of our current collab corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c152c2c3-0e98-4b6f-b1d2-ebcaf75a8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def match_embeds(\n",
    "    old_wgts:dict, # Embedding weights of the pretrained model\n",
    "    old_vocab:list, # Vocabulary (tokens and labels) of the corpus used for pretraining\n",
    "    new_vocab:dict # Current collab corpus vocabulary (`users` and `items`)\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Convert the `users` and `items` (possibly saved as `0.module.encoder.weight` and `1.attn.lbs_weight.weight` respectively) \n",
    "    embedding in `old_wgts` to go from `old_vocab` to `new_vocab`\n",
    "    \"\"\"\n",
    "    import pdb; pdb.set_trace()\n",
    "    u_bias, u_wgts = None, old_wgts.get('0.module.encoder.weight')\n",
    "    print(f\"{u_wgts.shape = }\")\n",
    "    i_bias, i_wgts = old_wgts.get('1.attn.lbs_bias.weight', None), old_wgts.get('1.attn.lbs_weight.weight')\n",
    "    print(f\"{i_wgts.shape = }\")\n",
    "    u_wgts_m, i_wgts_m = u_wgts.mean(0), i_wgts.mean(0)\n",
    "    new_u_wgts = u_wgts.new_zeros((len(new_vocab['token']), u_wgts.size(1)))\n",
    "    print(f\"{new_u_wgts.shape = }\")\n",
    "    new_i_wgts = i_wgts.new_zeros((len(new_vocab['label']), i_wgts.size(1)))\n",
    "    print(f\"{new_i_wgts.shape = }\")\n",
    "    if u_bias is not None:\n",
    "        u_bias_m = u_bias.mean(0)\n",
    "        new_u_bias = u_bias.new_zeros((len(new_vocab['token']), 1))\n",
    "    if i_bias is not None:\n",
    "        i_bias_m = i_bias.mean(0)\n",
    "        new_i_bias = i_bias.new_zeros((len(new_vocab['label']), 1))\n",
    "    u_old = old_vocab[0]\n",
    "    u_old_o2i = u_old.o2i if hasattr(u_old, 'o2i') else {w:i for i,w in enumerate(u_old)}\n",
    "    i_old = old_vocab[1]\n",
    "    i_old_o2i = i_old.o2i if hasattr(i_old, 'o2i') else {w:i for i,w in enumerate(i_old)}\n",
    "    u_miss, i_miss = 0, 0\n",
    "    for i,w in enumerate(new_vocab['token']):\n",
    "        idx = u_old_o2i.get(w, -1)\n",
    "        new_u_wgts[i] = u_wgts[idx] if idx>=0 else u_wgts_m\n",
    "        if u_bias is not None: new_u_bias[i] = u_bias[idx] if idx>=0 else u_bias_m\n",
    "        if idx == -1: u_miss = u_miss + 1\n",
    "    for i,w in enumerate(new_vocab['label']):\n",
    "        idx = i_old_o2i.get(w, -1)\n",
    "        new_i_wgts[i] = i_wgts[idx] if idx>=0 else i_wgts_m\n",
    "        if i_bias is not None: new_i_bias[i] = i_bias[idx] if idx>=0 else i_bias_m\n",
    "        if idx == -1: i_miss = i_miss + 1\n",
    "    old_wgts['0.module.encoder.weight'] = new_u_wgts\n",
    "    if '0.module.encoder_dp.emb.weight' in old_wgts: old_wgts['0.module.encoder_dp.emb.weight'] = new_u_wgts.clone()\n",
    "    if u_bias is not None: pass\n",
    "    old_wgts['1.attn.lbs_weight.weight'] = new_i_wgts\n",
    "    if '1.attn.lbs_weight_dp.emb.weight' in old_wgts: old_wgts['1.attn.lbs_weight_dp.emb.weight'] = new_i_wgts.clone()\n",
    "    if i_bias is not None: old_wgts['1.attn.lbs_bias.weight'] = new_i_bias\n",
    "    return old_wgts, u_miss, i_miss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669edc9f-72e7-4be4-a3b8-0ed4fd27a8aa",
   "metadata": {},
   "source": [
    "## Create a `Learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "eb441ac1-d763-4922-8624-8e020a640344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_pretrained_keys(\n",
    "    model, # Model architecture\n",
    "    wgts:dict # Model weights\n",
    ") -> tuple:\n",
    "    \"Load relevant pretrained `wgts` in `model\"\n",
    "    sd = model.state_dict()\n",
    "    u_wgts, u_bias = wgts.get('0.module.encoder.weight', None), None\n",
    "    print(f\"inside load_keys{u_wgts.shape = }\")\n",
    "    if u_wgts is not None: sd['u_weight.weight'].data = u_wgts.data\n",
    "    if u_bias is not None: sd['u_bias.weight'].data = u_bias.data\n",
    "    i_wgts, i_bias = wgts.get('1.attn.lbs_weight.weight', None), wgts.get('1.attn.lbs_bias.weight', None)\n",
    "    print(f\"inside load{i_wgts.shape = }\")\n",
    "    if i_wgts is not None: sd['i_weight.weight'].data = i_wgts.data\n",
    "    if i_bias is not None: sd['i_bias.weight'].data = i_bias.data\n",
    "    return model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "360213ad-8a42-4f10-ad19-ef9acc26c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CollabLearner(Learner):\n",
    "    \"Basic class for a `Learner` in Collab.\"\n",
    "    @delegates(save_model)\n",
    "    def save(self, \n",
    "        file:str, # Filename for the state_directory of model\n",
    "        **kwargs):\n",
    "        \"\"\"\n",
    "        Save model and optimizer state (if `with_opt`) to `self.path/self.model_dir/file`\n",
    "        Save `self.dls.classes` to `self.path.self.model_dir/collab_vocab.pkl`\n",
    "        \"\"\"\n",
    "        model_file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "        vocab_file = join_path_file(file+'_vocab', self.path/self.model_dir, ext='.pkl')\n",
    "        save_model(model_file, self.model, getattr(self,'opt', None), **kwargs)\n",
    "        save_pickle(vocab_file, self.dls.classes)\n",
    "        return model_file\n",
    "    \n",
    "    def load_vocab(self,\n",
    "        wgts_fname:str, #Filename of the saved weights\n",
    "        vocab_fname:str, # Saved vocabulary filename in pickle format\n",
    "        model=None # Model to load parameters from, deafults to `Learner.model`\n",
    "    ):\n",
    "        \"Load the vocabulary (`users` and/or `items`) from a pretrained model and adapt it to the collab vocabulary.\"\n",
    "        old_vocab = load_pickle(vocab_fname)\n",
    "        new_vocab = self.dls.classes\n",
    "        distrib_barrier()\n",
    "        wgts = torch.load(wgts_fname, map_location=lambda storage,loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model'] # Just in case the pretrained model was saved with an optimizer\n",
    "        wgts, *_ = match_embeds(wgts, old_vocab, new_vocab)\n",
    "        load_pretrained_keys(self.model if model is None else model, wgts)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f02a8a92-7213-4888-91ff-089d6246d359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"CollabLearner\" class=\"doc_header\"><code>class</code> <code>CollabLearner</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>CollabLearner</code>(**`dls`**, **`model`**, **`loss_func`**=*`None`*, **`opt_func`**=*`Adam`*, **`lr`**=*`0.001`*, **`splitter`**=*`trainable_params`*, **`cbs`**=*`None`*, **`metrics`**=*`None`*, **`path`**=*`None`*, **`model_dir`**=*`'models'`*, **`wd`**=*`None`*, **`wd_bn_bias`**=*`False`*, **`train_bn`**=*`True`*, **`moms`**=*`(0.95, 0.85, 0.95)`*) :: `Learner`\n",
       "\n",
       "Basic class for a `Learner` in Collab."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CollabLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c7894-9ad8-472a-b54e-593269cea8ce",
   "metadata": {},
   "source": [
    "It works exactly as a normal `learner`, the only difference is that it also saves the `items` vocabulary used by `self.model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fa328-8e5a-45f5-b86c-9c202f4d36b5",
   "metadata": {},
   "source": [
    "The following function lets us quickly create a `Learner` for collaborative filtering from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f7f8cc78-127c-4dc3-b7d3-d0fc3fecf029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Learner.__init__)\n",
    "def collab_learner(dls, n_factors=50, use_nn=False, emb_szs=None, layers=None, config=None, y_range=None, loss_func=None, pretrained=False, **kwargs):\n",
    "    \"Create a Learner for collaborative filtering on `dls`.\"\n",
    "    emb_szs = get_emb_sz(dls, ifnone(emb_szs, {}))\n",
    "    if loss_func is None: loss_func = MSELossFlat()\n",
    "    if config is None: config = tabular_config()\n",
    "    if y_range is not None: config['y_range'] = y_range\n",
    "    if layers is None: layers = [n_factors]\n",
    "    if use_nn: model = EmbeddingNN(emb_szs=emb_szs, layers=layers, **config)\n",
    "    else:      model = EmbeddingDotBias.from_classes(n_factors, dls.classes, y_range=y_range)\n",
    "    learn = CollabLearner(dls, model, loss_func=loss_func, **kwargs)\n",
    "    if pretrained:\n",
    "        try: fnames = [list(learn.path.glob(f'**/clas/*clas*.{ext}'))[0] for ext in ['pth', 'pkl']] \n",
    "        except: IndexError: print(f'The model in {learn.path} is incomplete, re-train it'); raise\n",
    "        learn = learn.load_vocab(*fnames, model=learn.model)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bbccae-b927-454b-bee7-f8e473fd95a9",
   "metadata": {},
   "source": [
    "If `use_nn=False`, the model used is an `EmbeddingDotBias` with `n_factors` and `y_range`. Otherwise, it's a `EmbeddingNN` for which you can pass `emb_szs` (will be inferred from the `dls` with `get_emb_sz` if you don't provide any), `layers` (defaults to `[n_factors]`) `y_range`, and a `config` that you can create with `tabular_config` to customize your model. \n",
    "\n",
    "`loss_func` will default to `MSELossFlat` and all the other arguments are passed to `Learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "48e3defd-d2bf-40fa-8e92-d76f5b49bb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>1097</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1255504951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>561</td>\n",
       "      <td>924</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1172695223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157</td>\n",
       "      <td>260</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1291598691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358</td>\n",
       "      <td>1210</td>\n",
       "      <td>5.0</td>\n",
       "      <td>957481884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130</td>\n",
       "      <td>316</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1138999234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0      73     1097     4.0  1255504951\n",
       "1     561      924     3.5  1172695223\n",
       "2     157      260     3.5  1291598691\n",
       "3     358     1210     5.0   957481884\n",
       "4     130      316     2.0  1138999234"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.ML_SAMPLE)\n",
    "ratings = pd.read_csv(path/'ratings.csv')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d96803ac-46af-4eb4-837d-070120dfb96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>212</td>\n",
       "      <td>1721</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>364</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>539</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>460</td>\n",
       "      <td>1270</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>500</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>665</td>\n",
       "      <td>1270</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>105</td>\n",
       "      <td>6539</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>475</td>\n",
       "      <td>3114</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>130</td>\n",
       "      <td>4886</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>243</td>\n",
       "      <td>380</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = CollabDataLoaders.from_df(ratings, bs=64)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "02abb03d-9b3c-46d8-b96c-bda19df7fe48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.477255</td>\n",
       "      <td>2.338984</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    learn = collab_learner(dls, y_range=(0,5), path=d)\n",
    "    learn.fit(1)\n",
    "    \n",
    "    # Test save created a file\n",
    "    learn.save('tmp')\n",
    "    assert (Path(d)/'models/tmp.pth').exists()\n",
    "    assert (Path(d)/'models/tmp_vocab.pkl').exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7e1a9b7b-22e9-4e12-9c86-64047ec91efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_text.models.core.ipynb.\n",
      "Converted 03_text.learner.ipynb.\n",
      "Converted 04_metrics.ipynb.\n",
      "Converted 05_collab.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
