{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5d8d823-2303-49fa-ba64-7881bcb70b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83bbfc56-7adf-43a9-b1db-395254e11a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *\n",
    "from fastai.text.learner import *\n",
    "from fastai.callback.rnn import *\n",
    "from xcube.text.models.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de734b9-c91e-42d1-a986-3d9a4f4e8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6918a5a-ce9c-4222-bf49-fde8e0268244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d9282-73d1-4658-a896-ab690f74b689",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Learner for the text application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2140a3d-8230-4bfa-ac34-0d36441a774e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a39c937-70c1-438b-84d9-34d7ef4a8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from xcube.text.models.core import _model_meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dd36b44-1764-4c07-814c-df115a2abc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_text_vocab(dls:DataLoaders) -> list:\n",
    "    \"Get text vocabulary from `DataLoaders`\"\n",
    "    vocab = dls.vocab\n",
    "    if isinstance(vocab, L): vocab = vocab[0]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93bbf616-ed91-4681-8733-ba59703b8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_label_vocab(dls:DataLoaders) -> list:\n",
    "    \"Get label vocabulary from `DataLoaders`\"\n",
    "    vocab = dls.vocab\n",
    "    if isinstance(vocab, L): vocab = vocab[1]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c861c18-839f-4423-972c-1632fc3289e6",
   "metadata": {},
   "source": [
    "## Base `Learner` for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5af50343-383d-4284-b11e-d3016d54e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Learner.__init__)\n",
    "class TextLearner(Learner):\n",
    "    \"Basic class for a `Learner` in NLP.\"\n",
    "    def __init__(self, \n",
    "        dls:DataLoaders, # Text `DataLoaders`\n",
    "        model, # A standard PyTorch model\n",
    "        alpha:float=2., # Param for `RNNRegularizer`\n",
    "        beta:float=1., # Param for `RNNRegularizer`\n",
    "        moms:tuple=(0.8,0.7,0.8), # Momentum for `Cosine Annealing Scheduler`\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(dls, model, moms=moms, **kwargs)\n",
    "        self.add_cbs(rnn_cbs())\n",
    "\n",
    "    def save_encoder(self, \n",
    "        file:str # Filename for `Encoder` \n",
    "    ):\n",
    "        \"Save the encoder to `file` in the model directory\"\n",
    "        if rank_distrib(): return # don't save if child proc\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        torch.save(encoder.state_dict(), join_path_file(file, self.path/self.model_dir, ext='.pth'))\n",
    "\n",
    "    def load_encoder(self, \n",
    "        file:str, # Filename of the saved encoder \n",
    "        device:(int,str,torch.device)=None # Device used to load, defaults to `dls` device\n",
    "    ):\n",
    "        \"Load the encoder `file` from the model directory, optionally ensuring it's on `device`\"\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if device is None: device = self.dls.device\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        distrib_barrier()\n",
    "        wgts = torch.load(join_path_file(file,self.path/self.model_dir, ext='.pth'), map_location=device)\n",
    "        encoder.load_state_dict(clean_raw_keys(wgts))\n",
    "        self.freeze()\n",
    "        return self\n",
    "\n",
    "    def load_pretrained(self, \n",
    "        wgts_fname:str, # Filename of saved weights \n",
    "        vocab_fname:str, # Saved vocabulary filename in pickle format\n",
    "        model=None # Model to load parameters from, defaults to `Learner.model`\n",
    "    ):\n",
    "        \"Load a pretrained model and adapt it to the data vocabulary.\"\n",
    "        old_vocab = load_pickle(vocab_fname)\n",
    "        new_vocab = _get_text_vocab(self.dls)\n",
    "        distrib_barrier()\n",
    "        wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model'] #Just in case the pretrained model was saved with an optimizer\n",
    "        wgts = match_embeds(wgts, old_vocab, new_vocab)\n",
    "        load_ignore_keys(self.model if model is None else model, clean_raw_keys(wgts))\n",
    "        self.freeze()\n",
    "        return self\n",
    "\n",
    "    #For previous versions compatibility. Remove at release\n",
    "    @delegates(load_model_text)\n",
    "    def load(self, \n",
    "        file:str, # Filename of saved model \n",
    "        with_opt:bool=None, # Enable to load `Optimizer` state\n",
    "        device:(int,str,torch.device)=None, # Device used to load, defaults to `dls` device\n",
    "        **kwargs\n",
    "    ):\n",
    "        if device is None: device = self.dls.device\n",
    "        if self.opt is None: self.create_opt()\n",
    "        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "        load_model_text(file, self.model, self.opt, device=device, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def load_lbs(self,\n",
    "        wgts_fname:str, # Filename of the saved label embeddings of colab\n",
    "        lbs_fname:str, # Saved Vocabulary of colab labels in pickle format \n",
    "        device:(int,str,torch.device)=None # Device used to load, defaults to `dls` device\n",
    "    ):\n",
    "        \"Load the label embeddings learned by colab stored in `file`, optionally ensuring it's on `device`\"\n",
    "        decoder = get_model(self.model)[1]\n",
    "        if device is None: device = self.dls.device\n",
    "        if hasattr(decoder, module): decoder = decoder.module\n",
    "        attn = decoder.attn\n",
    "        old_lbs_vocab = load_pickle(lbs_fname)\n",
    "        new_lbs_vocab = _get_label_vocab(self.dls)\n",
    "        distrib_barrier()\n",
    "        wgts = torch.load(wgts_fname, map_location= lambda storage,loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model'] #Just in case the pretrained model was saved with an optimizer\n",
    "        wgts = match_embeds(wgts, old_lbs_vocab, new_lbs_vocab)\n",
    "        load_ignore_keys(decoder.attn.lbs_emb, clean_raw_keys(wgts))\n",
    "        self.freeze()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c077c1d-1fde-4c45-ae1e-6f6cd3f58b4c",
   "metadata": {},
   "source": [
    "Adds a `ModelResetter` and an `RNNRegularizer` with `alpha` and `beta` to the callbacks, the rest is the same as `Learner` init. \n",
    "\n",
    "This `Learner` adds functionality to the base class:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001ab92-67ce-4357-b447-0b85c0cbd384",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `Learner` convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38916689-ddc4-498f-881d-36ddebd9e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@delegates(Learner.__init__)\n",
    "def text_classifier_learner(dls, arch, seq_len=72, config=None, backwards=False, pretrained=True, drop_mult=0.5, n_out=None,\n",
    "                           lin_ftrs=None, ps=None, max_len=72*20, y_range=None, **kwargs):\n",
    "    \"Create a `Learner` with a text classifier from `dls` and `arch`.\"\n",
    "    vocab = _get_text_vocab(dls)\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from the data, set `dls.c` or pass `n_out`\"\n",
    "    model = get_text_classifier(arch, len(vocab), n_out, seq_len=seq_len, config=config, y_range=y_range,\n",
    "                                drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps, max_len=max_len)\n",
    "    meta = _model_meta[arch]\n",
    "    learn = TextLearner(dls, model, splitter=meta['split_clas'], **kwargs)\n",
    "    url = 'url_bwd' if backwards else 'url'\n",
    "    if pretrained:\n",
    "        if url not in meta:\n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta[url], c_key='model')\n",
    "        try: fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n",
    "        learn = learn.load_pretrained(*fnames, model=learn.model[0])\n",
    "        learn.freeze()\n",
    "    return learn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "054f91e1-d298-4090-9f07-f687471afd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_text.models.core.ipynb.\n",
      "Converted 03_text.learner.ipynb.\n",
      "Converted 04_metrics.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
