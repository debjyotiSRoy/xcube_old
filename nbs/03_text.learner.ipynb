{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8d823-2303-49fa-ba64-7881bcb70b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbfc56-7adf-43a9-b1db-395254e11a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *\n",
    "from fastai.text.learner import *\n",
    "from fastai.callback.rnn import *\n",
    "from fastai.text.models.awdlstm import *\n",
    "from xcube.text.models.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de734b9-c91e-42d1-a986-3d9a4f4e8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6918a5a-ce9c-4222-bf49-fde8e0268244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d9282-73d1-4658-a896-ab690f74b689",
   "metadata": {},
   "source": [
    "# Learner for the text application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b331383-0c65-4d88-9689-4eaa2c0eef47",
   "metadata": {},
   "source": [
    "## Loading label embeddings from a pretrained colab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88dfc32-d2b6-4107-9b5e-5e3f9933380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_text_vocab(dls:DataLoaders) -> list:\n",
    "    \"Get text vocabulary from `DataLoaders`\"\n",
    "    vocab = dls.vocab\n",
    "    if isinstance(vocab, L): vocab = vocab[0]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bbf616-ed91-4681-8733-ba59703b8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_label_vocab(dls:DataLoaders) -> list:\n",
    "    \"Get label vocabulary from `DataLoaders`\"\n",
    "    vocab = dls.vocab\n",
    "    if isinstance(vocab, L): vocab = vocab[1]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e749a-83ac-41ad-b5b8-b7d141bc0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def match_collab(\n",
    "    old_wgts:dict, # Embedding weights of the colab model\n",
    "    collab_vocab:dict, # Vocabulary of `token` and `label` used for colab pre-training\n",
    "    lbs_vocab:list # Current labels vocabulary\n",
    ") -> dict:\n",
    "    \"Convert the label embedding in `old_wgts` to go from `old_vocab` in colab to `lbs_vocab`\"\n",
    "    bias, wgts = old_wgts.get('i_bias.weight', None), old_wgts.get('i_weight.weight')\n",
    "    wgts_m = wgts.mean(0)\n",
    "    new_wgts = wgts.new_zeros((len(lbs_vocab), wgts.size(1)))\n",
    "    if bias is not None:\n",
    "        bias_m = bias.mean(0)\n",
    "        new_bias = bias.new_zeros((len(lbs_vocab), 1))\n",
    "    collab_lbs_vocab = collab_vocab['label']\n",
    "    collab_o2i = collab_lbs_vocab.o2i if hasattr(collab_lbs_vocab, 'o2i') else {w:i for i,w in enumerate(collab_lbs_vocab)}\n",
    "    missing = 0\n",
    "    for i,w in enumerate(lbs_vocab):\n",
    "        idx = collab_o2i.get(w, -1)\n",
    "        new_wgts[i] = wgts[idx] if idx>=0 else wgts_m\n",
    "        if bias is not None: new_bias[i] = bias[idx] if idx>=0 else bias_m\n",
    "        if idx == -1: missing = missing + 1\n",
    "    old_wgts['i_weight.weight'] = new_wgts\n",
    "    if bias is not None: old_wgts['i_bias.weight'] = new_bias\n",
    "    return old_wgts, missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0c5d8-7104-4971-a78f-d17be39abccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = {'u_weight.weight': torch.randn(3,5), \n",
    "        'i_weight.weight': torch.randn(4,5),\n",
    "        'u_bias.weight'  : torch.randn(3,1),\n",
    "        'i_bias.weight'  : torch.randn(4,1)}\n",
    "collab_vocab = {'token': ['#na#', 'sun', 'moon', 'earth', 'mars'],\n",
    "                'label': ['#na#', 'a', 'c', 'b']}\n",
    "lbs_vocab = ['a', 'b', 'c']\n",
    "new_wgts, missing = match_collab(wgts.copy(), collab_vocab, lbs_vocab)\n",
    "test_eq(missing, 0)\n",
    "test_close(wgts['u_weight.weight'], new_wgts['u_weight.weight'])\n",
    "test_close(wgts['u_bias.weight'], new_wgts['u_bias.weight'])\n",
    "with ExceptionExpected(ex=AssertionError, regex=\"close\"):\n",
    "    test_close(wgts['i_weight.weight'][1:], new_wgts['i_weight.weight'])\n",
    "    test_close(wgts['i_bias.weight'][1:], new_wgts['i_bias.weight'])\n",
    "old_w, new_w = wgts['i_weight.weight'], new_wgts['i_weight.weight']\n",
    "old_b, new_b = wgts['i_bias.weight'], new_wgts['i_bias.weight']\n",
    "for (old_k,old_v), (new_k, new_v) in zip(wgts.items(), new_wgts.items()): \n",
    "    if old_k.startswith('u'): test_eq(old_v.size(), new_v.size())\n",
    "    else: test_ne(old_v.size(), new_v.size());\n",
    "    # print(f\"old: {old_k} = {old_v.size()}, new: {new_k} = {new_v.size()}\")\n",
    "test_eq(new_w[0], old_w[1]); test_eq(new_b[0], old_b[1])\n",
    "test_eq(new_w[1], old_w[3]); test_eq(new_b[1], old_b[3])\n",
    "test_eq(new_w[2], old_w[2]); test_eq(new_b[2], old_b[2])\n",
    "test_shuffled(list(old_b[1:].squeeze().numpy()), list(new_b.squeeze().numpy()))\n",
    "test_eq(torch.sort(old_b[1:], dim=0)[0], torch.sort(new_b, dim=0)[0])\n",
    "test_eq(torch.sort(old_w[1:], dim=0)[0], torch.sort(new_w, dim=0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c861c18-839f-4423-972c-1632fc3289e6",
   "metadata": {},
   "source": [
    "## Base `Learner` for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e89ee-1737-4069-9627-8a5c6b31db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_collab_keys(\n",
    "    model, # Model architecture\n",
    "    wgts:dict # Model weights\n",
    ") -> tuple:\n",
    "    \"Load only collab `wgts` (`i_weight` and `i_bias`) in `model`, keeping the rest as is\"\n",
    "    sd = model.state_dict()\n",
    "    lbs_weight, i_weight = sd.get('1.attn.lbs_weight.weight', None), wgts.get('i_weight.weight', None)\n",
    "    lbs_bias, i_bias = sd.get('1.attn.lbs_weight.bias', None), wgts.get('i_bias.weight', None) \n",
    "    if lbs_weight is not None and i_weight is not None: lbs_weight.data = i_weight.data\n",
    "    if lbs_bias is not None and i_bias is not None: lbs_bias.data = i_bias.data\n",
    "    if '1.attn.lbs_weight_dp.emb.weight' in sd:\n",
    "        sd['1.attn.lbs_weight_dp.emb.weight'] = i_weight.data.clone()\n",
    "    return model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2a2e1-41d5-4d57-b4df-5925ec6096d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = awd_lstm_clas_config.copy()\n",
    "config.update({'n_hid': 10, 'emb_sz': 5})\n",
    "\n",
    "tst = get_text_classifier(AWD_LSTM, 100, 3, config=config)\n",
    "old_sd = tst.state_dict().copy()\n",
    "\n",
    "import copy\n",
    "old_sd = copy.deepcopy(tst.state_dict())\n",
    "\n",
    "load_collab_keys(tst, new_wgts)\n",
    "test_ne(old_sd['1.attn.lbs_weight.weight'], tst.state_dict()['1.attn.lbs_weight.weight'])\n",
    "test_eq(tst.state_dict()['1.attn.lbs_weight.weight'], new_wgts['i_weight.weight'])\n",
    "test_ne(old_sd['1.attn.lbs_weight_dp.emb.weight'], tst.state_dict()['1.attn.lbs_weight_dp.emb.weight'])\n",
    "test_eq(tst.state_dict()['1.attn.lbs_weight_dp.emb.weight'], new_wgts['i_weight.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af50343-383d-4284-b11e-d3016d54e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Learner.__init__)\n",
    "class TextLearner(Learner):\n",
    "    \"Basic class for a `Learner` in NLP.\"\n",
    "    def __init__(self, \n",
    "        dls:DataLoaders, # Text `DataLoaders`\n",
    "        model, # A standard PyTorch model\n",
    "        alpha:float=2., # Param for `RNNRegularizer`\n",
    "        beta:float=1., # Param for `RNNRegularizer`\n",
    "        moms:tuple=(0.8,0.7,0.8), # Momentum for `Cosine Annealing Scheduler`\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(dls, model, moms=moms, **kwargs)\n",
    "        self.add_cbs(rnn_cbs())\n",
    "\n",
    "    def save_encoder(self, \n",
    "        file:str # Filename for `Encoder` \n",
    "    ):\n",
    "        \"Save the encoder to `file` in the model directory\"\n",
    "        if rank_distrib(): return # don't save if child proc\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        torch.save(encoder.state_dict(), join_path_file(file, self.path/self.model_dir, ext='.pth'))\n",
    "    \n",
    "    @delegates(save_model)\n",
    "    def save(self,\n",
    "        file:str, # Filename for the state_directory of the model\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save model and optimizer state (if `with_opt`) to `self.path/self.model_dir/file`\n",
    "        Save `self.dls.vocab` to `self.path/self.model_dir/clas_vocab.pkl`\n",
    "        \"\"\"\n",
    "        model_file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "        vocab_file = join_path_file(file+'_vocab', self.path/self.model_dir, ext='.pkl')\n",
    "        save_model(model_file, self.model, getattr(self, 'opt', None), **kwargs)\n",
    "        save_pickle(vocab_file, self.dls.vocab)\n",
    "        return model_file\n",
    "\n",
    "    def load_encoder(self, \n",
    "        file:str, # Filename of the saved encoder \n",
    "        device:(int,str,torch.device)=None # Device used to load, defaults to `dls` device\n",
    "    ):\n",
    "        \"Load the encoder `file` from the model directory, optionally ensuring it's on `device`\"\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if device is None: device = self.dls.device\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        distrib_barrier()\n",
    "        wgts = torch.load(join_path_file(file,self.path/self.model_dir, ext='.pth'), map_location=device)\n",
    "        encoder.load_state_dict(clean_raw_keys(wgts))\n",
    "        self.freeze()\n",
    "        return self\n",
    "\n",
    "    def load_pretrained(self, \n",
    "        wgts_fname:str, # Filename of saved weights \n",
    "        vocab_fname:str, # Saved vocabulary filename in pickle format\n",
    "        model=None # Model to load parameters from, defaults to `Learner.model`\n",
    "    ):\n",
    "        \"Load a pretrained model and adapt it to the data vocabulary.\"\n",
    "        old_vocab = load_pickle(vocab_fname)\n",
    "        new_vocab = _get_text_vocab(self.dls)\n",
    "        distrib_barrier()\n",
    "        wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model'] #Just in case the pretrained model was saved with an optimizer\n",
    "        wgts = match_embeds(wgts, old_vocab, new_vocab)\n",
    "        load_ignore_keys(self.model if model is None else model, clean_raw_keys(wgts))\n",
    "        self.freeze()\n",
    "        return self\n",
    "\n",
    "    #For previous versions compatibility. Remove at release\n",
    "    @delegates(load_model_text)\n",
    "    def load(self, \n",
    "        file:str, # Filename of saved model \n",
    "        with_opt:bool=None, # Enable to load `Optimizer` state\n",
    "        device:(int,str,torch.device)=None, # Device used to load, defaults to `dls` device\n",
    "        **kwargs\n",
    "    ):\n",
    "        if device is None: device = self.dls.device\n",
    "        if self.opt is None: self.create_opt()\n",
    "        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n",
    "        load_model_text(file, self.model, self.opt, device=device, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def load_collab(self,\n",
    "        wgts_fname:str, # Filename of the saved collab model\n",
    "        collab_vocab_fname:str, # Saved Vocabulary of collab labels in pickle format \n",
    "        model=None # Model to load parameters from, defaults to `Learner.model`\n",
    "    ):\n",
    "        \"Load the label embeddings learned by collab model`, and adapt it to the label vocabulary.\"\n",
    "        collab_vocab = load_pickle(collab_vocab_fname)\n",
    "        lbs_vocab = _get_label_vocab(self.dls)\n",
    "        distrib_barrier()\n",
    "        wgts = torch.load(wgts_fname, map_location=lambda storage,loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model'] #Just in case the pretrained model was saved with an optimizer\n",
    "        wgts, _ = match_collab(wgts, collab_vocab, lbs_vocab)\n",
    "        load_collab_keys(self.model if model is None else model, wgts)\n",
    "        self.freeze()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c077c1d-1fde-4c45-ae1e-6f6cd3f58b4c",
   "metadata": {},
   "source": [
    "Adds a `ModelResetter` and an `RNNRegularizer` with `alpha` and `beta` to the callbacks, the rest is the same as `Learner` init. \n",
    "\n",
    "This `Learner` adds functionality to the base class:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001ab92-67ce-4357-b447-0b85c0cbd384",
   "metadata": {},
   "source": [
    "## `Learner` convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39c937-70c1-438b-84d9-34d7ef4a8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from xcube.text.models.core import _model_meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38916689-ddc4-498f-881d-36ddebd9e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@delegates(Learner.__init__)\n",
    "def text_classifier_learner(dls, arch, seq_len=72, config=None, backwards=False, pretrained=True, collab=False, drop_mult=0.5, n_out=None,\n",
    "                           lin_ftrs=None, ps=None, max_len=72*20, y_range=None, **kwargs):\n",
    "    \"Create a `Learner` with a text classifier from `dls` and `arch`.\"\n",
    "    vocab = _get_text_vocab(dls)\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from the data, set `dls.c` or pass `n_out`\"\n",
    "    model = get_text_classifier(arch, len(vocab), n_out, seq_len=seq_len, config=config, y_range=y_range,\n",
    "                                drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps, max_len=max_len)\n",
    "    meta = _model_meta[arch]\n",
    "    learn = TextLearner(dls, model, splitter=meta['split_clas'], **kwargs)\n",
    "    url = 'url_bwd' if backwards else 'url'\n",
    "    if pretrained:\n",
    "        if url not in meta:\n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta[url], c_key='model')\n",
    "        try: fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n",
    "        learn = learn.load_pretrained(*fnames, model=learn.model[0])\n",
    "    if collab:\n",
    "        try: fnames = [list(learn.path.glob(f'**/collab/*collab*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        except IndexError: print(f'The collab model in {learn.path} is incomplete, re-train it!'); raise\n",
    "        learn = learn.load_colab(*fnames, model=learn.model[1])\n",
    "    learn.freeze()\n",
    "    return learn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f6c93-6031-40fb-8bc9-e32a9c519643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
