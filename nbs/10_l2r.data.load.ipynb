{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b549cba-b245-4c41-ad56-a601fed02972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval:false\n",
    "! [ -e /content ] && pip install -Uqq fastai # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c3b0b1d-1206-45e8-bb99-bef4260792c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp l2r.data.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78fcac62-a980-40b0-8527-ae7aa0dff3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.torch_imports import *\n",
    "from fastai.data.load import DataLoader\n",
    "from xcube.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "211df762-470c-4e5b-b2c9-730aaff09543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c109685-2a49-4647-aae5-1e00807f38a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef19f336-314c-4c2f-9b42-622a2d6007ff",
   "metadata": {},
   "source": [
    "# L2R DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9ebe55c-7664-4f1d-a464-1381c3b5bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2RDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.sl, self.lbs_chunks = kwargs.pop('sl', None), kwargs.pop('lbs_chunks', None)\n",
    "        if self.sl is None: self.sl = 64\n",
    "        if self.lbs_chunks is None: self.lbs_chunks = 4\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def randomize(self):\n",
    "        seed = np.random.default_rng().integers(0, 2**32-1, 1).item()\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def shuffle_fn(self, idxs): return self.rng.permutation(idxs)\n",
    "\n",
    "    def get_idxs(self):\n",
    "        if self.n is not None: idxs = range(self.n)\n",
    "        if self.shuffle: idxs = (idx for idx in self.shuffle_fn(idxs))\n",
    "        return idxs\n",
    "    \n",
    "    def create_batch(self, start_idx):\n",
    "        return self.dset[start_idx: min(start_idx+self.bs, self.dset.shape[0])]\n",
    "        # if self.device: to_device(btch, self.device)\n",
    "        # return btch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil((np.ceil(self.dataset.shape[1]/self.sl) * self.lbs_chunks)/self.bs))\n",
    "    \n",
    "    def before_iter(self):\n",
    "        # shuffling\n",
    "        randperm = torch.randint(low=0, high=self.dataset.shape[1], size=(self.dataset.shape[1],))\n",
    "        self.dataset = self.dataset[:, randperm]\n",
    "        # self.lbs_chunks = 4\n",
    "        size_of_dim0 = torch.ceil(self.dataset.new_empty(1).fill_(self.dataset.shape[0]/self.lbs_chunks)).item()\n",
    "        pad_len_dim0 = int(self.lbs_chunks * np.floor(self.dataset.shape[0]/self.lbs_chunks) + self.lbs_chunks - self.dataset.shape[0])\n",
    "        self.dataset_pad = F.pad(self.dataset, (0,0,0,0,0,pad_len_dim0), value=-1)\n",
    "\n",
    "        trn_sqs = list(torch.split(self.dataset_pad, split_size_or_sections=self.sl, dim=1))\n",
    "        test_eq(len(trn_sqs), np.ceil(self.dataset_pad.shape[1]/self.sl))\n",
    "        test_eq(trn_sqs[-1].shape, (self.dataset_pad.shape[0], self.dataset_pad.shape[1]%self.sl,4))\n",
    "        deficit = self.sl - trn_sqs[-1].shape[1]\n",
    "        if deficit: \n",
    "            test_eq(trn_sqs[-1].shape, (self.dataset_pad.shape[0], self.dataset_pad.shape[1]%self.sl,4));\n",
    "            # trn_sqs[-1] = torch.concat((trn_sqs[-1], self.dataset_pad.new_empty((trn_sqs[-1].shape[0], deficit,3)).fill_(-1)), dim=1)\n",
    "            trn_sqs[-1] = trn_sqs[-1].repeat_interleave(self.sl//trn_sqs[-1].shape[1], dim=1)\n",
    "        test_eq(trn_sqs[-1].shape, (self.dataset_pad.shape[0], self.sl,4));\n",
    "        # self.dset = torch.concat(trn_sqs)\n",
    "        # self.dset = torch.stack(trn_sqs)\n",
    "        \n",
    "        trn_sqs = map(partial(torch.chunk, chunks=self.lbs_chunks), trn_sqs)\n",
    "        trn_sqs = itertools.chain.from_iterable(trn_sqs)\n",
    "        self.dset = trn_sqs\n",
    "        # test_eq(self.dset.shape, (self.dataset_pad.shape[0]*len(trn_sqs), self.sl, 3))\n",
    "        # test_eq(self.dset.shape, (len(trn_sqs), self.dataset_pad.shape[0], self.sl, 3))\n",
    "        # print(f\"{self.dset.shape=}\")\n",
    "        # yield from (btch for btch in dset.split(self.bs))\n",
    "    \n",
    "    def create_batches(self, samps):\n",
    "            # trn_sqs = list(torch.split(self.dataset, split_size_or_sections=self.sl, dim=1))\n",
    "            # test_eq(len(trn_sqs), np.ceil(self.dataset.shape[1]/self.sl))\n",
    "            # test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.dataset.shape[1]%self.sl,3))\n",
    "            # deficit = self.sl - trn_sqs[-1].shape[1]\n",
    "            # if deficit: \n",
    "            #     test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.dataset.shape[1]%self.sl,3));\n",
    "            #     trn_sqs[-1] = torch.concat((trn_sqs[-1], self.dataset.new_empty((trn_sqs[-1].shape[0], deficit,3)).fill_(-1)), dim=1)\n",
    "            # test_eq(trn_sqs[-1].shape, (self.dataset.shape[0], self.sl,3));\n",
    "            # # self.dset = torch.concat(trn_sqs)\n",
    "            # self.dset = torch.stack(trn_sqs)\n",
    "            # # test_eq(self.dset.shape, (self.dataset.shape[0]*len(trn_sqs), self.sl, 3))\n",
    "            # test_eq(self.dset.shape, (len(trn_sqs), self.dataset.shape[0], self.sl, 3))\n",
    "            # print(f\"{self.dset.shape=}\")\n",
    "            # # yield from (btch for btch in dset.split(self.bs))\n",
    "        # chunks = range(0, self.dset.shape[0], self.bs)\n",
    "        # with ProcessPoolExecutor(self.n_workers) as ex:\n",
    "        # with Pool(processes=self.num_workers) as pool:\n",
    "        # yield from pool.imap_unordered(self.create_batch, chunks, 16)\n",
    "        # yield from map(self.create_batch, chunks)\n",
    "        # yield from chunked(self.dset, chunk_sz=self.bs)\n",
    "        yield from (torch.stack(btch) for btch in self.chunkify(self.dset))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1a0ee-56eb-40d9-aafe-6c69d791a922",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b3cc98c0-b181-48ce-9028-6ee2f18d0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
