{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f6eed62a-b076-418b-9293-53db08ae0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval:false\n",
    "! [ -e /content ] && pip install -Uqq fastai # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f9b2ba66-c8bf-4819-8b37-e6fd554bbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp l2r.learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "72f1100f-5d24-4a14-9751-1f0d761722a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.torch_imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.optimizer import *\n",
    "from fastai.torch_core import *\n",
    "from fastcore.all import *\n",
    "from xcube.imports import *\n",
    "from xcube.metrics import *\n",
    "from xcube.l2r.gradients import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "305c3600-b316-4849-bf55-17a709f1dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "59d8b9a6-e8dd-4ada-8d21-9af75386d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b33347-7ad3-4fb5-b4df-faeed99e6152",
   "metadata": {},
   "source": [
    "# Learner for Learning to Rank Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "06df17bc-d84f-4fdb-ab1a-45c9487c8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2RLearner:\n",
    "    def __init__(self, model, dls, grad_func, loss_func, lr, cbs, opt_func=SGD, path=None):\n",
    "        store_attr(but='cbs')\n",
    "        # if cbs: \n",
    "        #     for cb in cbs: cb.learn = self\n",
    "        self.path = Path(path) if path is not None else getattr(dls, 'path', Path('.'))\n",
    "        self.cbs = L()\n",
    "        self.add_cbs(cbs)\n",
    "\n",
    "    def add_cb(self, cb):\n",
    "        cb.learn = self\n",
    "        setattr(self, cb.name, cb)\n",
    "        self.cbs.append(cb)\n",
    "        return self\n",
    "\n",
    "    def add_cbs(self, cbs):\n",
    "        L(cbs).map(self.add_cb)\n",
    "        return self\n",
    "        \n",
    "    # def one_batch(self, losses, ndcgs, ndcgs_at_6, accs, track_trn=True, logger=None, grad_logger=None, metric_logger=None, **kwargs): #cb\n",
    "    def one_batch(self, *args, **kwargs):\n",
    "        self('before_batch')\n",
    "        self.preds = self.model(self.xb)\n",
    "        if self.model.training: # training\n",
    "            srtd_preds, lambda_i = self.grad_func(self.preds, self.xb)\n",
    "            srtd_preds.backward(lambda_i)\n",
    "            \n",
    "            self('after_backward')\n",
    "            \n",
    "            # free memory\n",
    "            lambda_i = None\n",
    "            import gc; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # ## tracking gradients #cb\n",
    "            # for name,param in self.model.named_parameters():\n",
    "            # # import pdb; pdb.set_trace()\n",
    "            # # from IPython import embed; embed()\n",
    "            #     grad = param.grad.data.detach().clone()\n",
    "            #     grad_logger[name].append(grad)\n",
    "            \n",
    "            # tracking loss #cb\n",
    "            # if logger is not None:\n",
    "            #     with torch.no_grad():\n",
    "            #         loss = self.loss_func(self.preds, self.xb)\n",
    "            #         logger.append(loss.mean())\n",
    "            #         # losses.append(loss.mean()) #cb\n",
    "            \n",
    "            ## stepping the params\n",
    "            self.opt.step()\n",
    "            ## zeroing the grad before next batch\n",
    "            self.opt.zero_grad()\n",
    "            \n",
    "            # tracking metrics during training #cb\n",
    "            # if track_trn:\n",
    "            #     with torch.no_grad():\n",
    "            #         *_, _ndcg, _ = ndcg(self.preds, self.xb)\n",
    "            #         btch_ndcg_mean = _ndcg.mean()\n",
    "            #         ndcgs.append(btch_ndcg_mean)\n",
    "            #         btch_acc_mean = accuracy(self.xb, self.model).mean()\n",
    "            #         accs.append(btch_acc_mean)\n",
    "            \n",
    "        # else: # validation #cb\n",
    "        #     loss = self.loss_func(self.preds, self.xb)\n",
    "        #     losses.append(loss.mean())\n",
    "        #     *_, _ndcg, _ndcg_at_k = ndcg(self.preds, self.xb, k=6)\n",
    "        #     ndcgs.append(_ndcg.mean())\n",
    "        #     ndcgs_at_6.append(_ndcg_at_k.mean())\n",
    "        #     acc = accuracy(self.xb, self.model)\n",
    "        #     accs.append(acc.mean())\n",
    "            \n",
    "        self('after_batch')\n",
    "        \n",
    "        # return losses, ndcgs, ndcgs_at_6, accs    #cb\n",
    "        \n",
    "    # def one_epoch(self, train, mb, **kwargs): #cb\n",
    "    def one_epoch(self, train, **kwargs):\n",
    "        # losses, ndcgs, ndcgs_at_6, accs = [], [], [], [] #cb\n",
    "        self.model.training = train\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        self('before_epoch')\n",
    "        # for self.num, self.xb in enumerate(progress_bar(dl, parent=mb, leave=False)): #cb\n",
    "        for self.iter_num, self.xb in enumerate(self.dl):\n",
    "            # losses, ndcgs, ndcgs_at_6, accs = self.one_batch(losses, ndcgs, ndcgs_at_6, accs, **kwargs) # cb\n",
    "            self.one_batch(**kwargs)\n",
    "        # _li = [losses, ndcgs, ndcgs_at_6, accs] # cb\n",
    "        # _li = [torch.stack(o) if o else torch.Tensor() for o in _li] # cb\n",
    "        # [losses, ndcgs, ndcgs_at_6, accs] = _li #cb\n",
    "        # logger = [round(o.mean().item(), 4) if o.sum() else \"NA\" for o in _li] # cb\n",
    "        self('after_epoch')\n",
    "        # pdb.set_trace()\n",
    "        # if not self.model.training and metric_logger is not None: metric_logger.append(logger) #cb\n",
    "        # return logger # cb\n",
    "    \n",
    "    def create_opt(self):\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        # self.opt.clear_state()\n",
    "        return self.opt\n",
    "    \n",
    "    # def fit(self, n_epochs, best=None, track_trn=True, **kwargs):\n",
    "    def fit(self, n_epochs, **kwargs):\n",
    "        opt = getattr(self, 'opt', None)\n",
    "        if opt is None: self.create_opt()\n",
    "        self.n_epochs = n_epochs\n",
    "        # self.track_trn = track_trn\n",
    "        self('before_fit')\n",
    "        # mb = master_bar(range(self.n_epochs)) # cb\n",
    "        # columns=['train_loss', 'train_ndcg', 'train_ndcg@6', 'train_acc', 'val_loss', 'val_ndcg (candi. 32)', 'val ndcg@6 (candi. 32)', 'val_acc']\n",
    "        # pdf = pd.DataFrame(columns=columns)#, index=index)\n",
    "        # pdf.index.name = 'epoch'\n",
    "        # if best is not None and best[0] not in columns: raise NameError(best[0]+'metric is not trackable, please check name!')\n",
    "        try:\n",
    "            # for self.epoch,_ in enumerate(mb):\n",
    "            for self.epoch,_ in enumerate(range(self.n_epochs)):\n",
    "            #     pdf.loc[self.epoch] = pd.Series(dict(zip(columns, self.one_epoch(True, mb, **kwargs) + self.one_epoch(False, mb, **kwargs)))) #cb\n",
    "                # self.one_epoch(True, mb, **kwargs)\n",
    "                # self.one_epoch(False, mb, **kwargs)\n",
    "                self.one_epoch(True, **kwargs)\n",
    "                self.one_epoch(False, **kwargs)\n",
    "            #     if best is not None: \n",
    "            #         current = pdf.loc[self.epoch][best[0]]\n",
    "            #         if current >= best[1]:\n",
    "            #             best[1] = current\n",
    "            #             self.save(best[2])\n",
    "            #     display_df(pdf.iloc[[self.epoch]])\n",
    "            # clear_output(wait=True)\n",
    "            # display_df(pdf)\n",
    "        except CancelFitException: pass \n",
    "        self('after_fit')\n",
    "    \n",
    "    def validate(self, *args, **kwargs):\n",
    "        columns=['val_loss', 'val_ndcg (candi. 32)', 'val ndcg@6 (candi. 32)', 'val_acc']\n",
    "        pdf = pd.DataFrame(columns=columns)\n",
    "        pdf.index.name = 'epoch'\n",
    "        try: \n",
    "            val = dict(zip(columns, self.one_epoch(False, None, **kwargs)))\n",
    "            pdf = pd.DataFrame([val])\n",
    "            display_df(pdf)\n",
    "        except CancelFitException: pass\n",
    "    \n",
    "    def __call__(self, name):\n",
    "        for cb in self.cbs: getattr(cb, name, noop)()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fd87c-b8e0-4236-bd80-6f1414ef64ef",
   "metadata": {},
   "source": [
    "**Serializing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1059cdef-53f1-4f7c-bfff-87816f9a1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "@delegates(save_model)\n",
    "def save(self:Learner, file, **kwargs):\n",
    "    \"Save model and optimizer state (if 'with_opt') to `self.path/file`\"\n",
    "    file = join_path_file(file, self.path, ext='.pth')\n",
    "    save_model(file, self.model, getattr(self, 'opt', None), **kwargs)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "49ad0bde-478c-49a8-b125-106444ef965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "@delegates(load_model)\n",
    "def load(self:Learner, file, device=None, **kwargs):\n",
    "    \"Load model and optimizer state (if `with_opt`) from `self.path/file` using `device`\"\n",
    "    if device is None and hasattr(self.dls, 'device'): device = self.dls.device\n",
    "    self.opt = getattr(self, 'opt', None)\n",
    "    if self.opt is None: self.create_opt()\n",
    "    file = join_path_file(file, self.path, ext='.pth')\n",
    "    load_model(file, self.model, self.opt, device=device, **kwargs)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c74968-6cff-4704-b16b-ccf494472a6d",
   "metadata": {},
   "source": [
    "### Learner convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5360c28a-5f98-4a83-82cd-c4a261756569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_learner(model, dls, grad_fn=rank_loss3, loss_fn=loss_fn2, lr=1e-5, cbs=None, opt_func=partial(SGD, mom=0.9), lambrank=False):\n",
    "    if lambrank: grad_fn = partial(grad_fn, lambrank=lambrank)\n",
    "    learner = L2RLearner(model, dls, grad_fn, loss_fn, lr, cbs, opt_func=opt_func)\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f7b38-1e2c-49c5-8231-f4e0a5130e86",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "294269ac-6a2f-4bc0-8160-7e4ea3cab241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3d18a-baab-4434-b4a7-03f5fdd8cb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
