# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_layers.ipynb (unless otherwise specified).

__all__ = ['LinBnDrop', 'XMLAttention']

# Cell
from fastai.imports import *
from fastai.torch_imports import *
from fastai.torch_core import *
from fastai.layers import *

# Cell
class LinBnDrop(nn.Sequential):
    "Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers"
    def __init__(self, n_in, n_out=None, bn=True, ln=True, p=0., act=None, lin_first=False):
        layers = [BatchNorm(n_out if ln and lin_first else n_in, ndim=1)] if bn else []
        if p != 0: layers.append(nn.Dropout(p))
        lin = [nn.Linear(n_in, n_out, bias=not bn)] if ln else []
        if ln and act is not None: lin.append(act)
        layers = lin+layers if lin_first else layers+lin
        super().__init__(*layers)

# Cell
class XMLAttention(Module):
    "Compute label specific attention weights for each token in a sequence"
    def __init__(self, n_lbs, emb_sz):
         self.lbs_emb = nn.Embedding(n_lbs, emb_sz)
         self.lbs_emb.weight.data.normal_(0, 0.01)

    def forward(self, x):
        attn_wgts = F.softmax(x @ self.lbs_emb.weight.transpose(0,1), dim=1)
        return attn_wgts.transpose(1,2) @ x
